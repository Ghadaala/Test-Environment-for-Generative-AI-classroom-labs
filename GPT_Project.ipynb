{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyIoBbFcqJWM"
      },
      "source": [
        "Transformers library [Transformers Hugging Face Library ðŸ¤—](https://huggingface.co/docs/transformers/en/index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an LLM (Large Language Model)"
      ],
      "metadata": {
        "id": "kYrUOPb6tlMm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17h3oI5L3Zkl"
      },
      "source": [
        "## Import the required libraries and tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLxZULvEKESq"
      },
      "outputs": [],
      "source": [
        "import requests # To read the dataset\n",
        "import numpy as np # fast array computations\n",
        "import tensorflow as tf # for building deep learning models (LLM models)\n",
        "from transformers import GPT2Tokenizer # ðŸ¤— by Hugging Face\n",
        "import pandas as pd # to store the history records and plot the loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GFWKHG50Jfe",
        "outputId": "e73fdd50-8529-4d0b-b756-c9d91049663a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Version: [37906, 318, 1107, 1049]\n",
            "Decoded Version: Python is really great\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "encoded = tokenizer.encode(\"Python is really great\")\n",
        "print(f\"Encoded Version: {encoded}\")\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(f\"Decoded Version: {decoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVhIMUd-sZNB"
      },
      "source": [
        "**Tokenization** shows the text as a series of tokens or useful meaningful units that are larger than individual characters, but smaller than phrases and sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq5m191ITENs"
      },
      "source": [
        "## Get the data to model it!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(\"https://programmingoceanacademy.s3.ap-southeast-1.amazonaws.com/Practical_labs/Data+Science+and+Data+Analysis+Tables+%26+comparisons.txt\")"
      ],
      "metadata": {
        "id": "YG_oRxKVv14W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.status_code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pIVAWmiv-y-",
        "outputId": "2739feef-2232-44a0-bab9-0d646675070a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "R5mj8AfDwEPI",
        "outputId": "d305431e-2b55-4aa7-d90e-f775a09309d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\r\\n\\r\\n\\r\\nComparisons in Data Science and AI Bootcamp - Programming Ocean Academy-2024\\r\\nProgramming Ocean Academy - Data Storge and Processing\\rAspect\\rWarehouse\\rLake\\rPipeline\\rDatabase\\rData Mart\\rPurpose\\rStores structured data for analysis\\rStores both structured and unstructured data\\rTransfers data from one system to another\\rStores structured data efficiently\\rSubset of a data warehouse tailored for specific business line or department\\rData Type\\rPrimarily structured\\rBoth structured and unstructured\\rTypically structured\\rStructured data\\rStructured data\\rScale\\rCan handle large volumes of data\\rDesigned for massive data storage\\rDesigned for data movement\\rScales according to capacity\\rSmaller subset of a data warehouse\\rProcessing\\rMay involve data transformation\\rCan store data without prior schema definition\\rDirects data flow without altering content\\rMay involve complex querying\\rFocuses on specific business needs\\rAccess Pattern\\rRead-intensive for analytics\\rRead and write-intensive\\rTypically write-intensive, with reads possible\\rMix of read and write operations\\rTailored for specific users or departments\\rLatency\\rGenerally low latency for retrieval\\rLow latency for accessing data\\rLow latency for data transfer\\rGenerally low latency operations\\rOptimized for faster access within a specific domain\\rMaintenance\\rRequires periodic maintenance\\rMay require ongoing optimization\\rMaintenance depends on pipeline complexity\\rRequires regular maintenance\\rRequires maintenance to align with evolving business needs\\rExample\\rA retail company\\'s data warehouse stores transaction records, customer information, and inventory data. It collects and organizes structured data from various sources like point-of-sale systems, online sales platforms, and inventory management software. Analysts use this data warehouse to analyze sales trends, customer behavior, and inventory management strategies.\\rA healthcare organization\\'s data lake stores a vast array of data types, including electronic health records (structured), medical images (unstructured), patient-generated data from wearables (semi-structured), and research publications (unstructured). Researchers and data scientists can access this data lake to perform advanced analytics, such as predictive modeling for disease diagnosis or population health management.\\rAn e-commerce company implements a data pipeline to transfer real-time sales data from its online store to a centralized data warehouse. The pipeline collects transactional data, performs necessary transformations (e.g., cleansing, enrichment), and loads the data into the warehouse for analysis. This pipeline ensures that the data is continuously flowing and available for timely decision-making.\\rA financial institution uses a relational database management system (RDBMS) to store customer banking information, including account balances, transaction history, and loan details. The database supports complex querying for activities such as fraud detection, risk assessment, and customer relationship management. It ensures data integrity, consistency, and security for critical banking operations.\\rWithin a large manufacturing company, the sales department maintains a data mart specific to its operations. This sales data mart contains information on sales transactions, customer demographics, and product performance metrics relevant to the sales team\\'s objectives. By focusing on sales-related data and providing tailored analytics tools, the data mart helps sales managers track performance, forecast sales, and optimize marketing strategies for their business unit.\\r\\r\\n\\r\\nProgramming Ocean Academy -DATA SCIENE BOOTCAMP\\rCriteria\\rHadoop\\rHive\\rSpark\\rCategory\\rBig Data Framework\\rData Warehousing and Query Language\\rUnified Data Processing Framework\\rData Processing Model\\rBatch Processing\\rBatch Processing (SQL-like queries)\\rBatch and Real-Time Processing\\rData Storage\\rHadoop Distributed File System (HDFS)\\rHDFS, Other File Systems\\rIn-Memory Storage (Resilient Distributed Datasets - RDDs)\\rQuery Language\\rHQL (Hive Query Language)\\rHiveQL (SQL-like)\\rSpark SQL, Python, Scala, Java, and more\\rEase of Use\\rComplex, requires Java programming skills\\rSQL-like queries, easier for SQL users\\rProvides APIs in multiple languages for ease of use\\rData Transformation\\rLimited data transformation capabilities\\rETL capabilities for data transformation\\rExtensive data transformation capabilities\\rData Processing Speed\\rSlower due to batch processing\\rFaster than Hadoop, optimized for queries\\rIn-memory processing for high speed\\rUse Cases\\rBig data storage and batch processing\\rData warehousing, ad-hoc querying\\rBatch and real-time data processing, machine learning, graph processing, and more\\rEcosystem\\rPart of the Hadoop ecosystem\\rPart of the Hadoop ecosystem\\rPart of the Hadoop ecosystem, but can be used independently\\rResource Management\\rHadoop\\'s YARN ResourceManager\\rIntegrates with Hadoop\\'s resource management\\rBuilt-in cluster manager, standalone mode available\\rFault Tolerance\\rProvides fault tolerance through replication\\rProvides fault tolerance through replication\\rProvides fault tolerance through lineage information and data recomputation\\rReal-Time Processing\\rNot suitable for real-time processing\\rLimited real-time capabilities\\rSupports real-time processing through streaming and batch modes\\rPerformance Tuning\\rComplex tuning required for optimal performance\\rEasier performance tuning\\rProvides various optimization techniques\\r\\r\\nData Processing with three important Apache Data Science Tools- Programming Ocean Academy\\rAspect\\rApache Hadoop\\rApache Hive\\rApache Spark\\rOverview\\rA collection of tools for distributed storage and processing of big data.\\rA data warehouse built on top of Hadoop for querying and managing large datasets.\\rA distributed data analytics framework for complex data processing in real-time.\\rMain Components\\r- Hadoop Distributed File System (HDFS)\\r- Runs on HDFS or other data storage systems like Apache HBase.\\r- In-memory processing engine with support for multiple programming languages.\\rStorage System\\r- HDFS: Splits large files into blocks and distributes across multiple nodes.\\r- Stores metadata and query results.\\r- Can access data from various sources, including HDFS and Hive.\\rData Handling\\r- Handles structured, semi-structured, and unstructured data.\\r- Handles data stored in HDFS or HBase, optimized for large-scale queries.\\r- Handles interactive analytics, streaming data, machine learning, and ETL tasks.\\rProcessing\\r- Batch processing, with parallel computation on distributed data blocks.\\r- Read-based queries, suitable for ETL and data warehousing but not for high-speed transaction processing.\\r- Real-time processing, in-memory computations, spills to disk only when necessary.\\rScalability\\r- Scales linearly by adding more nodes to the cluster.\\r- Scales with Hadoop\\'s distributed storage system, but queries have high latency.\\r- Scales efficiently with its in-memory processing and can be used with standalone or Hadoop clustering.\\rFault Tolerance\\r- Replicates data blocks on multiple nodes to prevent data loss.\\r- Dependent on Hadoop\\'s fault tolerance, but queries can be slow.\\r- Fault-tolerant with in-memory data recovery.\\rPerformance\\r- Cost-effective and reliable, but can be slow due to disk-based storage and batch processing.\\r- High latency due to batch-oriented processing and large-scale data queries.\\r- High-speed computations with in-memory processing, reducing latency.\\rReal-time Capabilities\\r- Not designed for real-time processing.\\r- Not suitable for real-time processing due to high latency.\\r- Designed for real-time data processing and streaming analytics.\\rUse Cases\\r- Storing large datasets, data warehousing, handling diverse data formats.\\r- ETL, reporting, and data analysis where high latency is acceptable.\\r- Interactive analytics, streaming data processing, machine learning, and ETL tasks.\\rProgramming Languages\\r- Primarily Java-based.\\r- SQL-like query language.\\r- Supports Java, Scala, Python, R, and SQL.\\rIntegration\\r- Can be integrated with various tools and systems for big data processing.\\r- Built on Hadoop, integrates with HDFS and HBase.\\r- Can run on top of Hadoop, and integrates with HDFS, Hive, and other data sources.\\r\\r\\n\\r\\n\\rData Mining Processes - Programming Ocean Academy - Data Science Bootcamp\\rStep\\rDescription\\rEstablishing Data Mining Goals\\rSet clear objectives considering cost-benefit trade-offs, accuracy, and usefulness of results.\\rSelecting Data\\rIdentify quality data sources, considering type, availability, and cost-effectiveness.\\rPreprocessing Data\\rClean and refine raw data, removing errors, handling missing data, and ensuring integrity.\\rTransforming Data\\rModify data to reduce complexity or fit analysis, using techniques like aggregation or conversion.\\rStoring Data\\rStore transformed data securely, allowing unrestricted access for analysis and ensuring privacy.\\rMining Data\\rApply various analysis methods and algorithms, including visualization, to uncover hidden trends.\\rEvaluating Mining Results\\rTest model performance, gather stakeholder feedback, and iteratively refine analysis.\\r\\r\\nProgramming Ocean Academy - Data Science Bootcamp- FEB- 2024\\rAspect\\rRecommender Systems\\rFraud Detection\\rApplication Area\\rUsed in various platforms for recommendations, such as Netflix and Facebook\\rPrimarily applied in retail banking and finance for detecting fraudulent activities\\rObjective\\rRecommending relevant content or products to users based on their preferences and behavior\\rIdentifying potentially fraudulent transactions in real-time\\rTechniques\\rUtilizes algorithms like collaborative filtering, content-based filtering, and matrix factorization\\rEmploys machine learning models such as anomaly detection, supervised learning, and unsupervised learning\\rData Sources\\rRelies on user interactions, ratings, and historical behavior to generate recommendations\\rAnalyzes transaction data, user behavior patterns, and historical fraud cases to train models\\rReal-time\\rRecommendations can be generated in real-time based on user actions and preferences\\rFraud detection operates in real-time to quickly flag suspicious transactions for further investigation\\rBusiness Impact\\rEnhances user experience and increases engagement by providing personalized recommendations\\rHelps prevent financial losses by identifying and mitigating fraudulent activities in the banking sector\\r\\r\\n\\r\\nProgramming Ocean Academy - Data Science Bootcamp- FEB -2024\\rAspect\\rMachine Learning\\rGenerative AI\\rDeep Learning\\rDefinition\\rUtilizes algorithms to learn patterns from data and make predictions or decisions\\rFocuses on generating new data based on learned patterns, mimicking human creativity\\rSubset of machine learning, employing multiple layers of neural networks for complex tasks\\rFocus\\rPredictions, classifications, and clustering\\rCreation of new data, such as images, music, and text\\rComplex pattern recognition, speech recognition, and image processing\\rTechniques\\rIncludes supervised learning, unsupervised learning, and reinforcement learning\\rUtilizes models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)\\rRelies on neural network architectures like convolutional neural networks (CNNs) and recurrent neural networks (RNNs)\\rTraining Data\\rRequires labeled data for supervised learning, and may use unlabeled data for unsupervised learning\\rTrained on large volumes of data to learn underlying patterns and distributions\\rTrained on extensive datasets to optimize model parameters and learn complex features\\rOutput\\rPredictions or classifications based on input data\\rGenerates new data samples based on learned patterns\\rOutputs predictions, classifications, or generated content based on input\\rApplications\\rWidely used in various domains, including finance, healthcare, and marketing\\rApplied in art generation, content creation, and data augmentation\\rUsed in speech recognition, image processing, natural language processing, and autonomous driving\\rComputational Power\\rRequires moderate to high computational resources, depending on model complexity\\rDemands significant computational power, often utilizing GPUs for training\\rNeeds extensive computational resources due to complex network architectures and large datasets\\r\\r\\nProgramming Ocean Academy - Data Science Bootcamp - Feb 2024\\rFile Format\\rDescription\\rCharacteristics\\rDelimited Text File Formats\\rText files where each line has values separated by a delimiter. Comma-separated values (CSVs) and tab-separated values (TSVs) are common examples.\\r- Delimiter separates values - Common delimiters: comma, tab, colon, vertical bar, space - Rows represent records - First row often contains column headers\\rMicrosoft Excel Open XML Spreadsheet (XLSX)\\rXML-based spreadsheet format developed by Microsoft. Can contain multiple worksheets with data organized into rows and columns.\\r- XML-based - Multiple worksheets in a workbook - Rows and columns organize data - Cells contain data - Generally accessible to other applications - Known for security - Cannot save malicious code\\rExtensible Markup Language (XML)\\rMarkup language with set rules for encoding data. Suitable for sending information over the internet.\\r- Markup language - Readable by humans and machines - Designed for internet data transmission - No predefined tags like HTML - Platform and programming language independent - Simplifies data sharing between systems\\rPortable Document Format (PDF)\\rDeveloped by Adobe to present documents independent of software, hardware, and operating systems.\\r- Developed by Adobe - Document format - Independent of software/hardware - Ensures consistent display across devices - Commonly used in legal and financial documents - Can be used for form data entry\\rJavaScript Object Notation (JSON)\\rText-based open standard for transmitting structured data over the web.\\r- Text-based - Open standard - Structured data transmission - Language-independent - Readable in any programming language - Easy to use - Compatible with various browsers - Suitable for sharing data of any size and type, including audio and video - Commonly used in APIs and web services for data exchange\\r\\r\\nProgramming Ocean Academy -Data Science Bootcamp - FEB 2024\\rDifferent Definitions of Data Scientists\\rDefinition\\rKey Points\\rSimon Rogers\\rA data scientist finds solutions to problems by analyzing data (big or small) using appropriate tools and communicates findings effectively.\\rSimon Rogers (Data Science Definition)\\rData science is what data scientists do.\\rProfessor Eric Miller\\rEngineering is what engineers do; similarly, data science is what data scientists do.\\rDr. Vincent Granville\\rDefines a data scientist as one who can process large datasets efficiently, distrusts statistical models, and requires strong mathematical background.\\rDr. Vincent Granville (Data Science Definition)\\rData science is distinguished from statistics, but requires a background in algebra, calculus, and probability/statistics.\\rDr. Patil\\rA data scientist is a blend of skills that can unlock insights from data and tell compelling stories with data.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rCharacteristic\\rStructured Data\\rSemi-Structured Data\\rUnstructured Data\\rDefinition\\rData with a well-defined structure or schema.\\rData with some organizational properties, lacks a fixed schema.\\rData without a discernible structure or schema.\\rStorage Format\\rStored in well-defined schemas (databases).\\rCannot be stored in rows and columns like databases.\\rTypically stored in files or NoSQL databases.\\rExamples\\rSQL databases, spreadsheets, online forms.\\rXML, JSON, emails, binary executables.\\rWeb pages, social media feeds, images, documents.\\rAccessibility\\rEasily accessible and analyzed with standard tools.\\rRequires parsing and interpretation for analysis.\\rOften requires specialized tools for analysis.\\rAnalysis\\rCan be analyzed using standard data analysis methods.\\rRequires customized parsing and processing.\\rMay involve manual analysis or specialized tools.\\rData Model\\rAdheres to a specified data model.\\rHas some organizational properties but lacks a rigid schema.\\rNo identifiable structure or schema.\\rHierarchy\\rFollows a strict hierarchy.\\rContains tags and elements for grouping data.\\rNo inherent hierarchy; data may be disorganized.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rData Source\\rDescription\\rExamples\\rRelational Databases\\rUsed for storing structured data with well-defined schemas.\\rSQL Server, Oracle, MySQL, IBM DB2\\rFlat Files\\rStore data in plain text format with records separated by delimiters.\\rCSV files\\rXML Datasets\\rContain data marked up using tags, suitable for hierarchical data representation.\\rOnline surveys, bank statements\\rAPIs and Web Services\\rProvide interfaces for accessing and retrieving data from various sources.\\rTwitter API, Facebook API, stock market APIs\\rWeb Scraping\\rExtracts relevant data from unstructured web sources based on defined parameters.\\rBeautifulSoup, Scrapy, Pandas, Selenium\\rData Streams\\rAggregates constant streams of data from sources like IoT devices, applications, and social media.\\rApache Kafka, Apache Spark Streaming, Apache Storm\\rRSS Feeds\\rCaptures updated data from online forums and news sites for streaming to user devices.\\rNews websites, online forums\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rKey Points\\rSummary\\rRelational Databases\\r- Flexibility of relational databases for structured data. - Challenges include moving data between different databases and dealing with versioning issues.\\rEvolution of Unstructured Data\\r- Rise of unstructured data (logs, documents, XML, JSON). - Relational databases face challenges with heavy write-intensive applications like IoT and social media data.\\rVariety of Data Sources\\r- Data engineers need to work with standard formats (CSV, JSON, XML) and proprietary formats. - Data sources include relational databases, NoSQL databases, big data repositories, data at rest, streaming data, and data in motion.\\rChallenges with Data Formats\\r- Log data presents challenges due to its unstructured nature, often requiring custom tools for analysis. - XML, while popular in the past, can be resource-intensive due to its tags. - JSON gained popularity for its simplicity and is widely used in RESTful APIs. - Newer formats like Apache Avro are gaining popularity for their efficiency.\\rHandling Data Conversion Challenges\\r- Example of converting data from Db2 to SQL Server. - Challenges include differences in import/export processes and handling special characters in the data. - Different tables may require different delimiters due to varying special characters.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rMetadata Overview\\rMetadata is data that describes other data, crucial in databases, data warehousing, and business intelligence systems. Three main types: technical, process, and business metadata.\\rTechnical Metadata\\rDefines data structures, stored in system catalogs. Includes tables, data catalogs, and information on databases, tables, and columns.\\rProcess Metadata\\rDescribes processes in business systems. Monitors system performance, data movement, and user access.\\rBusiness Metadata\\rHelps users discover and understand data. Includes acquisition details, data descriptions, and data source connections.\\rMetadata Management\\rInvolves developing policies for accessing and integrating data. Focuses on creating a reliable data catalog for efficient data organization. Enhances data discovery, repeatability, and governance.\\rImportance of Metadata Management\\rEnhances data governance by providing context and lineage. Ensures data quality, consistency, and security. Facilitates effective data management and usage across the organization.\\rPopular Metadata Management Tools\\r- IBM InfoSphere Information Server - CA Erwin Data Modeler - Oracle Warehouse Builder - SAS Data Integration Server - Talend Data Fabric - Alation Data Catalog - SAP Information Steward - Microsoft Azure Data Catalog - IBM Watson Knowledge Catalog - Oracle Enterprise Metadata Management (OEMM) - Adaptive Metadata Manager - Unifi Data Catalog - data.world - Informatica Enterprise Data Catalog\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTerm\\rDefinition\\rComma-separated values (CSVs)\\rDelimited text files where the delimiter is a comma. Used to store structured data.\\rDelimited text file formats\\rText files are used to store data where each line or row has values separated by a delimiter. A delimiter is a sequence of one or more characters specifying the boundary between values. Common delimiters include comma, tab, colon, vertical bar, and space.\\rNoSQL databases\\rDatabases are designed to store and manage unstructured data and provide analysis tools for examining this type of data.\\rOnline Transaction Processing (OLTP) Systems\\rSystems that focus on handling business transactions and storing structured data.\\rRelational databases\\rDatabases are designed to store structured data with well-defined schemas and support standard data analysis methods and tools.\\rSensors\\rDevices such as Global Positioning Systems (GPS) and Radio Frequency Identification (RFID) tags generate structured data.\\rSpreadsheets\\rSoftware applications like Excel and Google Spreadsheets are used for organizing and analyzing structured data.\\rSQL Databases\\rDatabases that use Structured Query Language (SQL) for defining, manipulating, and querying data in structured formats.\\rTab-separated values (TSVs)\\rDelimited text files where the delimiter is a tab. Used as an alternative to CSV when literal commas are present in text data.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTerm\\rDefinition\\rData Repository\\rA general term used to refer to data that has been collected, organized, and isolated so that it can be used for business operations or mined for reporting and data analysis. It can be a small or large database infrastructure with one or more databases that collect, manage, and store data sets. In this video, we will provide an overview of the different types of repositories your data might reside in, such as databases, data warehouses, and big data stores, and examine them in greater detail in further videos.\\rDatabase\\rA collection of data, or information, designed for the input, storage, search and retrieval, and modification of data. A Database Management System, or DBMS, is a set of programs that creates and maintains the database. It allows you to store, modify, and extract information from the database using a function called querying.\\rRelational Databases\\rAlso referred to as RDBMSes, relational databases build on the organizational principles of flat files, with data organized into a tabular format with rows and columns following a well-defined structure and schema. Unlike flat files, RDBMSes are optimized for data operations and querying involving many tables and much larger data volumes. Structured Query Language, or SQL, is the standard querying language for relational databases.\\rNon-Relational Databases\\rAlso known as NoSQL, or \"Not Only SQL\", non-relational databases emerged in response to the volume, diversity, and speed at which data is being generated today, mainly influenced by advances in cloud computing, the Internet of Things, and social media proliferation. Built for speed, flexibility, and scale, non-relational databases made it possible to store data in a schema-less or free-form fashion. NoSQL is widely used for processing big data.\\rData Warehouse\\rA central repository that merges information coming from disparate sources and consolidates it through the extract, transform, and load process (ETL process) into one comprehensive database for analytics and business intelligence. At a very high-level, the ETL process helps you to extract data from different data sources, transform the data into a clean and usable state, and load the data into the enterprise\\'s data repository.\\rData Marts\\rData marts are subsets of data warehouses that focus on specific business lines or departments.\\rData Lakes\\rData lakes are repositories that store vast amounts of raw data in its native format until it is needed.\\rBig Data Stores\\rDistributed computational and storage infrastructure to store, scale, and process very large data sets.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTerm\\rDefinition\\rRelational Database\\rA collection of data organized into a table structure, where the tables can be linked based on common data. Tables consist of rows and columns, with rows representing records and columns representing attributes. Tables can be related based on shared data, allowing for the retrieval of new tables with a single query. Relational databases use Structured Query Language (SQL) for querying data. They minimize data redundancy, offer data integrity through data type constraints, and provide security features for controlled access to data.\\rExamples of Relational Databases\\r- IBM DB2 - Microsoft SQL Server - MySQL - Oracle Database - PostgreSQL - Amazon Relational Database Service (RDS) - Google Cloud SQL - IBM DB2 on Cloud - Oracle Cloud - SQL Azure\\rAdvantages of Relational Databases\\r- Flexibility: Changes can be made to the database structure while it\\'s running. - Reduced redundancy: Minimizes data duplication. - Ease of backup and disaster recovery: Offers easy export/import options and continuous mirroring in cloud-based solutions. - ACID-compliance: Ensures data accuracy and consistency. - Well-documented and mature technology.\\rUse Cases for Relational Databases\\r- Online Transaction Processing (OLTP): Well-suited for transaction-oriented tasks with frequent queries and updates. - Data Warehouses: Optimized for Online Analytical Processing (OLAP) for historical data analysis. - IoT Solutions: Lightweight database solution for collecting and processing data from edge devices.\\rLimitations of Relational Databases\\r- Not suitable for semi-structured and unstructured data analytics. - Migration between RDBMSs requires identical schemas and data types. - Limits on the length of data fields.\\rContinued Relevance of RDBMS\\rDespite the evolution of data technologies, such as big data and IoT, relational databases continue to be the predominant technology for working with structured data\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rData Science Task Categories\\rData Asset Management\\rCode Asset Management\\rExecution Environments\\rDevelopment Environments\\rData Management\\r- Collect, persist, and retrieve data securely, efficiently, and cost-effectively from various sources like Twitter, Flipkart, Media, and Sensors.\\r- Organize and manage important data collected from different sources in a central location.\\r- Provides system resources to execute and verify the code.\\r- Provides a workspace and tools to develop, implement, execute, test, and deploy source code.\\rData Integration and Transformation\\r- Extract, Transform, and Load (ETL) data from multiple repositories into a central Data Warehouse.\\r- Version control and collaboration for managing changes to software projects\\' code.\\r- Libraries to compile the source code.\\r- IDEs like IBM Watson Studio for developing, testing, and deploying source code.\\rData Visualization\\r- Graphical representation of data and information using charts, plots, maps, etc.\\r- Organizing and managing data with versioning and collaboration support.\\r- Tools for compiling and executing code.\\r- Testing and simulation tools provided by IDEs to emulate real-world behavior.\\rModel Building\\r- Train data and analyze patterns using machine learning algorithms.\\r- Unified view for managing an inventory of assets.\\r- System resources for executing and verifying code.\\r- Cloud-based execution environments like IBM Watson Studio for preprocessing, training, and deploying models.\\rModel Deployment\\r- Integrate developed models into production environments via APIs.\\r- Share, collaborate, and manage code files simultaneously.\\r- Tools for compiling and executing code.\\r- Integrated tools like IBM Watson Studio and IBM Cognos Dashboard Embedded for developing deep learning and machine learning models.\\rModel Monitoring and Assessment\\r- Continuous quality checks to ensure model accuracy, fairness, and robustness.\\r- N/A\\r- Libraries for compiling and executing code.\\r- N/A\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rData Management Tools\\rData Integration and Transformation Tools\\rData Visualization Tools\\rModel Tools\\rCode Asset Management Tools\\rData Asset Management Tools\\rMySQL, PostgreSQL, MongoDB, Apache CouchDB, Apache Cassandra, Hadoop File System, Ceph, Elastic search\\rApache AirFlow, KubeFlow, Apache Kafka, Apache Nifi, Apache SparkSQL, NodeRED\\rPixie Dust, Hue, Kibana, Apache Superset\\rApache PredictionIO, Seldon, Kubernetes, Redhat OpenShift, MLeap, TensorFlow service, TensorFlow lite, TensorFlow dot JS\\rGit, GitHub, GitLab, Bitbucket\\rApache Atlas, ODPi Egeria, Kylo\\r\\r\\n\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rDevelopment Environments\\rDescription\\rJupyter\\r- Interactive Python programming tool.  - Supports over a hundred different programming languages through kernels.  - Unifies documentation, code, output, shell commands, and visualizations in a single document.  - Jupyter Lab is the next version of Jupyter Notebooks, offering more modern and modular architecture.\\rApache Zeppelin\\r- Inspired by Jupyter Notebooks, provides a similar experience.  - Integrated plotting capability without requiring external libraries.  - Allows extension of capabilities using additional libraries.\\rRStudio\\r- Development environment for statistics and data science, exclusively for R and associated R libraries.  - Offers Python development within the R environment.  - Unifies programming, execution, debugging, remote data access, exploration, and visualization into one tool.\\rSpyder\\r- Mimics the functionality of RStudio for the Python world.  - Integrates code, documentation, and visualizations into a single canvas.\\rCluster Execution Environments\\rDescription\\rApache Spark\\r- Batch data processing engine with linear scalability.  - Used across various industries, offering parallel processing capabilities.\\rApache Flink\\r- Stream-processing engine focusing on real-time data streams.  - Supports both batch and stream processing paradigms.\\rRay\\r- Focuses on large-scale deep learning model training.  - Latest development in data science execution environments.\\rFully Integrated and Visual Tools\\rDescription\\rKNIME\\r- Visual user interface with drag-and-drop capabilities.  - Built-in visualization and extension capabilities with R, Python, and connectors to Apache Spark.\\rOrange\\r- Less flexible than KNIME but easier to use.  - Offers visual tools for data integration, transformation, visualization, and model building.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTool Category\\rCommercial Tools\\rData Management\\r- Oracle Database  - Microsoft SQL Server  - IBM Db2\\rData Integration and Transformation\\r- Informatica PowerCenter - IBM InfoSphere DataStage - SAP products - Oracle products - SAS products - Talend products - Microsoft products - Watson Studio Desktop (includes Data Refinery)\\rData Visualization\\r- Tableau - Microsoft Power BI - IBM Cognos Analytics - Watson Studio Desktop\\rModel Tools for Building\\r- SPSS Modeler - SAS Enterprise Miner - Watson Studio Desktop (includes SPSS Modeler)\\rModel Tools for Deployment\\r- SPSS Collaboration and Deployment Services\\rModel Tools for Monitoring and Assessment\\rNo relevant commercial tools available; open source preferred\\rCode Asset Management\\rOpen-source tools like Git and GitHub are standard\\rData Asset Management\\r- Informatica Enterprise Data Governance - IBM Information Governance Catalog\\rDevelopment Environment\\r- Watson Studio (cloud-based and desktop version) - H2O Driverless AI\\rFully Integrated Visual Tools\\r- Watson Studio (together with Watson Open Scale) - H2O Driverless AI\\r\\r\\n\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTool Category\\rCommercial Cloud Tools\\rFully Integrated Visual Platform\\r- Watson Studio and Watson Open Scale  - Microsoft Azure Machine Learning  - H2O Driverless AI\\rData Management\\r- Amazon Web Services DynamoDB  - Cloudant  - IBM Db2 as a service\\rData Integration and Transformation\\r- Informatica Cloud Data Integration - IBM Data Refinery (part of Watson Studio)\\rData Visualization\\r- Datameer  - IBM Cognos Business Intelligence suite  - IBM Data Refinery (part of Watson Studio)\\rModel Building\\r- Watson Machine Learning  - Google AI Platform Training\\rModel Deployment\\r- SPSS Collaboration and Deployment Services  - Amazon SageMaker Model Monitor  - Watson Machine Learning\\rModel Monitoring\\r- Watson OpenScale\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rCriteria for Determining Language to Learn\\rPopular Languages\\rConsider your needs and the problems you are trying to solve\\rPython, R, SQL\\rAssess who you are solving these problems for\\rScala, Java, C++, Julia\\r\\rJavaScript, PHP, Go, Ruby, Visual Basic\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rUsers of Python\\rBenefits of Using Python\\rDiversity and Inclusion Efforts of the Python Community\\rExperienced Programmers\\r- Clear and readable syntax\\r- Code of Conduct ensuring safety and inclusion for all members\\rBeginners\\r- Huge global community and wealth of documentation\\r- Community initiatives like PyLadies creating safe and inclusive spaces\\rData Professionals\\r- Versatility for data science, AI, machine learning, web development, etc.\\r- Mentorship programs like PyLadies helping underrepresented groups become active participants and leaders in the Python community\\r\\r- Rich ecosystem of libraries and frameworks for various tasks\\r\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rUsers of the R Language\\rBenefits of Using R\\rGlobal Communities for Connecting with Other R Users\\r- Statisticians\\rFree software, allowing for private, commercial, and public use\\r- useR\\r- Mathematicians\\r- Supported by a wide global community of people who want to solve big problems\\r- WhyR\\r- Data miners\\r- Array-oriented syntax makes it easier to translate from math to code for learners with no or minimal programming background\\r- SatRdays\\r\\r- Large repository of statistical knowledge\\r- R-ladies\\r\\r- Integration with other languages like C++, Java, Python, etc.\\r- R project website for R conferences and events\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp- Feb 2024\\rKey Points\\rSQL is officially pronounced as \"ess cue el\" or \"sequel\".\\rIt stands for \"Structured Query Language\" and is non-procedural.\\rSQL is simple and powerful, making it commonly used by data scientists.\\rDeveloped by IBM in 1974, making it older than Python and R.\\rSQL is designed for managing data in relational databases.\\rRelational databases consist of two-dimensional tables with fixed columns and variable rows.\\rSQL language elements include Clauses, Expressions, Predicates, Queries, and Statements.\\rLearning SQL is valuable for various data science roles, such as business/data analysts and data engineers.\\rSQL enables direct access to data, speeding up workflow executions.\\rIt is an ANSI standard, allowing for portability across different databases.\\rVarious SQL databases are available, including MySQL, PostgreSQL, Oracle, etc.\\rSQL syntax may vary depending on the relational database management system being used.\\rFocus on learning SQL for a specific database and plug into the community for support.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp- Feb 2024\\rLanguage\\rDescription\\rPopular Tools/Frameworks\\rJava\\rGeneral-purpose object-oriented programming language. Widely adopted in the enterprise space.\\rWeka, Java-ML, Apache MLlib, Deeplearning4, Hadoop\\rScala\\rGeneral-purpose language supporting functional programming and strong static typing. Interoperable with Java.\\rApache Spark (including Shark, MLlib, GraphX, Spark Streaming)\\rC++\\rGeneral-purpose programming language, extension of C. Provides faster processing, system programming capabilities.\\rTensorFlow, MongoDB, Caffe\\rJavaScript\\rGeneral-purpose language extending beyond browsers with Node.js. Not related to Java.\\rTensorFlow.js, R-js\\rJulia\\rDesigned for high-performance numerical analysis and computational science. Produces programs as fast as C or Fortran.\\rJuliaDB\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rOpen Source\\rFree Software\\rChampioned by the Open-Source Initiative (OSI)\\rDefined by the Free Software Foundation (FSF)\\rMore business-focused\\rMore focused on a set of values\\rExamples: Python, Apache Spark, TensorFlow\\rExamples: R, GNU Compiler Collection (GCC)\\rFree to use\\rFree to use\\rOften uses the General Public License (GNU)\\rOften uses licenses endorsed by the FSF\\rSupports collaboration\\rSupports collaboration\\rCan be used interchangeably with free software in many cases\\r-\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rCategory\\rLibraries\\rScientific Computing Libraries (Python)\\r- Pandas- NumPy\\rVisualization Libraries (Python)\\r- Matplotlib- Seaborn\\rHigh-Level ML/DL Libraries (Python)\\r- Scikit-learn- Keras\\rDeep Learning Libraries (Python)\\r- TensorFlow- Pytorch\\rLibraries in Other Languages\\r- Apache Spark (Python, R, Scala, SQL)- Vegas (Scala)- BigDL (Scala)- ggplot2 (R)\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rLibrary\\rDefinition\\rBenefits\\rCommon Uses\\rPandas\\rPython library providing data structures and functions for efficient data manipulation and analysis.\\r- Offers DataFrame object for easy manipulation of structured data.- Provides tools for data cleaning, reshaping, and merging.- Supports handling of missing data and time-series data.- Integration with other Python libraries such as NumPy and Matplotlib.\\rData cleaning, manipulation, analysis, and preparation.\\rNumPy\\rPython library for numerical computing, providing support for large, multi-dimensional arrays and matrices.\\r- Efficient handling of large arrays and matrices.- Extensive mathematical functions for array manipulation and operations.- Integration with other Python libraries.- Supports linear algebra, Fourier transform, and random number generation.\\rScientific computing, numerical analysis, linear algebra, statistical analysis.\\rMatplotlib\\rPython library for creating static, interactive, and animated visualizations in Python.\\r- Wide range of plot types including line plots, bar charts, scatter plots, histograms, etc.- Highly customizable plot appearance.- Support for multiple data formats and output types.- Seamless integration with Pandas and NumPy.\\rData visualization, exploratory data analysis, presentation of research findings.\\rSeaborn\\rStatistical data visualization library based on Matplotlib, providing a high-level interface for creating attractive and informative plots.\\r- Simplifies complex visualizations with concise syntax.- Supports statistical plotting with built-in themes and color palettes.- Integration with Pandas DataFrames.- Additional plot types and features compared to Matplotlib.\\rStatistical data visualization, exploratory data analysis, presentation-ready plots.\\rScikit-learn\\rPython library providing simple and efficient tools for data mining and data analysis.\\r- Simple and consistent API for building and evaluating machine learning models.- Comprehensive documentation and user-friendly interface.- Wide range of algorithms for classification, regression, clustering, and dimensionality reduction.- Integration with other Python libraries such as Pandas and NumPy.\\rMachine learning algorithms development and evaluation, predictive modeling, data mining.\\rKeras\\rHigh-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit.\\r- Simplifies the construction, training, and evaluation of deep learning models.- User-friendly interface for building complex neural networks.- Supports both CPU and GPU acceleration.- Seamless integration with TensorFlow and other deep learning frameworks.- Extensive community support and documentation.\\rDeep learning model development, prototyping, experimentation, research, production deployment.\\rTensorFlow\\rOpen-source machine learning framework developed by Google Brain, capable of running on CPUs, GPUs, and TPUs.\\r- Scalable and flexible architecture suitable for a wide range of applications.- High-performance computation with support for distributed computing.- Extensive library ecosystem for machine learning and deep learning tasks.- Integration with other frameworks such as Keras and PyTorch.\\rDeep learning model development, research, production deployment, scalable machine learning tasks.\\rPyTorch\\rOpen-source deep learning framework developed by Facebook\\'s AI Research lab, emphasizing flexibility and ease of use.\\r- Dynamic computation graph allowing for dynamic neural network architectures.- Easier debugging and prototyping with Pythonic syntax.- Support for GPU acceleration and distributed training.- Large and active community contributing to the framework\\'s development and ecosystem.- Integration with other Python libraries and frameworks.\\rDeep learning model development, research, experimentation, prototyping, production deployment.\\rApache Spark\\rGeneral-purpose cluster computing framework designed for fast and efficient large-scale data processing.\\r- In-memory computing for rapid data processing.- Fault-tolerance and reliability in distributed computing environments.- Support for various programming languages including Python, Scala, Java, and SQL.- Integration with other big data tools and frameworks.- Scalability and performance optimization for large-scale data analytics.\\rLarge-scale data processing, data analytics, machine learning on big data, distributed computing.\\rVegas (Scala)\\rScala library for creating statistical data visualizations, providing an API for generating charts and plots from Spark DataFrames.\\r- Seamless integration with Spark DataFrames for data visualization on distributed datasets.- Support for various chart types and styles, including bar charts, histograms, and scatter plots.- Extensible and customizable chart configuration options.- Ability to work with both local and distributed data sources.\\rStatistical data visualization, exploratory data analysis, big data visualization.\\rBigDL (Scala)\\rDistributed deep learning library for Apache Spark, enabling deep learning on distributed datasets.\\r- Enables deep learning model training and inference directly on Spark clusters.- Supports a wide range of deep learning models and algorithms.- Integration with Spark MLlib and DataFrame APIs.- Scalability for training models on large-scale datasets.- Compatibility with existing Spark ecosystem tools and libraries.\\rDistributed deep learning model training, large-scale deep learning tasks, big data analytics with deep learning.\\rggplot2 (R)\\rR package for creating elegant and complex data visualizations, inspired by the Grammar of Graphics.\\r- Provides a flexible and powerful syntax for creating sophisticated plots and charts.- Extensive customization options for plot appearance and layout.- Seamless integration with R\\'s data manipulation and analysis capabilities.- Support for complex data structures and statistical transformations.\\r\\r\\r\\n\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTerm\\rDefinition\\rAPI\\rAllows communication between two pieces of software by providing an interface for inputs and outputs without exposing the backend details.\\rPandas\\rExample of a library with an API that allows communication with software components for data processing without knowledge of the backend.\\rTensorFlow\\rBackend written in C++ with APIs available for Python, JavaScript, C++, Java, and Go, allowing communication and utilization of TensorFlow\\'s functionalities in different programming languages.\\rREST API\\rStands for Representational State Transfer API, enabling communication over the internet to utilize resources such as storage, data, and algorithms.\\rClient\\rYour program, which communicates with the web service through the REST API.\\rResource\\rThe web service accessed via the REST API, providing functionalities or data.\\rEndpoint\\rThe location where the client accesses the web service to send requests and receive responses.\\rRequest\\rSent by the client to the resource via HTTP methods, containing instructions for the operation to be performed.\\rResponse\\rReturned by the resource to the client via HTTP methods, containing information or data requested by the client.\\rHTTP Message\\rUsed to transmit data over the internet between the client and the resource.\\rJSON\\rData format used in HTTP messages for requests and responses, containing instructions or information in a structured manner.\\rExample: Text to Speech API\\rConverts speech to text; client sends an audio file via a POST request, and the API returns the text transcription in a response via a GET request.\\rExample: Language Translator API\\rTranslates text from one language to another; client sends text to be translated, and the API returns the translated text.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rData Set Definition\\rA structured collection of data representing information in various formats such as text, numbers, images, audio, or video files. Tabular data sets are organized into rows and columns, often stored in formats like CSV (Comma Separated Values).\\rTypes of Data Ownership\\rTraditionally, data sets were private, containing proprietary or confidential information, and were not shared publicly. However, many entities now provide data sets as \"open data,\" making them available for free to the public.\\rSources of Data\\rData sets are sourced from various entities including scientific institutions, governments, organizations, and companies. Governments and organizations worldwide publish data sets on topics such as the economy, society, healthcare, transportation, and the environment.\\rCommunity Data License Agreement (CDLA)\\rCDLA was created by the Linux Foundation to address licensing concerns related to the distribution and use of open data. It includes two licenses: CDLA-Sharing and CDLA-Permissive. These licenses define terms for using and modifying data sets, with CDLA-Permissive allowing modifications without the requirement to share changes.\\rCDLA-Sharing License\\rAllows users to use and modify data sets, with the requirement to share any modifications under the same license terms as the original data.\\rCDLA-Permissive License\\rAllows users to use and modify data sets without the requirement to share changes. Users are not obligated to share modifications to the data.\\rImpact of Open Data on Data Science\\rOpen data has played a significant role in the growth of data science, machine learning, and artificial intelligence by providing practitioners with access to a wide range of data sets.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rData Asset eXchange (DAX)\\rIBM\\'s open data repository, providing a curated collection of high-quality open data sets from IBM Research and trusted third-party sources.\\rPurpose of DAX\\rTo provide developers with access to unique, high-quality data sets for use in enterprise applications.\\rData Sets Available on DAX\\rDAX offers various data sets, including images, video, text, and audio data.\\rCommunity Data License Agreement (CDLA)\\rData sets on DAX are made available under the CDLA to foster data sharing and collaboration, ensuring clear license and usage terms.\\rNotebook Tutorials\\rDAX provides tutorial notebooks that guide developers through data cleaning, pre-processing, exploratory analysis, and more. Some data sets include advanced notebooks for tasks such as creating charts, training machine learning models, and performing statistical and time-series analysis.\\rAccessing DAX\\rDevelopers can access DAX on the IBM Developer website under \"Open Source at IBM\" and then selecting \"Data Asset eXchange\" from the dropdown menu.\\rExploring Data Sets on DAX\\rDevelopers can explore various open data sets on DAX, such as the \"NOAA Weather Data - JFK Airport\" data set, which contains weather data from John F. Kennedy Airport in New York.\\rAccessing Notebooks in Watson Studio\\rNotebooks associated with data sets on DAX can be accessed and executed in Watson Studio, allowing developers to perform data cleaning, pre-processing, exploratory analysis, and more.\\rData Files Available on DAX\\rDAX provides access to one or more data files associated with each data set. Developers can view and access these data files to use in their projects.\\rIntegration with IBM Cloud and Watson Studio\\rDevelopers can log into their IBM Cloud account, create a project, and load DAX notebooks into the project in Watson Studio. Notebooks can be executed in Watson Studio for data analysis and exploration\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rMachine Learning (ML)\\rUses algorithms (models) to identify patterns in data.\\rModel Training\\rProcess by which the model learns data patterns.\\rTypes of Machine Learning\\r- Supervised Learning: Requires input data and correct outputs for training. Includes regression and classification models. - Unsupervised Learning: Analyzes unlabeled data to identify patterns and structure. Includes clustering models. - Reinforcement Learning: Learns through trial and error to maximize rewards.\\rSupervised Learning Types\\r- Regression Models: Predict numeric values. - Classification Models: Predict categories or classes.\\rDeep Learning\\rSpecialized type of ML that emulates human brain functioning. Used for analyzing natural language, images, audio, video, and more.\\rTraining Deep Learning Models\\rRequires large labeled datasets and is computationally intensive. Frameworks: TensorFlow, PyTorch, Keras. Pre-trained models available in model zoos.\\rBuilding a Model Example\\r- Collect and prepare data. - Label raw training data. - Build or select a model. - Train the model on prepared data. - Analyze training results and iterate. - Deploy the trained model.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rModel Asset eXchange (MAX)\\r- Free open-source repository for deep learning models. - Provides ready-to-use and customizable deep learning microservices. - Offers models tested and deployable in local and cloud environments. - Models available under permissive open-source licenses.\\rBenefits of Using Pre-trained Models\\r- Reduces time to value. - Saves resources, labor, and time required for training from scratch.\\rComponents of Model-serving Microservice\\r- Pre-trained deep learning model. - Input pre-processing code. - Output post-processing code. - Standardized public API.\\rDistribution of MAX Microservices\\r- Built and distributed as open-source Docker images on GitHub. - Can be customized for personal and commercial use.\\rDeployment Automation Platforms\\r- Kubernetes: Automates deployment, scaling, and management of Docker images. - Red Hat OpenShift: Enterprise-grade Kubernetes platform available on multiple cloud platforms.\\rExploring Predefined Models\\r- Visit ml-exchange.org to view and use predefined models. - Example: Object detector model.\\rUsing CodePen for Model Interaction\\r- CodePen: Online tool for editing front-end languages. - Allows interaction with MAX TensorFlow.js model for object detection.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rNotes\\rDefinition of Jupyter Notebooks\\r- Originally known as \"iPython,\" developed for Python programming.\\r\\r- Renamed Jupyter to support additional languages: Julia, Python, and R, among others.\\r\\r- Browser-based application allowing creation and sharing of documents with code, equations, visualizations, and narrative text.\\rFeatures of Jupyter Notebooks\\r- Combines descriptive text, code blocks, and code output in a single file.\\r\\r- Generates output, including plots and tables, within the notebook file.\\r\\r- Exportable to PDF or HTML formats for sharing.\\rIntroduction to JupyterLab\\r- Browser-based application for accessing multiple Jupyter Notebook files and other code/data files.\\r\\r- Extends functionalities of Jupyter Notebooks with features like multiple notebooks, text editors, terminals, etc.\\r\\r- Compatible with various file formats such as CSV, JSON, PDF, Vega, and others.\\r\\r- Open-source and supports integration with cloud-based services like IBM and Google Colab.\\rInstallation and Usage\\r- Can be installed via command line using pip install.\\r\\r- Can be downloaded locally through Anaconda Platform from Anaconda.com.\\r\\r- Anaconda distribution includes Jupyter and JupyterLab.\\r\\r- Hosted version available in Skills Network Labs, eliminating the need for local installations.\\r\\r\\n\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rActions\\rRunning, Inserting, Deleting\\r- Click \"Run\" button or use Shift + Enter shortcut to execute cells.\\rCells\\r- Add new cell with the plus symbol in the toolbar.\\r\\rDelete cell by clicking \"Edit\" on the main menu bar, then \"Delete Cells,\" or using shortcut D x2.\\rWorking with Multiple Notebooks\\r- Open new notebook with plus button or \"File\" > \"Open a new launcher\" or \"Open a new notebook.\"\\r\\r- Move notebooks, place side by side, or on separate tabs.\\rPresenting Results\\r- Use Markdown cells for titles and text descriptions.\\r\\r- Convert cells and outputs into slides for presentation.\\r\\r- Slides functionality delivers code, visualizations, text, and outputs as part of a project.\\rShutting Down Notebooks\\r- Click stop icon on the sidebar to shut down notebooks.\\r\\r- Terminate all sessions at once or shut down individually.\\r\\r- \"No kernel\" at top right indicates inactive session. Close tabs.\\r\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rDefinition of Kernel\\r- A computational engine executing code in a Notebook file.\\r\\rVarious Jupyter Kernels available for different languages.\\rWorking with Kernels\\r- When a Notebook opens, the related kernel launches automatically.\\r\\r- Kernels perform computations and produce results when Notebook is executed.\\r\\r- Some languages pre-installed in Skills Network lab environment: Python, Apache, Julia, R, Swift.\\r\\r- Select language for Data Science project upon notebook launch.\\r\\r- Python kernel enables execution of Python cells, producing output.\\r\\r- Other kernels available: Apache, Julia, R, Swift.\\r\\r- Select kernel on launch page or from dropdown menu in top right corner of Notebook.\\r\\r- Manually install languages through command line interface (CLI) on local machine if not pre-installed.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rAspect\\rDescription\\rJupyter Architecture\\r- Implements a two-process model with a kernel and a client.\\r\\r- Client: Interface (e.g., browser in Jupyter Notebook) for sending code to the kernel.\\r\\r- Kernel: Executes code and returns results to the client for display.\\rJupyter Notebooks\\r- Represent code, metadata, contents, and outputs.\\r\\r\\t- Saved from browser to Notebook server as JSON file with .ipynb extension.\\rNotebook Server\\r- Responsible for saving and loading notebooks.\\rKernel Execution\\r- Executes code cells in the Notebook when user runs them.\\rConversion Process\\r- Uses NBConvert tool to convert files to other formats.\\r\\rConversion involves preprocessor modifying notebook, exporter converting to new format, and postprocessor finalizing output.\\rExample\\r- Converting notebook file to HTML involves preprocessor, exporter, and postprocessor, resulting in an HTML file that can be displayed via its URL.\\r\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rAspect\\rDescription\\rComputational Notebooks\\r- Combine code, computational output, explanatory text, and multimedia resources into a single document.\\rJupyter Notebook\\r- Popular type of computational notebook supporting dozens of programming languages.\\rJupyterLab and VS Code\\r- Popular environments for creating and modifying Jupyter Notebooks on a local device.\\rJupyterLab\\r- Web-based application allowing creation of code, interactive visualizations, text, and equations. Includes extensive pre-installed Python libraries like NumPy, Pandas, and Matplotlib.\\rAnaconda\\r- Free and open-source distributor for Python and R, featuring over 1500 libraries. Supports data science and machine learning tasks. Offers free community support and Anaconda Navigator GUI.\\rAnaconda Navigator\\r- Graphical user interface facilitating installation of new packages without CLI. Allows launching JupyterLab and installing additional environments.\\rJupyter Notebook (Anaconda3)\\r- Command to start Jupyter Notebook in Anaconda environment.\\rJupyterLab Dashboard\\r- Accessed via localhost in browser. Manages Jupyter Notebooks.\\rNotebook Creation\\r- Create new Jupyter Notebook by selecting Python 3 from the dropdown menu. Rename notebook from \\'Untitled\\' to desired name.\\rCell Types\\r- Code cells contain code to be executed in kernel and display output. Markdown cells contain rich text and display formatted output.\\rDownloading Notebooks\\r- Access download options via File menu. Select desired download format.\\rVisual Studio Code (VS Code)\\r- Free, open-source code editor supporting debugging and task-running operations. Compatible with Linux, Windows, and macOS. Offers syntax highlighting, auto-indentation, and more.\\rExtensions for VS Code\\r- Install Python extensions to execute Python code in VS Code. Access extensions via Extensions menu or Ctrl + Shift + X keys. Search for \"Python\" and install related extensions.\\rUsing VS Code with Jupyter\\r- Open VS Code from Anaconda Navigator or download from code.visualstudio.com. Configure extensions to execute Python code. Write and execute code in Jupyter Notebook format.\\rSaving in VS Code\\r- Save files by navigating to File menu and selecting Save.\\r\\r\\n\\r\\nAspect\\rData Analysis\\rData Analytics\\rDefinition\\rExtracting meaning from data for decision-making.\\rCollection, analysis, and interpretation of data to support decision-making and goal achievement.\\rScope\\rNarrow, focused on extracting insights from data.\\rBroad, encompasses data collection, analysis, interpretation, and decision support.\\rFocus\\rUnderstanding past events and trends.\\rUnderstanding past events, predicting future outcomes, and prescribing actions.\\rMethods\\rQuantitative and qualitative data analysis techniques.\\rData gathering, preparation, exploration, management, storage, evaluation, and sharing of insights.\\rPurpose\\rFacilitating decision-making based on historical data.\\rInforming decision-making across various organizational functions and predicting future outcomes.\\rTypes\\rQuantitative analysis, qualitative analysis.\\rDescriptive, diagnostic, predictive, prescriptive analytics.\\rTechniques\\rHypothesis analysis, regression analysis, content analysis.\\rVisualization, statistical analysis, machine learning, optimization.\\rExamples\\rAnalyzing sales data to identify trends.\\rPredicting customer behavior to optimize marketing strategies.\\rGoal\\rGain insights from data to inform decisions.\\rImprove decision-making, enhance efficiency, and achieve organizational objectives.\\rImpact\\rImmediate insights for specific questions.\\rLong-term strategic planning and continuous improvement.\\rTools\\rSpreadsheet software, statistical software, data visualization tools.\\rBusiness intelligence platforms, advanced analytics tools, machine learning algorithms.\\r\\r\\n\\r\\nAspect\\rData Analysis\\rData Analytics\\rMeaning\\rExtracting insights from data for decision-making.\\rDevelopment of data pipelines from raw data ingestion to delivery of data products.\\rScope\\rConcentrated on characterizing data and producing predictions.\\rBroad, covering data collection, organization, cleaning, reporting, visualization, and delivery.\\rFocus\\rUnderstanding data patterns and trends.\\rManaging data pipelines and optimizing data processes.\\rRoles\\rData analysts.\\rData engineers for infrastructure and logistics, data analysts for analysis components.\\rSkills\\rStatistical analysis, modeling, SQL, visualization.\\rData storage, modeling, communication, documentation.\\rDifference\\rData analysis is a component of data analytics, but not vice versa.\\rAll data analysis is a component of data analytics, but not all analytics processes involve analysis.\\rTypes\\rDescriptive, diagnostic, predictive, prescriptive analysis.\\rBusiness intelligence, financial modeling, marketing research.\\rTechniques\\rStatistical analysis, modeling, SQL, visualization.\\rData preparation, statistical analysis, modeling, data visualization.\\rBenefits\\rEnhanced decision-making based on historical data.\\rImproved processes, future predictions, and management actions.\\rDescription\\rConcentrated activity within the data analytics pipeline.\\rComprehensive process involving various roles and skills in managing data pipelines.\\r\\r\\nCloud computing - Programming Ocean Academy\\rAspect\\rDescription\\rDefinition\\rDelivery of on-demand computing resources (networks, servers, storage, applications) over the Internet, typically on a pay-as-you-go basis.\\rUses\\r- Web-based applications (e.g., Google Drive, Dropbox)\\r\\n - Business software (e.g., Salesforce)\\r\\n- Data storage and backup\\r\\n- Hosting websites and applications\\rBenefits\\r- Cost savings (no need for expensive infrastructure)\\r\\n- Scalability (expand resources as needed)\\r\\n- Accessibility (access from any device with internet connection)\\rStrengths\\r- High flexibility and scalability\\r\\n- Reduces hardware costs\\r\\n- Collaboration through real-time access\\r\\n- Regular updates and maintenance by providers\\rWeaknesses\\r- Security risks (data breaches, hacking)\\r\\n- Dependency on internet connectivity\\r\\n- Potential for downtime\\r\\n- Limited control over infrastructure management\\r\\r\\nA comparison table between IaaS, PaaS, and SaaS - Programming Ocean Academy For Data Science and Artificial Intelligence\\rAspect\\rIaaS (Infrastructure as a Service)\\rPaaS (Platform as a Service)\\rSaaS (Software as a Service)\\rDefinition\\rProvides virtualized computing resources like servers, storage, and networking\\rProvides platforms for developers to build, test, and deploy applications\\rDelivers software applications over the internet, hosted by a provider\\rUser Responsibility\\rUsers manage the infrastructure (OS, storage, networking, etc.)\\rUsers manage applications and data, but the platform is maintained by the provider\\rUsers only manage software settings, with the provider managing infrastructure and platform\\rUse Case\\rBest for IT admins or companies needing complete control over their infrastructure\\rIdeal for developers needing a ready-to-use platform for building apps\\rBest for end-users requiring access to cloud-based software applications\\rExamples\\rAmazon Web Services (AWS), Microsoft Azure, Google Compute Engine\\rGoogle App Engine, Microsoft Azure PaaS, Heroku\\rGoogle Workspace (Gmail, Docs), Salesforce, Dropbox\\rScalability\\rHigh scalability, users can increase/decrease resources as needed\\rScalable development environment\\rSoftware scalability based on subscription or licensing\\rControl\\rHighest level of control over resources and infrastructure\\rModerate control over applications but less over the underlying infrastructure\\rLeast control, users focus on using the software without worrying about infrastructure\\rCost Model\\rPay for virtualized hardware (servers, storage)\\rPay for platform and tools (databases, middleware)\\rSubscription-based pricing for software access\\rAdvantages\\r- Full control over infrastructure\\r- Simplifies development\\r- Easy to use\\r\\r- Flexible\\r- Reduces time to deploy\\r- Accessible from anywhere\\r\\r- Customizable\\r- No need to manage underlying infrastructure\\r- No maintenance or installation required\\rDisadvantages\\r- Complex to manage\\r- Limited customization of platform\\r- Less control over software and its configuration\\r\\r- Requires specialized knowledge for configuration\\r- Less control over infrastructure\\r- Dependent on the provider for updates\\r\\r\\nThe five essential characteristics of cloud computing- Programming Ocean Academy\\rCharacteristic\\rDescription\\rOn-demand Self-service\\rUsers can access computing resources (e.g., processing power, storage, network) as needed, without requiring human intervention from the service provider.\\rBroad Network Access\\rCloud services can be accessed over the network from a wide range of devices such as smartphones, tablets, laptops, and desktops.\\rResource Pooling\\rComputing resources are pooled to serve multiple users using a multi-tenant model, with dynamic resource allocation based on demand. Users are unaware of locations.\\rRapid Elasticity\\rResources can be quickly scaled up or down to meet user demands, providing flexible and scalable resource provisioning.\\rMeasured Service\\rResource usage is monitored and measured, allowing users to be charged based on actual consumption, ensuring cost efficiency and transparency.\\r\\r\\n\\r\\nA comparison of the three cloud deployment models- Programming Ocean Academy\\rDeployment Model\\rDescription\\rAdvantages\\rDisadvantages\\rPublic Cloud\\rCloud services are provided over the public internet and shared among multiple organizations.\\rCost-effective, scalable, no need for in-house hardware, easy to access.\\rLess control over security, potential for data privacy concerns, shared resources with other organizations.\\rPrivate Cloud\\rCloud infrastructure is exclusively used by a single organization, either managed internally or by a third-party, hosted on-premises or off-site.\\rGreater control over security, customized for organizational needs, enhanced privacy.\\rHigher costs due to dedicated infrastructure, requires more management, less scalable than public cloud.\\rHybrid Cloud\\rCombines public and private clouds, allowing data and applications to be shared between them.\\rFlexibility, optimized resource utilization, allows sensitive data to remain in private cloud while leveraging the scalability of the public cloud.\\rComplex management, higher cost than pure public cloud, potential integration issues between public and private clouds.\\r\\r\\n\\r\\nA comparison of the three cloud service models- Programming Ocean Academy\\rService Model\\rDescription\\rKey Features\\rExamples\\rAdvantages\\rDisadvantages\\rInfrastructure as a Service (IaaS)\\rProvides virtualized computing resources over the internet, including servers, storage, and networking.\\rControl over infrastructure, pay-as-you-go model, scalable.\\rAmazon Web Services (AWS), Microsoft Azure, Google Cloud\\rFlexibility to manage and control infrastructure, scalability, cost-effective for large workloads.\\rRequires technical expertise to manage, higher responsibility for security, system updates, and maintenance.\\rPlatform as a Service (PaaS)\\rProvides a platform that allows developers to build, deploy, and manage applications without dealing with the underlying infrastructure.\\rPre-configured platform, application development tools, scaling.\\rGoogle App Engine, Heroku, Microsoft Azure App Services\\rReduces time for app development, easy deployment, no infrastructure management needed, scalable.\\rLimited control over infrastructure, less flexibility for certain configurations, potential vendor lock-in.\\rSoftware as a Service (SaaS)\\rDelivers software applications over the internet on a subscription basis, with the provider managing infrastructure, platform, and software.\\rHosted software, accessible via browser, no maintenance required.\\rGoogle Workspace, Salesforce, Dropbox\\rNo need for installation or updates, accessible from any device, predictable subscription costs, provider manages everything.\\rLimited customization, data security concerns, dependency on internet connectivity, less control over software updates/features.\\r\\r\\nA comparison table for the 5 Vs of Big Data- Programming Ocean Academy for Data Science and Artificial Intelligence\\rBig Data\\rDefinition\\rExample\\rChallenges\\rOpportunities\\rVelocity\\rThe speed at which data is generated and processed.\\rYouTube uploads hours of video every minute.\\rProcessing and analyzing data in real-time.\\rReal-time insights for fast decision-making.\\rVolume\\rThe large scale or quantity of data generated.\\r2.5 quintillion bytes of data are produced daily.\\rStoring and managing massive datasets.\\rAbility to extract valuable insights from large datasets.\\rVariety\\rThe diversity of data types (structured, unstructured).\\rText, images, videos, sensor data from IoT devices.\\rIntegrating and processing different types of data.\\rMore comprehensive analysis by combining various data types.\\rVeracity\\rThe accuracy and reliability of the data.\\r80% of data is unstructured, which may lack consistency.\\rEnsuring data quality and eliminating false information.\\rBetter decision-making with accurate data.\\rValue\\rThe ability to derive meaningful insights from data.\\rBusiness insights, customer behavior, risk management.\\rExtracting value from vast, complex datasets.\\rIncreased profit, efficiency, and innovation through data-driven decisions.\\r\\r\\n\\r\\n\\rTraditional and new way in Processing of Big Data- Programming Ocean Academy\\rAspect\\rTraditional Data Processing\\rBig Data Processing\\rBig Data Technologies\\rIntegration of Techniques\\rData Science and Analytics Growth\\rEmerging Trends\\rData Handling\\rData brought to the computer for processing.\\rData is sliced, distributed, and processed across multiple computers.\\rHadoop framework for big data.\\rCombines traditional methods (e.g., statistics) with new techniques.\\rRapid growth and increasing relevance.\\rEvolution of deep learning and neural networks.\\rProcessing Approach\\rProgram runs on the complete data set.\\rMapReduce: Map process distributes data, Reduce process aggregates results.\\rDeveloped from Google\\'s approach.\\rUse of machine learning to analyze large data sets.\\rData science field was less known five years ago.\\rSignificant advancements in recent years.\\rScalability\\rLimited by single computer capacity.\\rLinear scaling: Double the servers, double the performance/data handling.\\rHadoop scales with the data.\\rScalability achieved through integration of computational techniques.\\rData science gaining traction rapidly.\\rIncreased application by major tech companies.\\rTechnology Impact\\rStandard databases and programming techniques.\\rRevolutionary impact on data processing and handling.\\rMajor impact on big data management.\\rCombines traditional statistics with machine learning advances.\\rGrowing importance in business and technology.\\rDeep learning technology expanding rapidly.\\rField Evolution\\rTraditional techniques in use for decades.\\rNew techniques developed to handle large datasets (e.g., Hadoop).\\rOriginated from Google\\'s innovation.\\rModern computational capabilities enhance traditional methods.\\rSignificant development in the last few years.\\rRapid development in neural network capabilities.\\rApplications\\rLimited by the capabilities of traditional systems.\\rEnables analysis of vast and complex data sets.\\rUsed by many companies for big data.\\rApplied in diverse areas from business to research.\\rEmerging as a critical field in analytics.\\rUsed extensively by tech giants.\\r\\r\\n\\r\\nFeature\\rAdam\\rSGD (Stochastic Gradient Descent)\\rFull Name\\rAdaptive Moment Estimation\\rStochastic Gradient Descent\\rLearning Rate\\rAdaptive (updates the learning rate for each parameter)\\rFixed or manually scheduled (can decay over time)\\rUpdate Rule\\rCombines momentum (first moment) and adaptive learning rates (second moment)\\rDirectly updates the weights using the gradient and learning rate\\rMomentum\\rImplicitly incorporates momentum through first and second moments (biased corrections)\\rRequires separate manual momentum term (optional)\\rConvergence Speed\\rGenerally faster due to adaptive learning rates\\rSlower, especially when learning rate is not well-tuned\\rMemory Usage\\rRequires more memory (stores first and second moments)\\rLess memory-intensive (only stores gradients)\\rUse Cases\\rWorks well for noisy and sparse data, often used in deep learning\\rSuitable for simpler or well-tuned tasks, traditional machine learning\\rHyperparameters\\rRequires tuning \\rMainly the learning rate (and momentum if used)\\rBias Correction\\rYes, bias-corrected estimates of first and second moments\\rNo bias correction\\rSensitivity to Learning Rate\\rLess sensitive to the initial learning rate (due to adaptivity)\\rHighly sensitive; requires careful tuning\\rApplicability\\rEffective for a wide range of tasks, especially deep neural networks\\rSuitable for smaller models and simpler optimization tasks\\rStrengths\\rAdaptive learning rates; works well for large datasets and models\\rSimple and efficient for convex optimization problems\\rWeaknesses\\rMore computationally intensive, may overfit on certain problems\\rProne to slow convergence or getting stuck in local minima\\rTypical Use\\rDeep learning tasks, especially CNNs, RNNs, NLP, etc.\\rBasic gradient-based optimization, small models\\r\\r\\n\\r\\n\\rProgramming Ocean Academy for Data Science and AI\\rThe factors contributing to the cost of running a Large Language Model (LLM)\\rCost Factor\\rDescription\\rModel Size\\rLarger models with more parameters require more memory (RAM/VRAM) and computational resources.\\rHardware Costs\\rGPUs or TPUs for training/inference are expensive; infrastructure includes servers, cooling, and power.\\rEnergy Consumption\\rHigh energy use during training and inference increases operational costs.\\rTraining Costs\\rLong training times, massive datasets, and hyperparameter tuning raise compute and energy costs.\\rInference Costs\\rQuery complexity, batch size, and low-latency requirements affect resource usage during real-time inference.\\rCloud Hosting vs. On-Premise\\rCloud costs include compute hours, storage, and scaling; on-premise costs cover infrastructure and maintenance.\\rSoftware & Licensing Costs\\rAdditional costs for optimization tools, distributed training, and API usage.\\rOperational & Maintenance Costs\\rIncludes model updating, retraining, and staffing costs for engineers and data scientists.\\rSecurity & Compliance\\rEncryption, secure storage, and compliance with data privacy regulations add to the operational overhead.\\rNetworking & Bandwidth\\rData transfer, especially across regions or clouds, and ensuring low-latency response times increase costs.\\r\\r\\nThe major four types in the Machine learning algorithm-Programming Ocean Academy\\rAspect\\rClassification\\rRegression\\rClustering\\rDimensionality Reduction\\rPurpose\\rCategorize data into discrete classes\\rPredict continuous values\\rGroup similar data points without labels\\rReduce the number of features while preserving the variance\\rKey Algorithms\\r\\n \\r\\n \\r\\n \\r- SVC\\r- Lasso\\r- KMeans\\r- PCA\\r\\r- KNeighbors\\r- ElasticNet\\r- Spectral Clustering\\r- Isomap\\r\\r- Naive Bayes\\r- Ridge\\r- GMM\\r- LLE\\r\\r- SGD Classifier\\r- SVR\\r \\r- Spectral Embedding\\rSample Size Considerations\\r\\n \\r- Works with both small (<100K) and large datasets\\r- For smaller datasets (<100K), suitable methods like SGD Regressor\\r- KMeans is good for small datasets (<10K)\\r- Works with smaller datasets (<10K) with algorithms like PCA, Isomap\\r\\r \\r- SVR for small and medium datasets\\r- MiniBatch KMeans for large datasets\\r \\rLabeled Data Requirement\\rRequires labeled data for training\\rRequires labeled data (features and target values)\\rDoes not require labeled data\\rTypically unsupervised; does not need labels\\rTypes of Predictions\\rPredict categories (e.g., spam or not spam)\\rPredict continuous quantities (e.g., house price)\\rFind groupings in data (e.g., customer segments)\\rTransform data to lower dimensions (e.g., visualize high-dimensional data)\\rCommon Challenges\\r\\n \\r- Choosing between linear and non-linear classifiers\\r- Overfitting with complex models\\r- Determining the optimal number of clusters\\r- Loss of information when reducing dimensions\\r\\r- Handling imbalanced data\\r- Feature importance\\r- Scalability with large data\\r- Choosing the correct number of components\\rKey Decision Factors\\r\\n \\r\\n \\r- Size of dataset\\r- Importance of features\\r- Number of clusters\\r- Number of features\\r\\r- Text data suitability\\r- Number of samples\\r- Size of dataset\\r- Variance preservation\\r\\r- Number of classes\\r- Whether features are correlated\\r- Number of known categories\\r- Computational efficiency\\rKey Use Cases\\r\\n \\r\\n \\r- Image or text classification\\r- Predicting stock prices\\r- Market segmentation\\r- Data visualization\\r\\r- Spam detection\\r- Forecasting sales\\r- Social network analysis\\r- Preprocessing for other algorithms\\r\\r- Disease diagnosis\\r- House price prediction\\r- Image segmentation\\r- Noise reduction\\r\\r\\n\\r\\nDimensionality Reduction techniques- Programming Ocean Academy\\rTechnique\\rDescription\\rUses\\rPros\\rCons\\rIdeal Scenarios\\rPrincipal Component Analysis (PCA)\\r\\n \\r\\n \\rA linear technique that projects data onto lower-dimensional spaces while maximizing variance.\\r\\n \\r\\n \\r- Data compression\\r\\n- Visualization\\r\\n- Noise reduction\\r- Simple to implement\\r\\n- Good for large datasets\\r\\n \\r- Assumes linearity\\r\\n- Can lose interpretability\\r\\n \\r- When the data has a large number of features with redundancy or correlation\\r\\n \\r\\n \\rLinear Discriminant Analysis (LDA)\\r\\n \\rProjects data onto lower dimensions while maximizing the separation between classes.\\r\\n \\r- Feature extraction for classification problems\\r\\n \\r- Improves class separation\\r\\n- Computationally efficient\\r- Only works for labeled data\\r\\n- Assumes Gaussian distribution\\r- When class separation is important for classification tasks\\r\\n \\rKernel PCA\\r\\n \\rNon-linear version of PCA using kernel methods to capture non-linear relationships.\\r\\n \\r- Non-linear feature reduction\\r\\n- Preprocessing for SVMs\\r- Captures complex relationships\\r\\n- Flexible for non-linear data\\r- Computationally expensive\\r\\n- Requires careful choice of kernels\\r- When the dataset has complex, non-linear relationships and patterns\\r\\n \\rt-Distributed Stochastic Neighbor Embedding (t-SNE)\\r\\n \\rNon-linear technique that preserves the local structure of data and is mainly used for visualization.\\r\\n \\r- Data visualization\\r\\n- Exploring high-dimensional data\\r- Excellent for visualizing complex, high-dimensional data\\r\\n \\r- Computationally expensive\\r\\n- Not suitable for large datasets\\r- For visualizing complex datasets in 2D or 3D, such as in exploratory data analysis\\r\\n \\rIsomap\\r\\n \\rA manifold learning technique that seeks to preserve the global structure of the data.\\r\\n \\r- Non-linear dimensionality reduction\\r\\n- Visualizing manifolds\\r- Captures global structure\\r\\n- Effective for manifold data\\r- Struggles with noise\\r\\n- Computationally expensive\\r- When the data lies on a low-dimensional manifold within a high-dimensional space\\r\\n \\rLocally Linear Embedding (LLE)\\r\\n \\rA manifold learning algorithm that preserves local neighborhoods in lower-dimensional projections.\\r\\n \\r- Non-linear dimensionality reduction\\r\\n- Visualization\\r- Preserves local structures\\r\\n- Effective for high-dimensional data\\r- Sensitive to noise\\r\\n- Does not preserve global structure\\r- When the goal is to preserve local relationships in high-dimensional data\\r\\n \\rFactor Analysis (FA)\\r\\n \\rA statistical method that models observed variables as linear combinations of potential latent factors.\\r\\n \\r- Feature extraction\\r\\n- Data exploration\\r- Simplifies data interpretation\\r\\n- Good for correlated variables\\r- Assumes linear relationships\\r\\n- Interpretability can be complex\\r- When the objective is to identify underlying factors explaining correlations between features\\r\\n \\rAutoencoders\\r\\n \\r\\n \\rNeural network-based technique that learns efficient representations of data.\\r\\n \\r\\n \\r- Feature learning\\r\\n- Image compression\\r\\n- Denoising\\r- Can model complex non-linear functions\\r\\n- Highly customizable\\r\\n \\r- Requires a lot of data and tuning\\r\\n- Computationally expensive\\r\\n \\r- When working with large datasets and non-linear data, especially in deep learning contexts\\r\\n \\r\\n \\rIndependent Component Analysis (ICA)\\r\\n \\rA technique that separates a multivariate signal into additive, independent components.\\r\\n \\r- Blind source separation\\r\\n- Signal processing\\r- Finds independent features\\r\\n- Good for non-Gaussian data\\r- Assumes statistical independence\\r\\n- Sensitive to noise\\r- When the task is to extract independent signals or sources, such as in audio processing\\r\\n \\rMultidimensional Scaling (MDS)\\r\\n \\rA non-linear technique that projects data into lower dimensions by preserving pairwise distances.\\r\\n \\r- Visualizing similarity or dissimilarity between data points\\r\\n \\r- Preserves distances\\r\\n- Effective for dissimilarity data\\r- Computationally expensive\\r\\n- Limited scalability\\r- When preserving the distance between data points is important, such as in perceptual mapping\\r\\n \\rRandomized PCA\\r\\n \\rA faster approximation of PCA for large datasets that uses random projections.\\r\\n \\r- Fast data compression\\r\\n- Initial feature reduction\\r- Efficient for very large datasets\\r\\n- Retains most variance\\r- Only an approximation of true PCA\\r\\n- Loss of precision\\r- When computational resources are limited and the dataset is extremely large\\r\\n \\rTruncated Singular Value Decomposition (SVD)\\r\\n \\r\\n \\rA matrix factorization technique used for dimensionality reduction, particularly in sparse data.\\r\\n \\r\\n \\r- Sparse data\\r\\n- Text analysis\\r\\n- Matrix factorization\\r- Handles sparse data well\\r\\n- Useful for text data\\r\\n \\r- Loses interpretability\\r\\n- Assumes linear relationships\\r\\n \\r- When dealing with sparse data, such as text (e.g., TF-IDF matrices)\\r\\n \\r\\n \\r\\r\\n\\r\\nClustering Algorithms Table- Programming Ocean Academy\\rAlgorithm\\rUse Case\\rKMeans\\rGeneral-purpose clustering for datasets where clusters are spherical in shape.\\rMiniBatch KMeans\\rFor large-scale datasets with performance constraints, a faster version of KMeans.\\rGaussian Mixture (GMM)\\rWhen clusters have varying sizes and elliptical shapes, and you want probabilistic cluster membership.\\rSpectral Clustering\\rUseful for non-convex clusters, e.g., graph-based clustering or when clusters are not easily separable.\\rMeanShift\\rFinds the center of clusters, good for data with an unknown number of clusters and arbitrarily shaped clusters.\\rAgglomerative Clustering\\rHierarchical clustering, effective for small datasets where relationships between data points are important.\\rDBSCAN\\rBest for data with noise, capable of finding clusters of varying shapes and sizes without specifying the number of clusters.\\rBirch\\rEfficient clustering method for large datasets, works well with data that is incrementally added.\\rAffinity Propagation\\rAutomatically determines the number of clusters based on message-passing between data points.\\rSpectral Embedding\\rDimensionality reduction method that can be used before clustering algorithms for better performance on structured data.\\rVBGMM (Variational Bayesian Gaussian Mixture Model)\\rProbabilistic clustering method where the number of clusters is not fixed; suitable for uncertain data.\\r\\r\\nA comprehensive comparison between Data Science, Data Analysis, and Data Engineering- Programming Ocean Academy\\rAspect\\rData Science\\rData Analysis\\rData Engineering\\rDefinition\\rThe field focused on extracting meaningful insights and predictions from large datasets using advanced statistical, machine learning, and AI techniques.\\rThe process of inspecting, cleaning, transforming, and modeling data to discover useful information and support decision-making.\\rThe practice of building and maintaining the infrastructure (pipelines, databases, etc.) that enables the collection, storage, and processing of large datasets for analysis and machine learning.\\rKey Responsibilities\\r\\n \\r\\n \\r\\n \\r\\n \\r- Develop machine learning models and algorithms.\\r\\n- Perform predictive analytics and AI model training.\\r\\n- Perform experimental design for A/B testing.\\r\\n- Extract actionable insights for business or research.\\r\\n- Communicate findings to stakeholders.\\r- Query, clean, and visualize datasets.\\r\\n- Conduct descriptive analysis of current and historical trends.\\r\\n- Build dashboards and reports.\\r\\n- Identify patterns and relationships in data.\\r\\n- Help decision-makers understand the implications of data.\\r- Design and build data pipelines.\\r\\n- Manage data storage systems (e.g., data warehouses, lakes).\\r\\n- Ensure data quality and availability.\\r\\n- Implement ETL (Extract, Transform, Load) processes.\\r\\n- Support data analysts and data scientists with clean and usable datasets.\\rGoal\\rGenerate predictions and insights using data-driven models, solving complex business problems with advanced analytics, and creating innovative solutions.\\rHelp businesses or researchers understand current data trends, identify key insights, and guide decisions with clear, accurate reports based on data.\\rEnsure that data is available, accessible, and organized properly, enabling smooth analysis and data science processes. Provide a solid infrastructure for handling large-scale datasets.\\rSkills Required\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r- Statistical analysis\\r\\n- Machine learning & AI\\r\\n- Programming (Python, R, SQL)\\r\\n- Data wrangling\\r\\n- Advanced mathematics\\r\\n- Data visualization\\r\\n- Knowledge of cloud platforms (AWS, Azure, GCP)\\r\\n- Model evaluation & optimization techniques\\r- Strong analytical skills\\r\\n- Data cleaning & transformation\\r\\n- Data visualization (e.g., Tableau, Power BI, Excel)\\r\\n- SQL querying\\r\\n- Basic statistical knowledge\\r\\n- Communication and reporting skills\\r\\n \\r\\n \\r- Programming (Python, Java, Scala, SQL)\\r\\n- Database design and management\\r\\n- ETL tools (e.g., Apache Airflow, Apache NiFi)\\r\\n- Cloud data storage (e.g., AWS Redshift, Google BigQuery)\\r\\n- Big data tools (Hadoop, Spark)\\r\\n- Data pipeline automation and orchestration\\r\\n \\r\\n \\rCommon Tools\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r- Jupyter Notebook\\r\\n- Python (pandas, NumPy, scikit-learn, TensorFlow, PyTorch)\\r\\n- R\\r\\n- SQL\\r\\n- Hadoop, Spark\\r\\n- Visualization tools (Matplotlib, Seaborn, Plotly, etc.)\\r\\n- Cloud computing platforms\\r\\n- GitHub, Docker\\r- Excel\\r\\n- SQL\\r\\n- Tableau, Power BI\\r\\n- Python (pandas, Matplotlib)\\r\\n- Google Analytics\\r\\n- Business Intelligence (BI) tools\\r\\n- SAS\\r\\n- R\\r- SQL databases (PostgreSQL, MySQL, Oracle)\\r\\n- NoSQL databases (MongoDB, Cassandra)\\r\\n- ETL tools (Apache Airflow, Talend)\\r\\n- Apache Hadoop, Apache Spark\\r\\n- Cloud platforms (AWS, Google Cloud, Azure)\\r\\n- Data orchestration tools (Kubernetes, Docker)\\r\\n \\r\\n \\rOutput\\rPredictive models, AI-driven insights, recommendations, optimized algorithms for decision-making, automation systems, or research findings.\\rReports, dashboards, KPIs, descriptive insights, and visual representations of data to guide business decisions.\\rOptimized, structured, and processed datasets, data pipelines, data lakes, and data warehouses ready for analysis or modeling.\\rEducational Background\\r\\n \\r\\n \\r- Advanced degrees (Master\\'s or PhD) in Computer Science, Statistics, Mathematics, or related fields.\\r\\n- Specialized certifications in data science and machine learning.\\r\\n- Strong theoretical knowledge in AI and ML concepts.\\r- Bachelor\\'s degree in statistics, economics, or a business-related field.\\r\\n- Certifications in analytics tools (Tableau, Power BI).\\r\\n- Basic understanding of statistical techniques and tools.\\r- Bachelor\\'s or Master\\'s degree in Computer Science, Engineering, or related fields.\\r\\n- Certifications in big data technologies, cloud data management (AWS, Google Cloud, etc.), and ETL systems.\\r\\n- Strong software engineering background.\\rMain Challenges\\r\\n \\r\\n \\r\\n \\r\\n \\r- Dealing with unstructured and large datasets.\\r\\n- Model overfitting/underfitting.\\r\\n- Interpreting complex algorithms.\\r\\n- Keeping up with fast-evolving AI and ML technologies.\\r\\n- Communication of complex models to non-technical stakeholders.\\r- Data quality issues (incomplete or inaccurate data).\\r\\n- Visualizing complex data in understandable ways.\\r\\n- Avoiding bias in data interpretation.\\r\\n- Ensuring that reports align with business objectives.\\r\\n- Managing diverse data sources.\\r- Managing big data storage and retrieval at scale.\\r\\n- Ensuring data security and integrity.\\r\\n- Dealing with data silos and complex data architectures.\\r\\n- Real-time data processing and ensuring low latency in large-scale systems.\\r\\n \\rCareer Path\\rData Scientist, AI/ML Engineer, Research Scientist, Business Intelligence Architect.\\rData Analyst, Business Analyst, BI Analyst, Operations Analyst.\\rData Engineer, Data Architect, Database Administrator, ETL Developer, Big Data Engineer.\\rAverage Salary\\r$120,000 - $160,000 per year (US).\\r$60,000 - $90,000 per year (US).\\r$110,000 - $150,000 per year (US).\\rIndustry Demand\\rHigh demand due to AI and automation trends. Especially sought after in tech, finance, healthcare, and research fields.\\rStrong demand in industries requiring reporting, trend analysis, business insights, e-commerce, and finance.\\rHigh demand in industries that rely on big data processing, cloud infrastructure, and data-driven decision-making, including tech, finance, logistics, and retail.\\r\\r\\n\\r\\n\\r\\nA comparison between Data Analysis, Data Visualization, and Data Modeling- Programming Ocean Academy\\rAspect\\rData Analysis\\rData Visualization\\rData Modeling\\rDefinition\\rThe process of inspecting, cleaning, transforming, and interpreting data to discover patterns, trends, and insights to support decision-making.\\rThe graphical representation of data using visual elements like charts, graphs, and maps to help stakeholders easily understand complex datasets.\\rThe process of defining and organizing the structure of a dataset, typically by creating relationships between different data entities to represent business processes.\\rKey Responsibilities\\r- Analyze data to find trends, correlations, and anomalies.\\r\\n- Perform statistical analysis and hypothesis testing.\\r\\n- Identify actionable insights.\\r- Design and create visual representations of data.\\r\\n- Transform raw data into meaningful charts and graphs.\\r\\n- Present data clearly and effectively to stakeholders.\\r- Design the structure of data systems (relational, hierarchical, or network models).\\r\\n- Define data entities, attributes, and relationships.\\r\\n- Optimize databases for usage.\\rGoal\\rExtract meaningful insights from raw data to inform decision-making and help businesses make data-driven decisions.\\rConvey insights, trends, and patterns through visual elements to facilitate better understanding and quicker decision-making by non-technical stakeholders.\\rCreate a structured framework for how data is stored, accessed, and used within systems to ensure consistency, accuracy, and scalability.\\rSkills Required\\r- Statistical knowledge (hypothesis testing, regression, etc.).\\r\\n- Data cleaning and preprocessing.\\r\\n- Proficiency in querying (SQL).\\r\\n- Python/R for analysis.\\r- Knowledge of design principles (color, layout, clarity).\\r\\n- Proficiency in visualization tools.\\r\\n- Understanding of different chart types and when to use them.\\r- Database design (ER diagrams, schema design).\\r\\n- Knowledge of SQL, NoSQL.\\r\\n- Data normalization and optimization techniques.\\r\\n- Entity-relationship modeling (ERMs).\\rCommon Tools\\r- Excel\\r\\n- SQL\\r\\n- Python (pandas, NumPy)\\r\\n- R\\r\\n- SAS\\r\\n- Google Analytics\\r- Tableau\\r\\n- Power BI\\r\\n- Matplotlib, Seaborn (Python)\\r\\n- Excel\\r\\n- Google Data Studio\\r- SQL databases (PostgreSQL, MySQL)\\r\\n- ER diagram tools (Lucidchart, draw.io)\\r\\n- Data modeling tools (Erwin, IBM InfoSphere Data Architect)\\r\\n- NoSQL databases (MongoDB).\\rOutput\\rStatistical reports, data summaries, trend analyses, dashboards, and key performance indicators (KPIs).\\rCharts, graphs, dashboards, infographics, and interactive visuals that clearly display key metrics, patterns, and trends.\\rEntity-relationship diagrams (ERD), schema definitions, database tables, and a structured data model for further analysis or system usage.\\rFocus\\rUnderstanding the \"what\" and \"why\" behind the data, performing both descriptive and inferential analysis.\\rSimplifying complex data and presenting it visually to communicate insights in a more digestible format to a broad audience.\\rStructuring the \"how\" data will be stored, organized, and related, creating a clear blueprint for efficient and accurate data management and usage.\\rEnd Users\\rData analysts, decision-makers, business strategists, and researchers who need to make informed decisions based on historical and real-time data.\\rBusiness executives, product managers, marketing teams, and non-technical stakeholders who need to quickly grasp trends, insights, and data-driven stories.\\rData engineers, database administrators, developers, and architects who implement and maintain the data structure in databases or data warehouses.\\rOutput Examples\\r- Descriptive statistics (mean, median, mode).\\r\\n- Correlation analysis.\\r\\n- Trend analysis over time.\\r\\n- Predictive models (e.g., forecasting future trends).\\r- Line charts for trends.\\r\\n- Bar charts for comparisons.\\r\\n- Pie charts for proportions.\\r\\n- Heatmaps for density.\\r\\n- Interactive dashboards for real-time data.\\r- ER diagrams to represent relationships.\\r\\n- Database schema definition (tables, columns).\\r\\n- Logical and physical data models.\\r\\n- Data normalization schemas.\\rMain Challenges\\r- Dealing with incomplete or messy datasets.\\r\\n- Misinterpretation of statistical results.\\r\\n- Ensuring that insights are actionable and relevant to the business goals.\\r- Choosing the right type of visualization for the data.\\r\\n- Ensuring that visualizations are not misleading.\\r\\n- Simplifying complex data without losing key insights.\\r- Ensuring that the data model is scalable, consistent, and supports all business operations.\\r\\n- Handling large, complex datasets while optimizing performance.\\rCareer Path\\rData Analyst, Business Analyst, Financial Analyst, Operations Analyst.\\rData Visualization Specialist, BI Analyst, Dashboard Designer, UX/UI Data Visualization Designer.\\rData Modeler, Database Architect, Data Engineer, Data Warehouse Architect.\\rEducational Background\\rTypically a Bachelor\\'s in Statistics, Computer Science, Economics, or Business, often supplemented by data analysis certifications.\\rA background in Data Analytics, Computer Science, Information Design, or Graphic Design, with an emphasis on communication and data visualization tools.\\rDegrees in Computer Science, Information Systems, or Database Management, with specializations in database design and data architecture.\\rIndustry Demand\\rStrong demand across industries that need actionable insights from data to improve operations, marketing, and finance.\\rGrowing demand, especially in businesses that require intuitive data dashboards and visuals to inform decision-making in areas like marketing, sales, and product development.\\rHigh demand in industries with complex data architectures and systems, such as tech, finance, logistics, and e-commerce, where data management is critical to operations.\\r\\r\\n\\r\\n\\r\\nA Convolutional Neural Network (CNN)- Programming Ocean Academy\\rStage\\rDescription\\rInput Image\\rThe network starts with an input image, and the goal is to classify or identify objects or patterns within the image.\\rConvolution\\rA small filter (called a kernel) moves over the image, looking at small sections at a time. It detects basic patterns like edges, textures, or color gradients.\\rFeature Map\\rThe output of the convolution operation is a feature map, which highlights important features from the image in a simplified, spatially structured format.\\rPooling\\rPooling reduces the size of the feature map by summarizing the most important information, helping the model become more efficient and less sensitive to small variations.\\rPooled Feature Map\\rThe result after pooling is a down-sampled version of the feature map that retains the essential characteristics of the image.\\rConvolution & Pooling (again)\\rThese steps are repeated multiple times to extract more complex and higher-level features from the image.\\rFlatten Layer\\rThe pooled feature maps are flattened into a one-dimensional vector, which can be passed into the next layers for further processing or classification.\\rFully Connected Layer\\rThe flattened feature vector is connected to a fully connected layer, where all features are combined and analyzed to make a final prediction.\\rOutput\\rThe network outputs a set of probabilities for each possible class. The class with the highest probability is selected as the model\\'s prediction.\\r\\r\\n\\r\\nAspect\\rConvolutional Layers\\rFlatten Layers\\rFully Connected Layers\\rDefinition\\rPerforms a convolution operation by sliding filters (kernels) over the input to extract features.\\rConverts multi-dimensional arrays (e.g., 2D or 3D) into a 1D vector.\\rA traditional neural network layer where every neuron is connected to every neuron in the previous layer.\\rPrimary Function\\rExtracts local features like edges, textures, and patterns from the input data, preserving spatial relationships.\\rPrepares multi-dimensional data (e.g., feature maps from convolutional layers) for input into fully connected layers.\\rCombines all features to make final predictions or decisions, performing global feature aggregation.\\rInput Shape\\rMulti-dimensional input (e.g., 2D for images: HÃ—WH \\\\times WHÃ—W, 3D with channels: HÃ—WÃ—CH \\\\times W \\\\times CHÃ—WÃ—C).\\rMulti-dimensional input (2D, 3D, etc.), flattened into a 1D vector.\\r1D vector as input (output from the Flatten layer or a previous fully connected layer).\\rOutput Shape\\rMulti-dimensional output (reduced height and width after convolution, but retains multiple channels).\\r1D vector (flattened representation of the input).\\r1D vector of neuron activations, size depends on the number of neurons in the layer.\\rUse Case\\rFeature extraction, especially in image and video processing tasks.\\rInterface between convolutional layers and fully connected layers.\\rFinal classification, regression, or prediction tasks in deep learning models.\\rParameters\\rSmall number of parameters (weights are shared across the input using kernels).\\rNo parameters-just reshapes the data.\\rLarge number of parameters, as each neuron connects to every input from the previous layer (dense connections).\\rSpatial Information\\rPreserves spatial relationships (position and structure) in the input data.\\rDiscards spatial information and reshapes data into a flat array.\\rDoes not preserve spatial relationships, instead focuses on global feature representation.\\rCommon Applications\\r\\n \\r- CNNs for image classification and object detection.\\r\\n- NLP with text representations (e.g., 1D convolutions).\\rTransitioning from feature maps (from CNNs) to fully connected layers for final classification.\\r\\n \\r- Final prediction in CNNs.\\r\\n- Used in classical neural networks for classification or regression.\\rRole in the Network\\rDetects low-level to high-level features hierarchically by applying multiple filters.\\rPrepares the extracted features for classification or prediction by reshaping them.\\rCombines all features and performs the final decision-making (output layer in classification/regression).\\rParameter Sharing\\rYes, weights are shared across the input space to reduce the number of parameters.\\rNo sharing simply reshapes data.\\rNo parameter sharing; each neuron has its own set of weights, leading to a large number of parameters.\\rComputation Cost\\rGenerally lower compared to fully connected layers due to weight sharing, but can be high for deep layers.\\rVery low, as it only reshapes the input data.\\rHigh computational cost due to dense connections (each neuron connected to all inputs).\\rExample\\r3x3 convolution filter applied to an image to detect edges.\\rFlattening a 32Ã—32Ã—3 times feature map into a vector of size 3072.\\rFully connected layer with 1000 neurons receiving input from a flattened vector.\\r \\r\\n\\r\\nDense (Fully Connected) Layers and Convolutional Layers - Programming Ocean Academy\\rAspect\\rDense (Fully Connected) Layers\\rConvolutional Layers\\rStructure\\rEvery neuron is connected to every other neuron in the next layer.\\rOnly a subset of neurons (kernel or filter) is connected to the input data, sliding over it spatially.\\rInput Shape\\rFlattened vector (e.g., a 1D array of pixel values from an image).\\r2D or 3D data (e.g., images with height, width, and channels).\\rWeights\\rEach connection has its own weight, leading to a large number of parameters.\\rShared weights within a kernel/filter across spatial locations.\\rNumber of Parameters\\rLarger number of parameters as each input is connected to every output.\\rFewer parameters due to weight sharing (e.g., a 3x3 filter for an entire image).\\rPrimary Function\\rLearns global features by treating the entire input as a whole.\\rLearns local features like edges, textures, or patterns in small regions.\\rDimensionality\\rTypically used for 1D input/output (after flattening).\\rWorks with 2D (e.g., image data) or 3D (e.g., video data).\\rActivation Functions\\rUsually followed by a non-linear activation like ReLU or sigmoid.\\rOften followed by an activation function like ReLU, and pooling layers for downsampling.\\rUse Cases\\rUsed in final classification or regression tasks.\\rUsed for feature extraction, especially with spatial data like images or video.\\rInterpretability\\rHarder to interpret; no clear link between input space and learned features.\\rMore interpretable; filters can reveal patterns like edges or textures learned from the input.\\rComputational Efficiency\\rLess efficient due to large number of parameters and lack of spatial awareness.\\rMore efficient due to local connections and weight sharing, which reduces computational cost.\\rData Requirements\\rRequires flattened data (2D images need to be reshaped to 1D vectors).\\rDirectly works with structured input like images, preserving spatial relationships.\\rExample Usage\\rOften used in final layers of a model to output probabilities or scores.\\rUsed in early layers for extracting spatial features from images or signals.\\r\\r\\nConvolutional Layer, Pooling Layer, Flatten Layer, and Dense Layer - Programming Ocean Academy\\rFeature\\rConvolutional Layer (Conv2D)\\rPooling Layer (MaxPooling2D)\\rFlatten Layer\\rDense Layer (Fully Connected Layer)\\rPurpose\\rExtracts features from the input by using filters (kernels) to scan and detect patterns like edges and textures.\\rReduces the spatial dimensions by downsampling, improving computational efficiency and reducing overfitting.\\rConverts a multidimensional tensor into a 1D vector to prepare it for fully connected (dense) layers.\\rProcesses extracted features to classify inputs or make predictions, where each neuron connects to all neurons in the previous and next layers.\\rInput/Output Shape\\rInput: 3D (height, width, channels)\\rInput: 3D (height, width, channels)\\rInput: 3D (height, width, channels)\\rInput: 1D vector (from flatten layer)\\r \\rOutput: Reduced 3D (height, width, filters)\\rOutput: Further reduced 3D (height, width, channels)\\rOutput: 1D vector\\rOutput: 1D vector (number of neurons in the dense layer)\\rKey Operations\\rApplies filters that convolve across the input. Uses strides, padding, and activation functions (e.g., ReLU).\\rPerforms max-pooling or average-pooling to summarize information within regions (e.g., 2x2 pooling).\\rFlattens the output of the previous layer by collapsing spatial dimensions into a single vector.\\rApplies a weighted sum of inputs and biases, followed by an activation function (e.g., ReLU, softmax) to make predictions.\\rParameters\\rNumber of filters, filter size (kernel), strides, activation function, padding\\rNo trainable parameters (just reduces input dimensions).\\rNo trainable parameters (reshapes data).\\rNumber of neurons, activation function, weights, and biases\\rCommon Use Cases\\rFeature extraction from images, audio, or spatial data, commonly used in CNNs for tasks like image recognition.\\rTypically used after convolutional layers to reduce spatial dimensions while preserving important features.\\rActs as a bridge between convolutional/pooling layers and dense layers, preparing the data for final classification or regression tasks.\\rFinal decision-making layer(s) of a model, typically used after flattening in CNNs. Responsible for classification or regression.\\r\\r\\nComparison of Different CNN Architectures and Their Use Cases Part One- Programming Ocean Academy\\rType\\rDescription\\rUse Case\\rExample\\rStandard CNN (Sequential CNN)\\rBasic form of CNN where layers are stacked in sequence. Includes Convolutional, Pooling, Flatten, and Dense Layers.\\rImage classification, object detection\\rLeNet-5 for digit classification\\rFully Convolutional Networks (FCN)\\rReplaces fully connected layers with convolutional layers to handle varying input sizes. Outputs pixel-wise predictions.\\rImage segmentation, semantic segmentation\\rFCN for semantic segmentation\\rDeep CNN (Very Deep CNN)\\rNetworks with many layers (dozens or hundreds) to capture complex patterns. Improves accuracy but requires more computational power.\\rComplex tasks like image recognition, object detection\\rVGGNet, ResNet (up to 152 layers)\\rRecurrent CNN (RCNN)\\rCombines CNNs with RNNs for handling sequential data like video frames. CNN for feature extraction, RNN for temporal dependencies.\\rVideo classification, action recognition\\rRCNN for video analysis\\rMultistream CNN\\rMultiple CNN streams process inputs from different sources, such as RGB and depth images in parallel, improving accuracy.\\rMultimodal data processing, combining inputs from different sensors\\rTwo-stream CNN for action recognition\\r\\r\\nComparison of Different CNN Architectures and Their Use Cases Part Two- Programming Ocean Academy\\rType\\rDescription\\rUse Case\\rExample\\rResidual Networks (ResNet)\\rUses residual blocks to solve the vanishing gradient problem in deep networks. Adds shortcuts (residual connections) to improve training efficiency.\\rImage classification, object detection\\rResNet-50, ResNet-152 for large-scale tasks\\rDilated (Atrous) CNN\\rUses dilated convolutions to expand the receptive field without losing resolution, capturing global context in images.\\rSemantic segmentation, speech analysis\\rDeepLab for semantic segmentation\\rRegion-Based CNN (R-CNN)\\rCombines CNNs with region proposal methods for object detection. More advanced versions like Fast and Faster R-CNN improve speed and accuracy.\\rObject detection, image segmentation\\rR-CNN, Fast R-CNN, Faster R-CNN\\rU-Net\\rEncoder-decoder architecture used for segmentation. Contracting path captures context, expanding path enables precise localization.\\rBiomedical image segmentation, medical imaging\\rU-Net for medical image segmentation\\rCapsule Networks (CapsNet)\\rUses capsules (groups of neurons) to store positional information and relationships between features, preserving spatial hierarchies.\\rImage classification, object detection, spatial relationships\\rCapsule Network for recognizing overlapping digits\\r\\r\\nTable that captures the entire CNN process from the image input to the final output-Part One - Programming Ocean Academy\\rStep\\rLayer Type\\rProcess Description\\rOutput\\r1. Input Image\\rInput Layer\\rThe image (e.g., of a character like Tweety) is fed into the network. The input has a certain width, height, and depth (e.g., 32x32x3 for a color image with 3 RGB channels).\\rRaw image data of size (e.g., 32x32x3).\\r2. Convolution\\rConvolutional Layer\\rFilters (kernels) slide over the image, detecting simple patterns like edges and corners. Each filter produces a feature map by applying convolution over the image.\\rFeature maps of size (e.g., 30x30x32) where 32 is the number of filters and 30x30 is the reduced spatial dimension (due to no padding).\\r3. Feature Maps\\rOutput of Convolution\\rThe output of the convolutional layer is multiple feature maps that represent different patterns detected from the input image.\\rFeature maps showing edges, corners, and basic features extracted from the image.\\r4. Pooling\\rPooling Layer\\rPooling layers (usually MaxPooling) reduce the spatial dimensions (width and height) of the feature maps while keeping the depth (number of filters) intact.\\rPooled feature maps of smaller size (e.g., 15x15x32), where the spatial dimensions have been halved but the depth remains the same.\\r5. Second Convolution\\rConvolutional Layer\\rAnother set of filters is applied, detecting more complex features by sliding over the pooled feature maps. These filters detect patterns of higher complexity (e.g., shapes).\\rNew feature maps with more complex patterns detected (e.g., 13x13x64, where 64 is the number of filters).\\r\\r\\nTable that captures the entire CNN process from the image input to the final output-Part Two - Programming Ocean Academy\\rStep\\rLayer Type\\rProcess Description\\rOutput\\r6. Second Pooling\\rPooling Layer\\rAnother pooling layer reduces the spatial dimensions further, simplifying the data while retaining essential features.\\rSmaller pooled feature maps (e.g., 6x6x64).\\r7. Third Convolution\\rConvolutional Layer\\rA deeper convolution layer with more filters is applied, detecting even more abstract patterns, refining the learned features.\\rMore complex feature maps (e.g., 4x4x64) representing more detailed patterns.\\r8. Flattening\\rFlatten Layer\\rThe 3D feature maps are flattened into a 1D vector so that they can be used by the fully connected (Dense) layers for classification.\\rA flattened vector (e.g., size 1024) representing all learned features from the previous layers.\\r9. Fully Connected Layer\\rDense Layer\\rThe flattened features are passed through a fully connected (dense) layer where each node is connected to every feature. These connections have weights and biases.\\rA vector of activations (e.g., size 64), where each node\\'s activation depends on the learned weights and biases.\\r10. Output Layer (Classification)\\rDense Layer\\rThe final dense layer produces a probabilistic output for each class using an activation function like softmax. Each output represents the probability of the image belonging to a specific class.\\rA probabilistic distribution for classification (e.g., 0.2 for Donald, 0.1 for Goofy, 0.7 for Tweety, indicating Tweety as the highest probability).\\r11. Final Prediction\\rOutput\\rThe highest probability is selected as the predicted class (e.g., Tweety), based on the probabilistic distribution produced in the final layer.\\rPredicted class for the input image (e.g., Tweety).\\r\\r\\n\\r\\n\\r\\nThe receptive field VS kernel (or filter) - Programming Ocean Academy\\rConcept\\rReceptive Field\\rKernel/Filter\\rDefinition\\rThe portion of the input image that a specific neuron in a layer \"sees\" or is influenced by. It grows as you move deeper into the network.\\rA small matrix (usually 3x3 or 5x5) used to slide over the input image and compute features like edges, textures, or patterns.\\rScope\\rRefers to the area of the input image contributing to a neuron\\'s activation, and it expands as the CNN layers stack.\\rA set of weights used in convolution to extract specific features from the input.\\rSize\\rGrows as layers are added. For example, after several layers of convolutions, a neuron may \"see\" a larger part of the original image.\\rFixed size (e.g., 3x3 or 5x5). Determines how many pixels in the input are processed at each convolution step.\\rRole\\rHelps capture increasingly larger patterns or structures in the input as you go deeper into the network.\\rExtracts local patterns (like edges, textures) from the input by computing weighted sums in small regions.\\rInteraction\\rDetermined by how many layers the image has passed through. Each convolution layer with filters adds to the size of the receptive field.\\rApplies locally across the input to compute feature maps. The number of filters determines the depth of the feature maps.\\rCalculation\\rDependent on the kernel size, stride, padding, and number of layers in the network. A neuron in deeper layers has a larger receptive field.\\rThe kernel slides over the input image, applying weights at each position to compute feature maps at that layer.\\r\\r\\n\\r\\nA comparison Between Flattening Matrices and Transforming them into Higher Dimensions - Programming Ocean Academy\\rAspect\\rFlattening Matrices\\rTransforming Matrices into Higher Dimensions\\rDefinition\\rFlattening refers to converting a matrix (or higher-dimensional tensor) into a single vector by rearranging its elements into one continuous sequence.\\rTransforming into higher dimensions means converting data into a more complex space with additional features or dimensions, often through techniques like kernel functions or feature extraction.\\rPurpose\\rSimplifies data for algorithms that require a 1D input (e.g., feeding image data into a neural network).\\rAllows algorithms (like SVM) to separate data more effectively by mapping it to a higher-dimensional space.\\rResulting Shape\\rConverts a matrix of shape (m , n) into a vector of shape (m , n).\\rConverts an input of nnn-dimensions into an input of more than nnn-dimensions (e.g., mapping 2D data into 3D or higher).\\rUse Case\\r- Image processing: flattening 2D images for input into a fully connected neural network.\\r\\n- Simplifying input for models that expect vectors.\\r- Used in non-linear machine learning models (e.g., SVM with kernel trick) to make data linearly separable.\\r\\n- Feature engineering or dimensionality increase in deep learning.\\rImpact on Data\\rThe structure of the matrix is lost since it is now a 1D vector, but all the values are preserved.\\rThe data is enriched by adding new dimensions, allowing for more complex relationships to be learned.\\rCommon Techniques\\rReshape function in libraries like NumPy or TensorFlow (e.g., reshape((-1,))).\\rKernel transformations (e.g., polynomial kernel, RBF kernel), feature expansion, neural network layers (embedding layers).\\rComputational Complexity\\rGenerally straightforward and computationally cheap (just a rearrangement of elements).\\rMay increase the computational cost significantly due to the added complexity of more dimensions (higher-dimensional operations).\\rLoss of Information\\rNo loss of information occurs; all elements are retained in the vector. However, the spatial relationship of the matrix elements may be lost.\\rNo loss of information occurs. Instead, it often adds more information by embedding data into a higher-dimensional space.\\rExamples\\r- Flattening a 28Ã—28 image into a 784-length vector.\\r\\n- Flattening tensors for fully connected layers in neural networks.\\r- Mapping 2D input data into a 3D space using polynomial or RBF kernels in SVM.\\r\\n- Convolution layers in CNNs can map 2D data into higher-dimensional feature spaces.\\r\\r\\n\\r\\nHow SVM calculates the similarity between vectors using different approaches, including linear and non-linear kernels- Programming Ocean Academy\\rType of Kernel\\rMathematical Formula\\rExplanation\\rLinear Kernel\\r\\n \\r\\n \\rk=(X_(i  ,  )  X_(j  )= X_(i  )*   X_j)\\r- The dot product of two vectors.\\r\\n- Measures the similarity based on the angle between the vectors.\\r\\n- High similarity if vectors point in the same direction; low or negative if orthogonal or opposite.\\rPolynomial Kernel\\r\\n \\r\\n \\rK(xi,xj)=(xiÂ·xj+c)dK(\\\\mathbf{x}_i, \\\\mathbf{x}_j) = (\\\\mathbf{x}_i \\\\cdot \\\\mathbf{x}_j + c)^dK(xi?,xj?)=(xi?Â·xj?+c)d\\r\\n \\r\\n \\r- Raises the dot product to a power ddd and adds a constant ccc.\\r\\n- Allows SVM to fit more complex decision boundaries.\\r\\n- Higher degree ddd increases flexibility of the model.\\rRadial Basis Function (RBF) Kernel / Gaussian Kernel\\r\\n \\r\\n \\rK(xi,xj)=exp?(-?xi-xj?22s2)K(\\\\mathbf{x}_i, \\\\mathbf{x}_j) = \\\\exp\\\\left(-\\\\frac{\\\\|\\\\mathbf{x}_i - \\\\mathbf{x}_j\\\\|^2}{2\\\\sigma^2}\\\\right)K(xi?,xj?)=exp(-2s2?xi?-xj??2?)\\r\\n \\r\\n \\r- Measures similarity based on the Euclidean distance between vectors.\\r\\n- High similarity for vectors close in space; low similarity for distant vectors.\\r\\n- s\\\\sigmas controls the spread of the Gaussian curve.\\rSigmoid Kernel\\r\\n \\r\\n \\rK(xi,xj)=tanh?(a(xiÂ·xj)+c)K(\\\\mathbf{x}_i, \\\\mathbf{x}_j) = \\\\tanh(\\\\alpha (\\\\mathbf{x}_i \\\\cdot \\\\mathbf{x}_j) + c)K(xi?,xj?)=tanh(a(xi?Â·xj?)+c)\\r\\n \\r\\n \\r- Similar to the activation function in neural networks.\\r\\n- Uses the tanh function to compute similarity.\\r\\n- Parameters a\\\\alphaa and ccc control the shape of the similarity function.\\r\\r\\n\\r\\n\\r\\nA comprehensive list of feature types used in AI and machine learning - Part One - Programming Ocean Academy\\rFeature Type\\rDefinition\\rExample\\rApplication\\rSpatial Features\\rCaptures positional or locational data\\rLocation of edges in images\\rImage classification, object detection\\rGlobal Features\\rSummarizes overall structure of data\\rAverage pixel intensity\\rScene recognition, sentiment analysis\\rLocal Features\\rDescribes characteristics of smaller regions\\rPixel patch representing a corner\\rFace recognition, texture analysis\\rTemporal Features\\rCaptures time-based changes\\rStock prices over time\\rVideo analysis, speech recognition\\rFrequency Features\\rBased on frequency domain\\rFourier coefficients\\rAudio processing, sensor data\\rContextual Features\\rCaptures surrounding environment or context\\rWord meaning from surrounding words\\rNLP, recommendation systems\\rStructural Features\\rDescribes underlying structure or relationships\\rConnections in social network graph\\rGraph analysis, chemical modeling\\rSemantic Features\\rCarries conceptual meaning from data\\rWord embeddings like BERT\\rNLP, machine translation\\rStatistical Features\\rDerived from statistical properties\\rMean, variance\\rAnomaly detection, feature engineering\\rHierarchical Features\\rCaptures patterns at different abstraction levels\\rEdges in lower CNN layers, objects in higher layers\\rDeep learning, object detection\\r\\r\\n\\r\\nA comprehensive list of feature types used in AI and machine learning - Part Two - Programming Ocean Academy\\rFeature Type\\rDefinition\\rExample\\rApplication\\rTexture Features\\rDescribes surface properties or patterns\\rHaralick texture features\\rMedical imaging, material classification\\rColor Features\\rDescribes color properties\\rRGB values, color histograms\\rImage retrieval, object detection\\rShape Features\\rCaptures geometric properties\\rContour descriptors, HOG\\rObject detection, handwriting recognition\\rDerived Features\\rEngineered from transformations\\rPolynomial features\\rFeature engineering, model optimization\\rLatent Features\\rHidden features learned by models\\rLatent factors in matrix factorization\\rDeep learning, recommendation systems\\rCategorical Features\\rRepresents discrete categories\\rGender, product category\\rClassification, recommendation systems\\rNumerical Features\\rRepresents quantitative values\\rAge, income\\rRegression, predictive modeling\\rBinary Features\\rHas only two possible values\\rYes/No, True/False\\rClassification, anomaly detection\\rOrdinal Features\\rOrdered but without fixed intervals\\rEducation level\\rClassification, ranking systems\\rSparse Features\\rContains many zeros or missing values\\rOne-hot encoded vectors\\rText classification, NLP\\rTime-Series Features\\rIndexed by time, captures sequential dependencies\\rAutocorrelation in stock prices\\rFinancial forecasting, predictive maintenance\\rCorrelation Features\\rQuantifies relationship between variables\\rPearson correlation coefficient\\rFeature selection, multicollinearity checking\\rInteraction Features\\rCreated by combining original features\\rBMI from height and weight\\rFeature engineering, non-linear models\\rDimensionality-Reduced Features\\rReduced dimensionality while retaining info\\rPCA components, t-SNE\\rHigh-dimensional data analysis\\rSpectral Features\\rDerived from spectral representation\\rPower spectral density, MFCC\\rAudio processing, speech recognition\\r\\r\\n\\r\\n\\r\\nA timeline in table format tracing the evolution of the term \"Data Science\" the source : Forbes- Programming Ocean Academy\\rYear\\rEvent Description\\r1962\\rJohn W. Tukey writes \"The Future of Data Analysis,\" emphasizing data analysis as an empirical science rather than merely mathematics. Tukey also coined the term \"bit\" in 1947.\\r1974\\rPeter Naur publishes \"Concise Survey of Computer Methods,\" introducing the term \"data science\" and defining it as the science of dealing with established data.\\r1977\\rThe International Association for Statistical Computing (IASC) is established, aiming to link statistical methodology with modern computer technology.\\r1989\\rGregory Piatetsky-Shapiro organizes the first Knowledge Discovery in Databases (KDD) workshop, which evolves into the annual ACM SIGKDD Conference on Knowledge Discovery and Data Mining.\\r1994\\rBusinessWeek publishes a cover story on \"Database Marketing,\" discussing companies\\' use of data to predict consumer behavior.\\r1996\\rThe term \"data science\" is included in the title of the International Federation of Classification Societies conference for the first time.\\r1996\\rUsama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth publish \"From Data Mining to Knowledge Discovery in Databases,\" defining data mining as a step in the Knowledge Discovery process.\\r1997\\rC. F. Jeff Wu calls for the renaming of statistics as data science in his inaugural lecture at the University of Michigan.\\r1997\\rThe journal \"Data Mining and Knowledge Discovery\" is launched, reflecting the rising popularity of data mining.\\r1999\\rJacob Zahavi comments on the challenges of data mining in \"Mining Data for Nuggets of Knowledge.\"\\r2001\\rWilliam S. Cleveland publishes \"Data Science: An Action Plan,\" proposing the establishment of data science as a new discipline.\\r2001\\rLeo Breiman discusses two cultures of statistical modeling, advocating for algorithmic modeling to solve problems with data.\\r2002\\rLaunch of the \"Data Science Journal,\" focusing on data management in science and technology.\\r2003\\rLaunch of the \"Journal of Data Science,\" aimed at presenting applications of statistical methods.\\r2005\\rThomas H. Davenport and others describe the emergence of competition based on analytics in \"Competing on Analytics.\"\\r2005\\rThe National Science Board calls for the development of a career path for data scientists in their report on digital data collections.\\r2007\\rThe Research Center for Dataology and Data Science is established at Fudan University, Shanghai.\\r2008\\rJISC publishes a study defining the role and career development of data scientists and curators.\\r2009\\rThe report \"Harnessing the Power of Digital Data for Science and Society\" emphasizes the need for data science experts.\\r2009\\rHal Varian describes statisticians as the \"sexy job\" of the next decade due to their ability to understand and extract value from data.\\r2009\\rKirk D. Borne and others advocate for training in data science for both specialists and non-specialists.\\r2009\\rMike Driscoll discusses the skills needed for data professionals in \"The Three Sexy Skills of Data Geeks.\"\\r2010\\rKenneth Cukier writes about the emergence of data scientists who combine skills in programming, statistics, and storytelling.\\r2010\\rMike Loukides describes data scientists as inherently interdisciplinary, capable of tackling various aspects of data problems.\\r2010\\rHilary Mason and Chris Wiggins propose a taxonomy of data science activities.\\r2010\\rDrew Conway introduces the \"Data Science Venn Diagram,\" illustrating the skill set required for data scientists.\\r2011\\rPete Warden highlights the ambiguity in defining data science, suggesting it involves interdisciplinary work across various data-related fields.\\r2011\\rDavid Smith discusses the rising popularity and application of the terms \"Data Science\" and \"Data Scientist.\"\\r2012\\rTom Davenport and D.J. Patil publish \"Data Scientist: The Sexiest Job of the 21st Century\" in the Harvard Business Review, highlighting the growing demand and importance of data scientists in the modern workforce.\\r\\r\\n\\r\\n\\r\\nA table summarizing the key milestones in the history of big data- the source: Forbes - Programming Ocean Academy\\rYear\\rMilestone\\r1941\\rThe term \"information explosion\" is first used.\\r1944\\rFremont Rider estimates U.S. university libraries will double in size every 16 years, predicting vast future storage needs.\\r1961\\rDerek Price publishes \"Science Since Babylon,\" noting exponential growth in scientific journals and knowledge.\\r1967\\rMarron and de Maine discuss the need for automatic data compression due to the information explosion.\\r1971\\rArthur Miller comments on measuring individuals by their data storage capacity.\\r1975\\rJapan begins the Information Flow Census, tracking information volume across media.\\r1980\\rI.A. Tjomsland discusses how data expands to fill available storage.\\r1981\\rHungary starts a research project on information industries, measuring information volume.\\r1983\\rIthiel de Sola Pool publishes findings on the growth of communication media and information flow.\\r1986\\rHal B. Becker estimates significant increases in data recording density.\\r1990\\rPeter J. Denning discusses the challenges posed by the overwhelming volume of information.\\r1996\\rDigital storage becomes more cost-effective than paper.\\r1997\\rMichael Cox and David Ellsworth introduce the term \"big data\" in a publication on out-of-core visualization.\\r1998\\rJohn R. Mashey presents on \"Big Data\" at a USENIX meeting.\\r2000\\rPeter Lyman and Hal R. Varian publish a study quantifying new information creation, estimating 1.5 exabytes produced in 1999.\\r2001\\rDoug Laney publishes on 3D data management, establishing volume, velocity, and variety as dimensions of big data.\\r2005\\rTim O\\'Reilly discusses the significance of data in Web 2.0.\\r2007\\rIDC forecasts that digital data will grow significantly, estimating 988 exabytes by 2010.\\r2008\\rCisco predicts IP traffic will double every two years, reaching half a zettabyte in 2012.\\r2009\\rA report finds Americans consumed information for 1.3 trillion hours in 2008, totaling 3.6 zettabytes.\\r2011\\rMartin Hilbert and Priscila Lopez estimate a shift from analog to digital storage, with 94% of storage capacity being digital by 2007.\\r2011\\rMcKinsey Global Institute reports on the rapid growth of stored data across sectors, estimating 7.4 exabytes of new data stored by enterprises in 2010.\\r2012\\rInternational Journal of Communications publishes a special section on information capacity.\\r\\r\\n\\r\\nA Short History of Information Technology (IT)- The source : Forbes- Programming Ocean Academy\\rYear\\rEvent\\r6/30/1945\\rJohn Von Neumann publishes the First Draft of a Report on the EDVAC, introducing the stored-program concept, which became the foundation of modern computer architecture.\\r5/22/1973 \\rBob Metcalfe invents Ethernet at Xerox PARC, enabling Local Area Networks (LANs) and revolutionizing computer connectivity in offices.\\r3/1/1989 \\rTim Berners-Lee circulates \"Information Management: A Proposal\" at CERN, which outlines the basic framework for the World Wide Web, leading to a global hypertext system for sharing information.\\r1980s\\rThe rise of the personal computer (PC) allows individuals to work digitally, but true impact emerges when PCs are connected via LANs, making email the \"killer app\" and increasing data generation exponentially.\\r1990s\\rThe World Wide Web gains popularity, facilitating the rapid spread of information beyond enterprise-focused activities and leading to the digitization of many aspects of daily life. New IT companies like Intel, Microsoft, Oracle, Cisco, Dell, and EMC emerge as dominant players.\\r6/15/1905\\rCERN releases the World Wide Web to the public, making the internet widely accessible and paving the way for a global network of connected computers.\\r2000s\\rThe IT industry shifts towards specialization, with horizontal players focusing on key components like semiconductors (Intel), operating systems (Microsoft), databases (Oracle), and networking (Cisco).\\r2000s-2010s\\rThe cloud computing revolution and the rise of \"big data\" change the IT landscape. The Internet of Things (IoT) connects billions of devices, vastly increasing the amount of data generated and shared.\\r6/12/1905\\rPeter J. Denning discusses the challenges posed by the overwhelming volume of information.\\r6/18/1905\\rDigital storage becomes more cost-effective than paper.\\r6/19/1905\\rMichael Cox and David Ellsworth introduce the term \"big data\" in a publication on out-of-core visualization.\\r6/20/1905\\rJohn R. Mashey presents on \"Big Data\" at a USENIX meeting.\\r6/22/1905\\rPeter Lyman and Hal R. Varian publish a study quantifying new information creation, estimating 1.5 exabytes produced in 1999.\\r6/23/1905\\rDoug Laney publishes on 3D data management, establishing volume, velocity, and variety as dimensions of big data.\\r6/27/1905\\rTim O\\'Reilly discusses the significance of data in Web 2.0.\\r6/29/1905\\rIDC forecasts that digital data will grow significantly, estimating 988 exabytes by 2010.\\r6/30/1905\\rCisco predicts IP traffic will double every two years, reaching half a zettabyte in 2012.\\r7/1/1905\\rA report finds Americans consumed information for 1.3 trillion hours in 2008, totaling 3.6 zettabytes.\\r7/3/1905\\rMartin Hilbert and Priscila Lopez estimate a shift from analog to digital storage, with 94% of storage capacity being digital by 2007.\\r7/3/1905\\rMcKinsey Global Institute reports on the rapid growth of stored data across sectors, estimating 7.4 exabytes of new data stored by enterprises in 2010.\\r7/4/1905\\rInternational Journal of Communications publishes a special section on information capacity.\\r\\r\\n\\r\\nA comparison of Scikit-learn, TensorFlow, and PyTorch- Programming Ocean Academy\\rAspect\\rScikit-learn\\rTensorFlow\\rPyTorch\\rPrimary Use\\rTraditional machine learning (ML) algorithms like classification, regression, clustering, and preprocessing.\\rDeep learning, neural networks, large-scale ML.\\rDeep learning, neural networks, research-oriented deep learning tasks.\\rAPI Style\\rSimple, high-level, easy to use, and well-suited for beginners.\\rFlexible but can be complex, uses high-level Keras API for ease.\\rMore Pythonic and intuitive, especially for researchers and developers.\\rCore Language\\rPython (also supports integration with other languages).\\rPython (primary), also supports C++, JavaScript, Java, Go, and others.\\rPython (primary), also supports C++.\\rType of ML Supported\\rTraditional ML (SVM, Decision Trees, Random Forest, etc.).\\rDeep learning (CNNs, RNNs, Transformers), some support for traditional ML.\\rDeep learning (CNNs, RNNs, Transformers).\\rGPU Support\\rNo direct support for GPU acceleration.\\rYes, strong support for GPU via CUDA and TPU.\\rYes, native GPU support via CUDA.\\rDistributed Computing\\rNot designed for distributed systems, though integration with Spark is possible.\\rSupports distributed training across multiple GPUs and machines.\\rSupports distributed training, including data parallelism and model parallelism.\\rEase of Use\\rVery easy to use for classical ML tasks with consistent APIs.\\rHigh-level API (Keras) is easy, but low-level API can be more complex.\\rIntuitive for deep learning; dynamic computational graphs make it easier to debug.\\rPerformance\\rEfficient for small to medium datasets but not optimized for massive datasets or deep learning.\\rOptimized for large-scale deep learning and high-performance computing.\\rHighly performant for deep learning, particularly on GPUs.\\rModel Deployment\\rLimited deployment options (e.g., via pickle or joblib for ML models).\\rTensorFlow Lite, TensorFlow Serving for production environments. Widely used for mobile and web.\\rSupports TorchServe for model serving, but deployment is less extensive than TensorFlow.\\rVisualization Tools\\rLimited; integrates with Matplotlib or Seaborn for visualization.\\rTensorBoard for model training visualization and debugging.\\rSupports integration with TensorBoard, but fewer built-in tools compared to TensorFlow.\\rCommunity & Ecosystem\\rLarge community for traditional ML; many pre-built algorithms and extensions.\\rVery large community, strong Google backing, extensive ecosystem (TensorFlow Hub, Lite, etc.).\\rGrowing community, strong in academia and research, especially due to dynamic computation graph and flexibility.\\rLearning Curve\\rLow, particularly for beginners in ML.\\rMedium to high; Keras API simplifies deep learning but TensorFlow\\'s full flexibility is complex.\\rLow to medium; dynamic computation graph and Pythonic design make it easier to learn for researchers.\\rDynamic vs Static Graphs\\rStatic computation graph (not suited for dynamic deep learning tasks).\\rPrimarily uses static computation graph (though supports eager execution).\\rDynamic computation graph, allowing more flexibility and ease for debugging.\\rPre-trained Models\\rFew, mostly for traditional ML (limited transfer learning options).\\rLarge model zoo (TensorFlow Hub) for deep learning tasks (e.g., BERT, ResNet).\\rPyTorch Hub offers a growing library of pre-trained models (e.g., ResNet, GPT).\\rPopularity\\rWidely used in industry and academia for traditional ML.\\rDominant in production environments for deep learning; used by many large companies (Google, Uber, etc.).\\rIncreasingly popular in academia and research, especially for cutting-edge deep learning models.\\r\\r\\n\\r\\n\\r\\nA comparison between Matplotlib and Seaborn- Programming Ocean Academy\\rFeature\\rMatplotlib\\rSeaborn\\rPurpose\\rGeneral-purpose plotting library\\rHigh-level interface for statistical plotting\\rEase of Use\\rMore control but requires detailed code\\rEasier to use with built-in themes and defaults\\rCustomization\\rHighly customizable but manual\\rBuilt on Matplotlib, offers automatic styling and improved aesthetics\\rPlot Types\\rBasic plots: line, scatter, bar, histogram, etc.\\rSpecialized for statistical plots like heatmaps, pair plots, violin plots\\rLearning Curve\\rSteeper for complex visualizations\\rEasier for quick, insightful visualizations\\rDefault Aesthetics\\rMinimal by default\\rStylish, colorful, and attractive by default\\rStatistical Visualization\\rRequires manual setup for statistical plots\\rBuilt-in support for statistical plots (e.g., KDE, regression)\\rIntegration\\rWorks well with low-level libraries like NumPy\\rIntegrates seamlessly with Pandas DataFrames and NumPy\\rThemes/Styles\\rCustomizable but requires manual settings\\rPredefined themes (e.g., \"darkgrid\", \"whitegrid\") for easy styling\\rPerformance\\rHighly performant for large datasets\\rSlightly slower due to additional abstraction over Matplotlib\\rAnnotations & Labels\\rRequires manual positioning and adjustments\\rSimplifies labeling with built-in functionality\\rData Handling\\rCan plot data from arrays, lists, DataFrames\\rPrimarily designed for Pandas DataFrames and tidy data\\rCommunity & Support\\rLarge community, extensive documentation\\rSmaller but still active community, based on Matplotlib\\r\\r\\n\\r\\n\\r\\nA comparison between the Training Set, Validation Set, and Testing Set- Programming Ocean Academy\\rAspect\\rTraining Set\\rValidation Set\\rTesting Set\\rPurpose\\rUsed to train the model by adjusting its weights and learning patterns.\\rUsed to tune the model\\'s hyperparameters and prevent overfitting.\\rUsed to evaluate the final model\\'s performance on unseen data.\\rData Composition\\rComprises the largest portion of the dataset (e.g., 70-80%).\\rTypically, smaller than the training set (e.g., 10-20%).\\rAlso, a small portion of the dataset (e.g., 10-20%).\\rModel Feedback\\rThe model learns from this data and updates its parameters based on the training process.\\rThe model receives feedback on its performance to help improve and optimize during training.\\rThe model does not receive feedback during this stage; it measures generalization.\\rUsage in Training Cycle\\rDirectly involved in the training cycle where the model learns the data features.\\rUsed during training to validate model performance and adjust hyperparameters, often via techniques like cross-validation.\\rUsed after the model is fully trained to assess accuracy and performance metrics.\\rImpact on Model\\rDirectly impacts how well the model learns from the data.\\rAffects the choice of hyperparameters, impacting model architecture and generalization ability.\\rIndicates how well the model will perform in real-world applications.\\rExample Ratio\\r70-80% of the total dataset.\\r10-20% of the total dataset.\\r10-20% of the total dataset.\\rData Leakage Risk\\rRisk of overfitting if too few examples are used or if the model is too complex.\\rOverfitting can still occur if validation set is not representative or if hyperparameters are not tuned carefully.\\rShould be completely separate to provide an unbiased evaluation of model performance.\\r\\r\\n\\r\\n\\r\\nA comparison between Overfitting, Underfitting, and a Balanced Model- Programming Ocean Academy\\rAspect\\rOverfitting\\rUnderfitting\\rBalanced Model\\rDefinition\\rThe model learns the training data too well, capturing noise and outliers, resulting in poor generalization to new data.\\rThe model is too simple to capture the underlying patterns in the data, leading to poor performance on both training and testing datasets.\\rThe model adequately captures the underlying patterns in the data, performing well on both training and testing datasets.\\rModel Complexity\\rHighly complex with too many parameters relative to the amount of data.\\rToo simple with too few parameters or features.\\rAppropriate complexity, balancing bias and variance.\\rTraining Performance\\rHigh accuracy on the training set.\\rLow accuracy on both training and testing sets.\\rGood accuracy on both training and testing sets.\\rTesting Performance\\rPoor accuracy on the testing set due to lack of generalization.\\rPoor accuracy on the testing set, similar to training set performance.\\rGood accuracy on the testing set, indicating effective generalization.\\rIndicators\\rLarge difference between training and testing accuracy (high training accuracy, low testing accuracy).\\rLittle difference in performance between training and testing sets, but both are low.\\rSimilar and satisfactory performance metrics for both training and testing sets.\\rCommon Causes\\rExcessive training time, too many features, or lack of regularization.\\rInsufficient training, overly simplistic model, or poor feature selection.\\rProper model selection, sufficient training, and appropriate feature engineering.\\rExample\\rA model that memorizes every data point in the training set, failing to predict new data accurately.\\rA linear model trying to fit a complex, non-linear relationship in the data.\\rA decision tree that finds a reasonable balance between complexity and generalization.\\r\\r\\n\\r\\n\\r\\n\\r\\nA table comparing Gini Index, Entropy, and Noise - Programming Ocean Academy\\rAspect\\rGini Index\\rEntropy\\rNoise\\rDefinition\\rA metric that measures the impurity of a split.\\rA metric from information theory that measures uncertainty or impurity in data.\\rUnwanted random variation or disturbances in data, unrelated to the underlying signal.\\rRange\\r0 to 0.5 (binary classification). 0 = pure node, 0.5 = maximum impurity.\\r0 to 1 (binary classification). 0 = pure node, 1 = maximum uncertainty.\\rDepends on the data; usually ranges from small perturbations to significant disruptions.\\rInterpretation\\rLower values indicate purer nodes (less impurity).\\rLower values indicate less disorder or uncertainty. Higher values show more disorder.\\rRepresents the level of distortion in the data, which can lead to inaccurate predictions.\\rRelation to Data\\rMeasures the probability that a randomly chosen element would be misclassified.\\rMeasures the amount of information needed to classify data points.\\rRepresents data irregularities that may obscure patterns or affect model accuracy.\\rRole in Decision Trees\\rUsed to determine the best split by minimizing impurity in child nodes.\\rUsed to select the best split by maximizing information gain.\\rConsidered a source of error or distortion; decision trees aim to reduce the impact of noise on splitting criteria.\\r\\r\\n\\r\\n\\r\\nHow GridSearch helps in finding the best hyperparameters- Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rGridSearch is an exhaustive search method to find the best combination of hyperparameters for a machine learning model.\\rProcess\\rIt tries every combination of hyperparameter values specified in a predefined grid and evaluates the model for each combination.\\rGrid Creation\\rA grid of hyperparameter values is defined by the user, consisting of various possible values for each hyperparameter.\\rSearch Strategy\\rExhaustive search over the entire grid of hyperparameters; each combination is evaluated independently.\\rEvaluation\\rFor each combination, the model is trained and evaluated (usually using cross-validation) to compute performance metrics.\\rPerformance Metric\\rThe user specifies a performance metric (e.g., accuracy, F1-score, etc.) to evaluate each hyperparameter combination.\\rBest Hyperparameter Selection\\rAfter evaluating all combinations, GridSearch selects the hyperparameter set that maximizes the chosen performance metric.\\rTime Complexity\\rCan be computationally expensive because it evaluates every possible combination of hyperparameters (especially with large grids).\\rAdvantages\\rSimple to implement, ensures all combinations are tested, and guarantees finding the best combination within the given grid.\\rLimitations\\rComputationally expensive, especially with many hyperparameters and large search spaces; doesn\\'t scale well to complex models.\\rUse Case\\rBest suited for models with smaller hyperparameter spaces or when computational resources are sufficient for exhaustive search.\\r\\r\\n\\r\\n\\r\\nA comparison between Logistic Regression, Support Vector Classifier (SVC), Decision Trees, and K-Nearest Neighbors (KNN)- Programming Ocean Academy\\rAspect\\rLogistic Regression\\rSupport Vector Classifier (SVC)\\rDecision Trees\\rK-Nearest Neighbors (KNN)\\rType of Algorithm\\rLinear model, probabilistic classifier\\rNon-linear (can also be linear with certain kernels)\\rNon-parametric, tree-based model\\rInstance-based learning (lazy learning algorithm)\\rMathematical Basis\\rUses a logistic (sigmoid) function to model the probability of binary outcomes\\rMaximizes the margin between data points and decision boundary (hyperplane)\\rUses a tree structure to split data based on feature importance\\rClassifies based on the majority class of nearest neighbors\\rUse Case\\rBinary classification problems (can be extended to multi-class)\\rClassification of both linear and non-linear data\\rSuitable for both classification and regression tasks\\rSuitable for classification tasks, can also be used for regression\\rAssumptions\\rAssumes a linear relationship between features and log-odds of the outcome\\rAssumes data can be separated with a hyperplane in high-dimensional space\\rAssumes no specific distribution, no parametric assumptions\\rNo assumptions about the distribution of data\\rInterpretability\\rHigh, coefficients show the impact of each feature\\rMedium, more difficult to interpret due to kernel transformations\\rMedium, easy to interpret the decision path in the tree\\rLow, as it only relies on the distance metric from the nearest neighbors\\rPerformance on Small Data\\rGood for small to medium-sized datasets\\rWorks well with smaller datasets but can scale with kernel trick\\rWorks well with small datasets, prone to overfitting\\rEffective for small datasets but can be slow on larger ones\\rPerformance on Large Data\\rEfficient for large datasets\\rCan be computationally expensive with large datasets\\rStruggles with large, complex datasets (prone to overfitting)\\rBecomes computationally expensive with large datasets\\rHandling of Non-linearity\\rNot suitable for non-linear problems without feature transformation\\rExcellent for non-linear problems using kernels\\rHandles non-linear data naturally through branching\\rCan handle non-linear problems well depending on the distance metric used\\rOverfitting Tendency\\rLess prone to overfitting if regularization is applied\\rCan overfit if the kernel is too complex\\rHighly prone to overfitting, especially without pruning\\rProne to overfitting if k (number of neighbors) is small\\rHyperparameters\\rRegularization strength (C), solver\\rKernel (linear, polynomial, RBF), C (penalty parameter), gamma\\rMax depth, min samples per split, splitting criterion\\rNumber of neighbors (k), distance metric (Euclidean, Manhattan, etc.)\\rComputational Complexity\\rLow, computationally efficient\\rHigher complexity due to support vectors and kernel computation\\rLow complexity for small trees but grows with tree depth\\rHigh, as it involves computing distances from each point to all other points\\rHandling Outliers\\rCan be sensitive to outliers\\rRobust to outliers if margin is wide\\rSensitive to outliers; can lead to skewed splits\\rSensitive to outliers since distance-based methods are affected by them\\rMemory Usage\\rLow, only requires storing coefficients\\rMedium to high, depends on the number of support vectors\\rLow to moderate, depends on tree depth\\rHigh, since it needs to store all training data for comparison\\rPrediction Speed\\rFast\\rFast after training, but training can be slow for complex kernels\\rFast once the tree is built\\rSlow, as it computes the distance to all points at prediction time\\rExample\\rEmail spam detection, disease diagnosis\\rImage classification, text classification\\rCredit scoring, customer churn prediction\\rHandwriting recognition, recommendation systems\\r\\r\\n\\r\\n\\r\\n\\r\\nA comparison between GridSearch and GridSearchCV- Programming Ocean Academy\\rFeature\\rGridSearch\\rGridSearchCV\\rDefinition\\rA process that evaluates all combinations of hyperparameters over a given set but does not involve cross-validation.\\rA method from sklearn.model_selection that performs exhaustive search over specified hyperparameter values with built-in cross-validation.\\rPrimary Use\\rManually implemented to find the best hyperparameters, usually without automatic cross-validation.\\rUsed to automatically tune hyperparameters with cross-validation built in, ensuring model robustness.\\rCross-Validation\\rDoes not perform cross-validation by default. You must manually split the data or use additional validation techniques.\\rPerforms cross-validation (CV) automatically based on the provided cv parameter (e.g., k-folds).\\rLibrary Support\\rNot directly supported by libraries like scikit-learn. Typically requires manual coding for parameter search.\\rDirectly supported by scikit-learn with the class GridSearchCV.\\rModel Evaluation\\rEvaluates model performance based on a given validation set, not using multiple splits for CV.\\rUses cross-validation, evaluating the model across multiple folds of training data to give a more reliable performance estimate.\\rOverfitting Risk\\rHigher risk of overfitting since it may evaluate the model only on a single validation set.\\rLower risk of overfitting due to cross-validation, as it tests the model across different data folds.\\rEfficiency\\rLess efficient in terms of ensuring generalization since it may focus on a specific dataset split.\\rMore efficient in evaluating the generalization of the model by testing on multiple data splits.\\rOutput\\rProvides the best parameters based on the specified validation set.\\rProvides the best parameters based on cross-validated performance across different folds\\r\\r\\n\\r\\nA table summarizing the processes the model goes through during one epoch- Programming Ocean Academy\\rStep\\rDescription\\r1. Shuffling\\rOptionally shuffle the dataset to randomize the order of samples before processing (helps prevent bias).\\r2. Batch creation\\rDivide the dataset into smaller subsets called batches for efficient processing during training.\\r3. Forward Propagation\\rFor each batch, the model passes the input data through the network layers to produce predictions.\\r4. Loss Calculation\\rCompare the model\\'s predictions to the actual labels using a loss function to quantify the error.\\r5. Backpropagation\\rCompute gradients (partial derivatives of the loss) by propagating the error backward through the network.\\r6. Weight Updates\\rUpdate the model\\'s weights using the computed gradients and an optimization algorithm (e.g., SGD, Adam).\\r7. Evaluation\\rOptionally evaluate the model\\'s performance on metrics like accuracy, using either training or validation data.\\r8. Repeat for All Batches\\rRepeat steps 3 to 7 for each batch in the dataset within the epoch.\\r9. Epoch-End Actions\\rOptional actions like validating on a separate dataset, adjusting the learning rate, or saving model checkpoints.\\r\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nA table explaining the terms related to deep learning - Programming Ocean Academy\\rTerm\\rExplanation\\rEpoch\\rOne complete pass through the entire training dataset. During an epoch, the model processes every data point in the dataset, adjusts its weights, and learns from the data.\\rLearning Rate\\rA hyperparameter that controls how much to adjust the model\\'s weights in response to the calculated gradients. A higher learning rate speeds up training but may overshoot, while a lower rate offers more precise adjustments but slower convergence.\\rActivation\\rThe function applied at each neuron in a neural network layer, introducing non-linearity into the model. Common activation functions include ReLU, sigmoid, and tanh.\\rRegularization\\rA technique used to prevent overfitting by adding a penalty (or constraint) to the loss function. Common methods include L1 (Lasso) and L2 (Ridge) regularization.\\rRegularization Rate\\rA hyperparameter that controls the strength of the regularization applied. A higher regularization rate increases the penalty on larger weights, helping to simplify the model and reduce overfitting.\\rRatio of Training to Test Size\\rThe proportion of the dataset used for training versus testing (or validation). A typical split might be 80% training and 20% testing. The training set is used for learning, while the test set evaluates model performance on unseen data.\\rNoise\\rUnwanted variations or randomness in the data that can distort the learning process. In deep learning, noise can arise from data collection errors, or it can be deliberately introduced as a regularization technique (e.g., dropout, adding Gaussian noise).\\rBatch Size\\rThe number of training examples processed in one iteration. Smaller batch sizes lead to noisier updates but require less memory and can generalize better. Larger batch sizes can be more stable but may lead to slower generalization. Common sizes include 32, 64, and 128.\\r\\r\\n\\r\\n\\r\\nA table explaining the anatomy of a neural network in deep learning- Programming Ocean Academy\\rComponent\\rExplanation\\rInput Layer\\rThe first layer in the neural network that receives the raw data (features) to be processed. Each neuron in this layer corresponds to one feature from the input data.\\rHidden Layers\\rIntermediate layers between the input and output layers, where most of the computations occur. The network can have one or more hidden layers. Each hidden layer applies weights, biases, and activation functions to transform the input into something meaningful for the model.\\rNeurons\\rThe basic units in each layer. Each neuron takes inputs, applies weights and biases, sums them, and passes the result through an activation function. The output is then passed to neurons in the next layer.\\rWeights\\rParameters that determine the strength and direction of the connection between neurons in adjacent layers. Weights are learned during training and adjusted to minimize the loss function.\\rBiases\\rAdditional parameters added to the weighted sum of inputs before applying the activation function. Biases help the model fit the data better by shifting the activation function.\\rActivation Functions\\rFunctions applied at each neuron to introduce non-linearity. Common functions include ReLU, sigmoid, and tanh, which allow the model to capture complex patterns in the data.\\rOutput Layer\\rThe final layer that provides the predictions or output of the model. The number of neurons in the output layer typically corresponds to the number of prediction classes or the target variables in regression.\\rForward Propagation\\rThe process where input data passes through the network from the input layer, through hidden layers, to the output layer, generating predictions based on current weights and biases.\\rLoss Function\\rA function that measures the error or difference between the predicted output and the actual target value. Common loss functions include Mean Squared Error for regression and Cross-Entropy Loss for classification.\\rBackpropagation\\rThe process of calculating the gradient of the loss function with respect to the weights and biases in the network, starting from the output layer and moving backward through the layers. This allows the network to learn by updating the weights to minimize the loss.\\rOptimization Algorithm\\rAn algorithm (such as Stochastic Gradient Descent or Adam) used to update the network\\'s weights and biases based on the gradients computed during backpropagation, to minimize the loss function.\\rLearning Rate\\rA hyperparameter that controls how much to adjust the weights during each update. A lower learning rate makes smaller adjustments and converges slowly, while a higher learning rate can speed up training but may overshoot the optimal solution.\\rEpoch\\rOne full pass through the entire training dataset. During an epoch, the model processes the data, calculates the loss, performs backpropagation, and updates the weights. Multiple epochs are needed for the model to learn effectively.\\rBatch Size\\rThe number of training samples processed in one forward and backward pass. Small batch sizes lead to noisier updates but can help generalize better, while large batch sizes provide more stable updates but may require more memory.\\rRegularization\\rTechniques (such as L1 and L2 regularization or dropout) used to prevent overfitting by adding a penalty to the loss function or randomly disabling neurons during training.\\rDropout\\rA regularization technique where a random set of neurons is ignored (dropped out) during each training iteration. This forces the model to learn more robust features and helps prevent overfitting.\\rSoftmax\\rA function often used in the output layer for multi-class classification tasks. It converts the output scores into probabilities, making it easier to interpret which class the model predicts.\\r\\r\\n\\r\\na comparative table outlining the key differences between Machine Learning and Deep Learning- Programming Ocean Academy\\rAspect\\rMachine Learning\\rDeep Learning\\rDefinition\\rA subset of artificial intelligence that enables systems to learn from data and improve their performance without explicit programming.\\rA subset of machine learning that uses neural networks with many layers to model complex patterns in large datasets.\\rData Requirements\\rTypically requires less data to train models effectively.\\rRequires large amounts of data to achieve high performance, often needing thousands to millions of samples.\\rFeature Engineering\\rOften requires manual feature extraction and selection to identify relevant input features for model training.\\rAutomatically learns features from raw data, eliminating the need for extensive manual feature engineering.\\rComplexity\\rGenerally simpler algorithms (e.g., linear regression, decision trees) that are easier to interpret.\\rMore complex architectures (e.g., CNNs, RNNs) that are often seen as \"black boxes\" due to their complexity.\\rTraining Time\\rTypically faster training times due to simpler algorithms and less data.\\rLonger training times, often requiring powerful hardware (GPUs) to handle large datasets and complex computations.\\rPerformance\\rPerforms well on structured data (like tables) but may struggle with unstructured data (like images and text).\\rExcels at processing unstructured data, making it suitable for tasks such as image recognition, natural language processing, and speech recognition.\\rModel Interpretability\\rModels are generally more interpretable, allowing easier understanding of how decisions are made.\\rModels are less interpretable, making it difficult to understand how decisions are derived from input data.\\rExamples\\rAlgorithms include linear regression, logistic regression, decision trees, random forests, and support vector machines.\\rCommon architectures include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers.\\rApplication Areas\\rCommonly used in applications like recommendation systems, fraud detection, and predictive analytics.\\rWidely used in advanced applications such as computer vision, speech recognition, and natural language processing.\\rHardware Dependency\\rUsually runs efficiently on standard CPUs.\\rOften requires specialized hardware, such as GPUs or TPUs, to accelerate the training of deep neural networks.\\r\\r\\n\\r\\nA table summarizing the breakthroughs in biology, proteins, and human health using AI models- Programming Ocean Academy\\rField\\rBreakthrough\\rImpact\\rProtein Folding\\rAlphaFold by DeepMind\\rAccurate prediction of 3D protein structures, revolutionizing drug discovery, disease understanding, and protein design.\\rDrug Discovery & Development\\rAI-driven drug design (e.g., Atomwise,BenevolentAI)\\rAccelerates the identification of drug candidates and optimization of existing drugs.\\r\\rPredictive toxicology\\rAssesses drug safety and toxicity before clinical trials, reducing costs and time.\\rGenomic Analysis\\rPrecision medicine\\rAI analyzes genetic data to tailor personalized treatments based on an individual\\'s genetic profile.\\r\\rSingle-cell RNA sequencing\\rHelps understand cellular diversity and the role of specific genes in diseases.\\rDisease Diagnosis\\rAI in medical imaging (e.g., radiology, pathology)\\rImproves accuracy in detecting abnormalities such as tumors and lesions in scans, speeding up diagnosis.\\r\\rPredictive analytics\\rAnalyzes health records to predict disease onset and progression (e.g., diabetes, cardiovascular diseases).\\rUnderstanding Disease Mechanisms\\rMolecular dynamics simulations\\rAI-enhanced simulations offer insights into protein interactions and disease pathways.\\r\\rSystems biology\\rIntegrates data from genomics, proteomics, etc., to understand complex disease mechanisms and identify new therapeutic targets.\\rVaccine Development\\rRapid antigen prediction\\rAI predicts vaccine candidates by modeling immune responses, accelerating vaccine development.\\rHealth Monitoring & Management\\rWearable technology\\rAI analyzes real-time health metrics from wearables, aiding early disease detection and personal health insights.\\r\\rTelemedicine and AI chatbots\\rEnhances patient interaction, provides symptom assessment, and facilitates remote healthcare services.\\rMental Health\\rSentiment analysis in social media\\rDetects early signs of mental health issues by analyzing language patterns, enabling timely intervention.\\r\\r\\n\\r\\n\\r\\nA table that outlines the most common optimization algorithms used in deep learning- Programming Ocean Academy\\rAlgorithm\\rDescription\\rCommon Uses\\rStochastic Gradient Descent (SGD)\\rUpdates weights after each training example rather than the entire dataset, making training faster.\\rLarge datasets; simple models; used for faster training, though convergence may be unstable.\\rMini-Batch Gradient Descent\\rUses small batches of data for weight updates, combining the benefits of SGD and full-batch methods.\\rPreferred in most deep learning applications for stable convergence and efficiency.\\rMomentum\\rAccelerates gradient descent by adding a fraction of the previous update to the current one.\\rUsed to avoid local minima and speed up convergence in deep networks.\\rRMSProp\\rAdapts the learning rate by dividing it by a running average of the magnitudes of recent gradients.\\rPopular for recurrent neural networks (RNNs) and noisy data; good for non-stationary environments.\\rAdam (Adaptive Moment Estimation)\\rCombines RMSProp and Momentum; computes adaptive learning rates for each parameter using first and second moments.\\rWidely used in deep learning for fast, efficient training across many tasks, including NLP and vision.\\rNadam\\rAdam with Nesterov momentum; looks ahead and adjusts gradient descent accordingly.\\rMore efficient than Adam in some cases; used for faster convergence in image classification and NLP.\\rAdaGrad\\rAdapts learning rate for each parameter individually based on the sum of historical squared gradients.\\rSuitable for sparse data; used in text classification, recommendation systems, and NLP tasks.\\rAdadelta\\rImproves AdaGrad by limiting the accumulation of past gradients to a fixed window size.\\rHandles decaying learning rates better than AdaGrad; useful in image recognition and speech tasks.\\rL-BFGS\\rA second-order optimization algorithm that approximates the Hessian matrix for faster convergence.\\rSmall datasets; shallow networks; sometimes used in fine-tuning pre-trained models.\\rNAG (Nesterov Accelerated Gradient)\\rA variant of Momentum that uses a look-ahead mechanism to adjust gradients.\\rFaster convergence compared to vanilla momentum; commonly used in deep networks.\\r\\r\\n\\r\\n\\r\\nA comprehensive table that outlines the common optimization algorithms in deep learning, along with their strengths and weaknesses-Programming Ocean Academy\\rAlgorithm\\rStrengths\\rWeaknesses\\rStochastic Gradient Descent (SGD)\\r\\n \\r\\n \\r- Fast updates with each training example\\r\\n- Good for large datasets\\r\\n- Can escape local minima\\r- High variance in updates\\r\\n- May converge slowly or oscillate if learning rate is too high\\r\\n \\rMini-Batch Gradient Descent\\r\\n \\r\\n \\r- Balances speed and convergence\\r\\n- Reduces variance in updates\\r\\n- Faster than full-batch\\r- Requires careful tuning of batch size\\r\\n- May still struggle with local minima\\r\\n \\rMomentum\\r\\n \\r\\n \\r- Speeds up convergence\\r\\n- Reduces oscillation\\r\\n- Helps escape local minima\\r- Can overshoot minima\\r\\n- Requires tuning of momentum parameter\\r\\n \\rRMSProp\\r\\n \\r- Adaptive learning rate helps with varying gradients\\r\\n- Effective for non-stationary objectives\\r- Requires tuning of decay factor\\r\\n- Can be sensitive to hyperparameter choices\\rAdam (Adaptive Moment Estimation)\\r\\n \\r\\n \\r- Combines advantages of RMSProp and Momentum\\r\\n- Fast convergence\\r\\n- Well-suited for large datasets\\r- Can be overly aggressive with learning rates\\r\\n- May lead to suboptimal solutions in some cases\\r\\n \\rNadam\\r\\n \\r- Incorporates Nesterov momentum for faster convergence\\r\\n- Often performs better than Adam\\r- More complex than Adam\\r\\n- Still requires careful tuning of hyperparameters\\rAdaGrad\\r\\n \\r- Good for sparse data and features\\r\\n- Adapts learning rate based on parameter importance\\r- Learning rate can decrease too quickly, leading to premature convergence\\r\\n \\rAdadelta\\r\\n \\r- Addresses learning rate decay issue in AdaGrad\\r\\n- Keeps track of recent gradients\\r- Still requires tuning of decay parameters\\r\\n- May converge slowly for some tasks\\rL-BFGS\\r\\n \\r- Fast convergence with fewer iterations\\r\\n- Good for small datasets\\r- Memory-intensive due to second-order approximation\\r\\n- Less suitable for large datasets\\rNAG (Nesterov Accelerated Gradient)\\r\\n \\r- More responsive to changes in the loss landscape\\r\\n- Generally faster convergence compared to Momentum\\r- More complex implementation\\r\\n- Requires careful tuning of parameters\\r\\r\\n\\r\\nA comparison between learning rate, loss rate, overfitting, and underfitting- Programming Ocean Academy\\rAspect\\rLearning Rate\\rLoss Rate\\rOverfitting\\rUnderfitting\\rDefinition\\rThe learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function.\\rThe loss rate refers to the error or difference between the predicted output and the actual output, typically measured by a loss function (e.g., MSE, Cross-Entropy).\\rOverfitting occurs when a model learns the training data too well, including noise and details, leading to poor generalization on unseen data.\\rUnderfitting happens when a model is too simple and fails to capture the underlying patterns in the data.\\rEffect on Model\\rControls how fast the model updates weights. A high learning rate can cause the model to converge too quickly to a suboptimal solution, while a low learning rate can lead to slow convergence or getting stuck in local minima.\\rA high loss rate indicates poor model performance, while a low loss rate indicates that the model\\'s predictions are closer to the actual values.\\rResults in high accuracy on training data but poor performance on testing data due to memorization of noise rather than learning the general pattern.\\rLeads to both poor training and testing accuracy because the model is too simplistic to capture complex patterns.\\rCauses\\rA high learning rate may skip the optimal solution, and a low learning rate may take too long to converge.\\rHigh loss rate occurs due to a poorly trained model, improper hyperparameters, or incorrect data preprocessing.\\rToo complex models with too many parameters, insufficient regularization, or too much training time.\\rToo simple models with insufficient complexity, inadequate training, or incorrect features.\\rSymptoms\\rHigh learning rate: unstable training and large loss oscillations. Low learning rate: very slow convergence.\\rHigh loss rate: large errors between predictions and true values. Low loss rate: predictions are close to true values.\\rHigh training accuracy and low testing accuracy (large gap).\\rLow accuracy on both training and testing data (small gap between them).\\rRelationship with Model Performance\\rOptimal learning rate ensures efficient convergence and better model performance.\\rA low loss rate indicates good model performance, while a high loss rate indicates poor performance and incorrect predictions.\\rHurts generalization by fitting the model too closely to the training data, capturing noise.\\rReduces performance as the model fails to learn important patterns, leading to low predictive accuracy.\\rPrevention\\rAdjusting the learning rate through techniques like learning rate schedules, adaptive learning rates, or grid search.\\rImprove model training, tune hyperparameters, use better loss functions, and improve data preprocessing.\\rUse techniques such as cross-validation, regularization (L1/L2), dropout, or reducing model complexity.\\rIncrease model complexity, add more features, improve feature engineering, or train for more epochs.\\rExample\\rLearning rate is a key parameter in gradient descent algorithms used in deep learning and machine learning models.\\rLoss rate is measured through loss functions such as Mean Squared Error (MSE) or Cross-Entropy in classification tasks.\\rA neural network with too many layers or a decision tree with too deep branches could overfit the data.\\rA linear regression model trying to fit nonlinear data or a shallow decision tree can lead to underfitting.\\r\\r\\n\\r\\n\\r\\n\\r\\nA comparison between convergence and divergence in the context of AI - Programming Ocean Academy\\rAspect\\rConvergence\\rDivergence\\rDefinition\\rConvergence refers to the process where a machine learning model\\'s training stabilizes over time, meaning the loss function or error decreases and approaches a minimum value.\\rDivergence refers to when a machine learning model\\'s training becomes unstable, where the loss function or error increases or fluctuates indefinitely without stabilizing.\\rBehavior in Training\\rThe model\\'s performance improves with each iteration as it moves toward the optimal solution.\\rThe model fails to learn effectively, resulting in increasing or highly oscillating loss, and the model moves away from the optimal solution.\\rImpact on Model\\rLeads to better generalization and improved predictive accuracy on unseen data.\\rPrevents the model from learning meaningful patterns, resulting in poor accuracy and poor generalization.\\rCauses\\r- Appropriate learning rate\\r\\n- Proper model architecture\\r\\n- Correct data preprocessing and loss function\\r- Learning rate is too high\\r\\n- Model is too complex\\r\\n- Improper data preprocessing or feature scaling\\r\\n- Incorrect gradient descent algorithm\\rSymptoms\\r- Gradual and steady decrease in loss function\\r\\n- Improved accuracy on both training and testing data\\r\\n- Stability during optimization\\r- Increasing or erratic loss function\\r\\n- Decreasing accuracy on both training and testing data\\r\\n- Instability during optimization (fluctuations in loss)\\rRelationship to Learning Rate\\rA proper learning rate helps the model converge to a global or local minimum.\\rA too-high learning rate often causes divergence, where the model overshoots the optimal solution repeatedly.\\rModel\\'s Path\\rThe model follows a path that narrows down to the minimum of the loss function, effectively optimizing its parameters.\\rThe model\\'s path moves erratically or away from the minimum of the loss function, resulting in poor parameter optimization.\\rPrevention\\r- Use an appropriate learning rate (with tuning)\\r\\n- Proper data normalization and regularization\\r\\n- Cross-validation to monitor progress\\r- Reduce learning rate\\r\\n- Simplify model architecture\\r\\n- Ensure correct data scaling and processing\\rExample\\rA well-tuned deep neural network trained using gradient descent that successfully reduces the loss and achieves high accuracy.\\rA deep neural network trained with a very high learning rate, causing the model\\'s loss function to increase drastically after each iteration.\\r\\r\\n\\r\\n\\r\\nA table summarizing the learning rate, loss function behavior, convergence, and divergence cases - Programming Ocean Academy\\rAspect\\rLow Learning Rate (Convergence)\\rHigh Learning Rate (Divergence)\\rOptimal Learning Rate (Convergence)\\rLearning Rate\\rVery small (e.g., 0.001)\\rToo high (e.g., 10)\\rBalanced (e.g., 0.01 - 0.1)\\rLoss Function Behavior\\rDecreases slowly over many iterations\\rLoss fluctuates or increases dramatically\\rDecreases steadily and reaches a minimum\\rConvergence\\rSlow convergence to the minimum, stable but takes longer\\rNo convergence, model fails to find the minimum, unstable\\rQuick convergence with stable optimization\\rDivergence\\rNone, but may take excessive time to optimize\\rHigh, model parameters oscillate or overshoot\\rNone, learning is balanced and efficient\\rTraining Loss\\rGradually decreases but can plateau for long periods\\rFluctuates wildly, increases or stays high\\rDecreases steadily, reaching low values\\rTest Loss\\rLow, but improvement is slow\\rHigh, indicating poor generalization\\rLow, with efficient training and good generalization\\rDecision Boundary\\rSmooth and correct, but may be delayed\\rErratic and inaccurate, model overfits or diverges\\rWell-defined and optimal for the dataset\\rWeight Adjustments\\rSmall, steady adjustments, slow progress\\rLarge adjustments, causing instability\\rAppropriate, balanced adjustments\\rTraining Behavior\\rStable but slow learning process\\rUnstable, unable to learn effectively\\rEfficient and stable learning process\\r\\r\\n\\r\\n\\r\\n\\r\\nA comparison of the most common metrics used to evaluate classification AI models- Programming Ocean Academy\\rMetric\\rDefinition\\rUse Case\\rFormula\\rRange\\rInterpretation\\rAccuracy\\rThe proportion of correctly classified instances (both true positives and true negatives) out of the total instances.\\rGood for balanced datasets.\\rAccuracy=  (TP+TN)/(TP+TN+FP+FN)\\r0 to 1\\rHigher values indicate better performance.\\rPrecision\\rThe proportion of true positive predictions out of all positive predictions. Measures how many selected items are relevant.\\rImportant when false positives are costly.\\rPrecision =  TP/(TP + FP)\\r0 to 1\\rHigh precision means fewer false positives.\\rRecall (Sensitivity)\\rThe proportion of actual positives correctly classified. Measures how many relevant items are selected.\\rImportant when false negatives are costly.\\rRecall (Sensitivity)=  TP/(TP+FN)\\r0 to 1\\rHigh recall means fewer false negatives.\\rF1 Score\\rThe harmonic mean of precision and recall. Provides a balanced measure when classes are imbalanced.\\rUseful in imbalanced datasets.\\rF1 Score=2 x   (Precision x Recall)/(Precision+Recall)\\r0 to 1\\rHigher F1 indicates a better balance between precision and recall.\\rSpecificity\\rThe proportion of true negatives correctly classified. Focuses on how well the model identifies negative instances.\\rCritical when detecting negatives is important.\\rSpecificity=  TN/(TN+FP)\\r0 to 1\\rHigh specificity means fewer false positives.\\r\\r\\n\\r\\n\\r\\nA comparison of the most common metrics used to evaluate classification AI models- Programming Ocean Academy\\rMetric\\rDefinition\\rUse Case\\rFormula\\rRange\\rInterpretation\\rROC-AUC\\rThe Area Under the Receiver Operating Characteristic Curve. Measures the ability of the model to discriminate between classes across various threshold settings.\\rUseful to compare models regardless of thresholds.\\r(ROC-AUC does not have a specific formula for direct calculation in classification, but it measures the area under the ROC curve.)\\r0.5 to 1\\rHigher AUC indicates better discrimination between positive and negative classes.\\rLog Loss (Cross-Entropy Loss)\\rMeasures the uncertainty of predictions by comparing predicted probabilities with actual class labels.\\rImportant in probabilistic classification models.\\rLog Loss=-  1/N  ?Â¦?(y log(p)+?  (1-y)log(1-p)\\r0 to 8\\rLower values indicate more accurate probabilistic predictions.\\rBalanced Accuracy\\rA modification of accuracy, which takes the average of recall obtained on each class, especially useful in imbalanced datasets.\\rBest for imbalanced datasets.\\rBlanaced Accuracy=  (Recall Positive+Recall Negative)/2\\r0 to 1\\rHigher balanced accuracy means better performance across all classes.\\rCohen\\'s Kappa\\rA metric that accounts for chance agreement between true and predicted classifications.\\rUse for assessing agreement beyond chance.\\rK=  (Po-Pe)/(1-Pe)\\r-1 to 1\\rA value of 1 represents perfect agreement; 0 represents no agreement beyond chance; negative values indicate worse-than-random performance.\\r\\r\\n\\r\\n\\r\\nA comparison of the most common metrics used to evaluate classification AI models- Programming Ocean Academy\\rMetric\\rDefinition\\rUse Case\\rFormula\\rRange\\rInterpretation\\rMatthews Correlation Coefficient (MCC)\\rConsiders true and false positives and negatives. Balanced and informative even for imbalanced datasets.\\rUseful for highly imbalanced datasets.\\rMCC=  ((TP x TN)-(FP x FN))/v((TP+FP)(TP+FN)(TN+FP)(TN+FN))\\r-1 to 1\\r1 indicates perfect prediction; 0 is no better than random; -1 indicates total disagreement between prediction and reality.\\rG-Mean\\rThe geometric mean of sensitivity (recall) and specificity. Focuses on both the true positive and true negative rates.\\rImportant in imbalanced datasets.\\rG-MEAN= v(Recall x Sensitivity)\\r0 to 1\\rA higher G-Mean indicates better balance between identifying positives and negatives.\\rF2 Score\\rA variation of the F1 score, giving more weight to recall than precision. Useful when false negatives are more critical than false positives.\\rEmphasizes recall over precision.\\rF2=?(1+2)?^2  x  (Precision x Recall)/(?(2?^(2 ) x Precision)+Recall)\\r0 to 1\\rA higher value indicates a better balance, with more importance on recall.\\rFalse Positive Rate (FPR)\\rThe proportion of actual negatives that are incorrectly classified as positive.\\rUseful when false positives are highly undesirable.\\rFPR=  FP/(FP+TN)\\r0 to 1\\rLower FPR indicates fewer false positives.\\rFalse Negative Rate (FNR)\\rThe proportion of actual positives that are incorrectly classified as negative.\\rUseful when false negatives are costly.\\rFNR =  FN/(FN+TP)\\r0 to 1\\rLower FNR indicates fewer false negatives.\\r\\r\\n\\r\\n\\r\\n\\r\\nA comparison of the most common metrics used to evaluate regression (Predictions) AI models- Programming Ocean Academy\\rMetric\\rDefinition\\rUse Case\\rFormula\\rRange\\rInterpretation\\rMean Absolute Error (MAE)\\rThe average magnitude of absolute errors between predicted and actual values.\\rSuitable for models where error magnitude matters and the direction of errors (positive/negative) is not important.\\rMAE =  (?_(i=1)^nÂ¦?|y_(i -   ) y ~_(i  ) |?)/n\\r[0, 8)\\rLower MAE means the model\\'s predictions are closer to the actual values, indicating better performance.\\r\\nHigher MAE indicates larger average errors, meaning the model is less accurate.\\rMean Squared Error (MSE)\\rThe average of the squared differences between predicted and actual values, giving more weight to larger errors.\\rUse when you want to penalize large errors more heavily, making the model focus on reducing big mistakes.\\rMSE =1/n  ?_(i=1)^nÂ¦?(Y_(i-  ) Y ~_i )^2 ?\\r[0, 8)\\rLower MSE indicates better performance; large errors are more penalized due to squaring.\\rRoot Mean Squared Error (RMSE)\\rThe square root of MSE, providing errors in the same units as the target variable.\\rUse when you want a metric in the same unit as the predicted variable while still penalizing large errors.\\rRMSE =v((?_(I=1)^NÂ¦?(X_(I-    ) (X_I ) ~  )^2 ?)/N)\\r[0, 8)\\rSimilar to MSE but easier to interpret since it\\'s in the same units as the data.\\rR-squared (RÂ²)\\rThe proportion of the variance in the dependent variable explained by the independent variables in the model.\\rUse to understand how well your model explains the variability of the data.\\rR^2  =1-  RSS/TSS\\r\\nR^2=1-  (?_IÂ¦?(Y_(I )- (Y_i ) ~  )^2 ?)/(?_iÂ¦?( Y_(I ) ?- (Y ) ~)^2 )\\r[0, 1]\\rHigher RÂ² values indicate that the model explains more of the variance in the target variable.\\r\\r\\n\\r\\n\\r\\n\\r\\nThe main types of machine learning - Programming Ocean Academy\\rType of Machine Learning\\rDefinition\\rData Requirement\\rUse Cases\\rOutput\\rExamples\\rSupervised Learning\\rThe model is trained on labeled data, where the input and corresponding output (target) are known.\\rRequires large amounts of labeled data.\\rClassification (e.g., spam detection), regression (e.g., house price prediction).\\rPredicts labels or continuous values.\\rLinear Regression, Logistic Regression, Decision Trees, SVM, k-NN\\rUnsupervised Learning\\rThe model learns patterns and structures from data that has no labeled output, finding hidden structures and relationships.\\rDoes not require labeled data.\\rClustering (e.g., customer segmentation), anomaly detection, association rule learning (e.g., market basket analysis).\\rIdentifies patterns, groups, or relationships in the data.\\rK-Means Clustering, PCA (Principal Component Analysis), Hierarchical Clustering, Autoencoders\\rSemi-Supervised Learning\\rThe model is trained on a small amount of labeled data combined with a large amount of unlabeled data, leveraging both types for learning.\\rA mix of labeled and unlabeled data.\\rWeb content classification, image and video recognition, medical imaging analysis.\\rPredicts labels or uncovers patterns in the unlabeled data.\\rSemi-Supervised SVM, Self-training, Label Propagation\\rReinforcement Learning\\rThe model learns through trial and error by interacting with an environment, receiving rewards or penalties for actions, aiming to maximize cumulative reward over time.\\rNo labeled data, feedback through rewards/penalties.\\rGame playing (e.g., AlphaGo), robotics, autonomous vehicles, recommendation systems.\\rDecision-making policy or strategy to maximize rewards.\\rQ-Learning, Deep Q-Networks (DQN), Proximal Policy Optimization (PPO)\\r\\r\\n\\r\\nThe main types of Deep Learning architectures- Programming Ocean Academy\\rDeep Learning Type\\rDefinition\\rUse Cases\\rStrengths\\rExamples\\rFeedforward Neural Networks (FNN)\\rThe simplest type of neural network, where information flows in one direction from input to output.\\rImage classification, speech recognition, basic predictive tasks.\\rEasy to implement, works well for simple tasks.\\rMulti-Layer Perceptron (MLP), Shallow Networks.\\rConvolutional Neural Networks (CNN)\\rNeural networks designed to process and recognize patterns in grid-like data (e.g., images), using convolutional layers.\\rImage and video recognition, object detection, medical image analysis.\\rStrong spatial feature detection, great for image-based tasks.\\rLeNet, AlexNet, VGG, ResNet, Inception.\\rRecurrent Neural Networks (RNN)\\rNetworks with loops allowing information to persist, enabling sequential data processing.\\rTime series prediction, natural language processing (NLP), speech.\\rHandles sequential data and temporal dependencies well.\\rLong Short-Term Memory (LSTM), Gated Recurrent Units (GRU).\\rGenerative Adversarial Networks (GAN)\\rConsists of two neural networks (generator and discriminator) competing against each other to generate new data.\\rImage generation, deepfake creation, data augmentation, art generation.\\rCan create highly realistic data, used in unsupervised learning.\\rDCGAN, StyleGAN, CycleGAN.\\rAutoencoders\\rA type of neural network used for learning efficient codings by compressing and reconstructing data.\\rAnomaly detection, image noise reduction, data compression.\\rGood at dimensionality reduction and unsupervised learning tasks.\\rVariational Autoencoder (VAE), Denoising Autoencoder.\\rTransformer Networks\\rNeural networks designed for processing sequential data, using self-attention mechanisms to capture global dependencies.\\rMachine translation, text generation, NLP tasks, speech recognition.\\rEfficient for long-range dependencies, parallel processing.\\rGPT, BERT, Transformer, T5, Vision Transformer (ViT).\\rDeep Belief Networks (DBN)\\rProbabilistic generative models composed of multiple layers of stochastic, latent variables.\\rFeature extraction, pre-training deep networks, unsupervised learning.\\rGood at unsupervised learning, extracting hierarchical representations.\\rStacked Restricted Boltzmann Machine (RBM).\\rSelf-Organizing Maps (SOM)\\rA type of unsupervised neural network that projects high-dimensional data onto a low-dimensional grid.\\rData visualization, clustering, dimensionality reduction.\\rGood for data visualization and understanding of complex patterns.\\rKohonen Self-Organizing Map.\\r\\r\\n\\r\\n\\r\\nSummary of Common Trade-offs in AI Models Evaluation Metrics - Programming Ocean Academy\\rMetric Pair\\rTrade-off\\rAccuracy vs. Precision/Recall\\rAccuracy might be misleading on imbalanced data; precision and recall may not increase together.\\rPrecision vs. Recall\\rIncreasing one typically decreases the other, depending on the tolerance for false positives vs. false negatives.\\rBias vs. Variance\\rHigh bias leads to underfitting; high variance leads to overfitting. Balancing them is key for generalization.\\rMSE vs. MAE\\rMSE penalizes large errors more, while MAE treats all errors equally; choice depends on how important outliers are.\\rSpeed vs. Accuracy\\rMore accurate models are often slower and more resource-intensive; simpler models are faster but may be less accurate.\\r\\r\\n\\r\\nThe key concepts of Supervised Machine Learning- Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rSupervised learning is a type of machine learning where the model is trained on labeled data (input-output pairs).\\rGoal\\rTo learn a mapping function from input features (X) to the target output (Y) and make accurate predictions on new, unseen data.\\rInput Data\\rLabeled data, meaning each training example consists of both input data and the corresponding correct output label.\\rOutput\\rPredictions or classifications made by the model based on the input data.\\rTraining Process\\rThe model adjusts its internal parameters based on the labeled data to minimize the error (loss function) between predictions and actual labels.\\rTypes of Tasks\\r1. Classification - Predicts discrete categories (e.g., spam or not spam).\\r\\n2. Regression - Predicts continuous values (e.g., house prices).\\rCommon Algorithms\\r- Linear Regression (for regression)\\r\\n- Logistic Regression (for classification)\\r\\n- Support Vector Machine (SVM) \\r\\n- Decision Trees\\r\\n- Random Forest\\r\\n- K-Nearest Neighbors (KNN)\\r\\n- Neural Networks\\rLoss Function\\rMeasures the difference between the predicted output and the actual output. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy for classification.\\rTraining Phase\\rThe model is fed labeled data to learn from. The goal is to minimize the loss function using algorithms like gradient descent.\\rTesting Phase\\rAfter training, the model is evaluated on new data (test data) to see how well it generalizes to unseen examples.\\rEvaluation Metrics\\r- Accuracy (for classification tasks)\\r\\n- Precision and Recall (for imbalanced classification)\\r\\n- Mean Absolute Error (MAE) (for regression)\\r\\n- R-squared (for regression)\\rAdvantages\\r- Models can be highly accurate if trained on sufficient and representative data.\\r\\n- Provides direct feedback by comparing predictions to actual labels.\\rDisadvantages\\r- Requires large amounts of labeled data, which can be expensive to collect.\\r\\n- May not generalize well to new, unseen data if the model overfits.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Unsupervised Machine Learning - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rUnsupervised learning is a type of machine learning where the model is trained on data without explicit labels or supervision.\\rGoal\\rTo identify patterns, structures, or relationships within data without predefined output labels.\\rInput Data\\rUnlabeled data, meaning the training examples consist only of input data without corresponding target labels.\\rOutput\\rPatterns, clusters, or reduced dimensions, providing insights into the structure of the data.\\rTraining Process\\rThe model identifies hidden patterns in the data, like clusters or associations, based on the input features alone.\\rTypes of Tasks\\r1. Clustering - Grouping similar data points (e.g., segmenting customers).\\r\\n2. Dimensionality Reduction - Reducing the number of features while preserving data variability (e.g., PCA).\\rCommon Algorithms\\r- K-Means Clustering\\r\\n- Hierarchical Clustering\\r\\n- DBSCAN\\r\\n- Principal Component Analysis (PCA)\\r\\n- t-SNE (for visualization)\\r\\n- Autoencoders\\rLoss Function\\rNo explicit loss function is used as there are no labels, but models often minimize some distance measure (e.g., Euclidean distance in clustering).\\rTraining Phase\\rThe model explores the structure of the input data, organizing it based on similarity or other characteristics, without needing labels.\\rTesting Phase\\rGenerally, unsupervised models are not evaluated with test data, but the quality of patterns (like cluster coherence) may be assessed using metrics like Silhouette Score.\\rEvaluation Metrics\\r- Silhouette Score (for clustering)\\r\\n- Inertia (sum of squared distances from the cluster center)\\r\\n- Variance Explained (for dimensionality reduction)\\rAdvantages\\r- No need for labeled data, which can be difficult or expensive to obtain.\\r\\n- Useful for exploring hidden structures and gaining insights from raw data.\\rDisadvantages\\r- Hard to evaluate since there is no ground truth for comparison.\\r\\n- Results can be harder to interpret, and performance depends on the specific problem and data.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Semi-Supervised Machine Learning- Programming Ocean Academy\\rAspect\\t\\rExplanation\\rDefinition\\rSemi-supervised learning combines elements of both supervised and unsupervised learning, using a small amount of labeled data and a large amount of unlabeled data.\\rGoal\\rTo leverage both labeled and unlabeled data to improve model performance while reducing the need for extensive labeled datasets.\\rInput Data\\rA combination of labeled data (typically a small subset) and a larger amount of unlabeled data.\\rOutput\\rPredictions or classifications, similar to supervised learning, but informed by both labeled and unlabeled data.\\rTraining Process\\rThe model is initially trained on the labeled data, then leverages patterns in the unlabeled data to refine and improve its predictions.\\rTypes of Tasks\\r1. Classification - Predicting categories using a small labeled dataset with additional unlabeled examples.\\r\\n2. Regression - Predicting continuous values by leveraging both labeled and unlabeled datasets.\\rCommon Algorithms\\r- Self-training\\r\\n- Co-training\\r\\n- Transductive SVM (TSVM)\\r\\n- Generative Models\\r\\n- Graph-based Methods\\rLoss Function\\rCombines supervised loss (e.g., Cross-Entropy for classification) on labeled data with an unsupervised component (e.g., entropy minimization or consistency regularization) for unlabeled data.\\rTraining Phase\\rInitially, the model learns from labeled data. Then it uses the structure in the unlabeled data to generalize and improve performance, often iteratively.\\rTesting Phase\\rLike supervised learning, the model is evaluated on a test set, usually consisting of labeled data, to measure generalization performance.\\rEvaluation Metrics\\r- Accuracy (for classification)\\r\\n- Precision/Recall\\r\\n- Root Mean Squared Error (RMSE) (for regression)\\r\\n- F1 Score\\rAdvantages\\r- Reduces the need for large amounts of labeled data.\\r\\n- Can significantly improve model performance by utilizing large amounts of unlabeled data.\\r\\n- Often more practical in real-world scenarios where labeling is costly or time-consuming.\\rDisadvantages\\r- Requires careful tuning of how labeled and unlabeled data are combined.\\r\\n- Unlabeled data can introduce noise or incorrect patterns if not handled correctly.\\r\\n- Complexity increases due to the hybrid approach.\\r\\r\\n\\r\\n\\r\\n\\r\\nThe key concepts of Reinforcement Machine Learning- Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rReinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties.\\rGoal\\rTo learn a policy that maximizes the cumulative reward by taking actions that lead to favorable outcomes over time.\\rInput Data\\rThe agent receives feedback from the environment in the form of states (representing the current situation) and rewards based on its actions.\\rOutput\\rA sequence of actions or decisions aimed at maximizing cumulative rewards (long-term success).\\rTraining Process\\rThe agent explores the environment and learns through trial and error. It updates its policy (decision-making strategy) based on the rewards or penalties received.\\rTypes of Tasks\\r1. Markov Decision Processes (MDPs) - Standard RL tasks with discrete states and actions.\\r\\r2. Games and Robotics - The agent interacts with dynamic environments, such as playing a game or controlling a robot.\\rCommon Algorithms\\r- Q-Learning\\r\\n- Deep Q-Networks (DQN)\\r\\n- Policy Gradient Methods\\r\\n- Actor-Critic Algorithms\\r\\n- Proximal Policy Optimization (PPO)\\rLoss Function\\rThe reward signal drives learning. The goal is to maximize the cumulative reward (discounted reward over time), often through techniques like Bellman equations.\\rTraining Phase\\rThe agent starts by exploring the environment and experimenting with different actions. Over time, it learns to take actions that maximize rewards using strategies like exploitation (choosing known good actions) and exploration (trying new actions).\\rTesting Phase\\rThe agent is evaluated based on how well it can maximize cumulative rewards in the environment after training. Typically, the agent\\'s performance is measured by the total reward it accumulates during testing.\\rEvaluation Metrics\\r- Total Reward (sum of rewards over time)\\r\\n- Convergence (how quickly the agent learns)\\r\\n- Exploration vs. Exploitation Tradeoff (balance between trying new actions and using learned actions)\\rAdvantages\\r- Useful for dynamic and complex environments where the agent learns through experience.\\r\\n- Can achieve superhuman performance in tasks like games or robotics.\\r\\n- Does not require labeled data, only a reward signal.\\rDisadvantages\\r- Training can be slow and computationally expensive.\\r\\n- Exploration can lead to suboptimal decisions or unsafe actions in real-world environments.\\r\\n- Requires careful tuning of reward structures and exploration strategies.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Feedforward Neural Networks (FNN) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rA Feedforward Neural Network (FNN) is the simplest type of artificial neural network where information flows in one direction-from input nodes, through hidden nodes (if any), to output nodes.\\rGoal\\rTo map input data to the appropriate output using a series of weighted connections, activation functions, and learned parameters.\\rInput Data\\rTypically structured data, such as numerical features, pixel values for images, or word embeddings for text.\\rOutput\\rDepends on the task:\\r\\n- Classification: Probabilities of class membership.\\r\\n- Regression: Continuous values.\\rArchitecture\\rComposed of an input layer, one or more hidden layers, and an output layer. Information moves forward through these layers.\\rTraining Process\\rUses backpropagation and an optimization algorithm (like Stochastic Gradient Descent) to update weights based on the difference between predicted output and actual output (error).\\rActivation Functions\\rCommon activation functions include:\\r\\n- ReLU (Rectified Linear Unit) for hidden layers.\\r\\n- Sigmoid or Softmax for output in classification tasks.\\r\\n- Linear for regression tasks.\\rLoss Function\\r- Mean Squared Error (MSE) for regression.\\r\\n- Cross-Entropy Loss for classification tasks.\\rTraining Phase\\rThe network is initialized with random weights. It learns through multiple epochs, adjusting weights by minimizing the error on the training data using backpropagation.\\rTesting Phase\\rOnce trained, the network is evaluated on a separate test dataset to assess its ability to generalize to new, unseen data.\\rEvaluation Metrics\\r- Accuracy (for classification tasks).\\r\\n- Mean Squared Error (MSE) or RÂ² (for regression).\\r\\n- Precision, Recall, F1-Score (for specific classification needs).\\rAdvantages\\r- Simple and efficient for structured data tasks.\\r\\n- Can approximate any continuous function given enough hidden units (Universal Approximation Theorem).\\rDisadvantages\\r- Prone to overfitting, especially with small datasets.\\r\\n- Not suitable for sequential or spatial data (e.g., images, time-series) without modifications like CNNs or RNNs.\\r\\n- Cannot capture complex relationships in data as effectively as other deep learning architectures.\\rUse Cases\\r- Simple classification tasks (e.g., binary classification or multi-class classification).\\r\\n- Basic regression tasks (e.g., house price prediction).\\r\\n- Feature extraction or dimensionality reduction.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Convolutional Neural Networks (CNN) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rConvolutional Neural Networks (CNN) are a class of deep learning networks designed to process grid-like data structures such as images by automatically learning spatial hierarchies of features.\\rGoal\\rTo efficiently recognize patterns and features in data (especially images) by using convolutional layers to detect spatial structures and reduce the number of parameters compared to fully connected networks.\\rInput Data\\rPrimarily image data (pixel grids), but also applicable to time series, video, and other structured data.\\rOutput\\rDepends on the task:\\r\\n- Classification: Predicts the probability of each class (e.g., image classification).\\r\\n- Localization/Detection: Identifies objects and their locations within images.\\r\\n- Regression: Predicts continuous values.\\rArchitecture\\rConsists of the following layers:\\r\\n- Convolutional Layers to extract features.\\r\\n- Pooling Layers for down sampling.\\r\\n- Fully Connected Layers for final decision-making.\\rConvolutional Layers\\rApplies filters (kernels) to the input data to create feature maps that capture essential features like edges, textures, and patterns.\\rPooling Layers\\rReduces the spatial dimensions of the feature maps (often using Max Pooling) to decrease computational load and focus on the most prominent features.\\rFully Connected Layers\\rConnects every neuron from the previous layer to the next, typically used at the end of the network for classification or regression tasks.\\rTraining Process\\rTrained using backpropagation and an optimizer (e.g., Adam or SGD). The network adjusts weights in both convolutional and fully connected layers by minimizing a loss function.\\rActivation Functions\\r- ReLU (Rectified Linear Unit): Commonly used after convolutional layers to introduce non-linearity.\\r\\n- Softmax: Used in the final layer for classification tasks to convert logits to class probabilities.\\rLoss Function\\r- Cross-Entropy Loss for classification tasks.\\r\\n- Mean Squared Error (MSE) for regression tasks.\\rTraining Phase\\rThe network learns filters and weights by passing data through the layers, minimizing the error over multiple iterations (epochs). Overfitting is often mitigated with dropout and data augmentation.\\rTesting Phase\\rAfter training, the network is evaluated on a separate test dataset to measure how well it generalizes to new, unseen data.\\rEvaluation Metrics\\r- Accuracy (for classification).\\r\\n- Precision, Recall, F1-Score (for object detection or segmentation).\\r\\n- IoU (Intersection over Union) (for localization tasks).\\rAdvantages\\r- Automatically detects important features in images (no manual feature engineering required).\\r\\n- Highly efficient for large image datasets.\\r\\n- Well-suited for image classification, object detection, and segmentation.\\rDisadvantages\\r- Computationally expensive, requiring high-performance GPUs.\\r\\n- Requires large amounts of labeled data for training.\\r\\n- Susceptible to overfitting without proper regularization.\\rUse Cases\\r- Image Classification: (e.g., recognizing objects in photos).\\r\\n- Object Detection: (e.g., identifying multiple objects in an image).\\r\\n- Image Segmentation: (e.g., medical image analysis).\\r\\n- Facial Recognition, Video Processing, and Autonomous Driving.\\r\\r\\n\\r\\n\\r\\n\\r\\nThe key concepts of Recurrent Neural Networks (RNN) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rRecurrent Neural Networks (RNN) are a class of neural networks designed to handle sequential data by maintaining a \"memory\" of previous inputs through loops within the network.\\rGoal\\rTo model time-dependent or sequential relationships in data, such as predicting future values based on past inputs or understanding sequential patterns.\\rInput Data\\rSequential data such as time series, text (sentences), speech, or video frames.\\rOutput\\r- Many-to-One: One output for the entire sequence (e.g., sentiment analysis).\\r\\n- Many-to-Many: One output for each input in the sequence (e.g., machine translation, video frame prediction).\\rArchitecture\\rConsists of recurrent layers where the output of a neuron is fed back into the input for the next step. This creates loops that allow the network to maintain information about past inputs.\\rRecurrent Layers\\rEach recurrent neuron has connections to the next neuron as well as feedback connections to itself, allowing it to retain information from previous time steps.\\rTraining Process\\rTrained using backpropagation through time (BPTT), where the gradient is propagated backward through the sequence to update weights.\\rActivation Functions\\r- Tanh: Often used in hidden layers to introduce non-linearity.\\r\\n- Sigmoid: Used in the output layer for binary classification tasks.\\r\\n- Softmax: Used for multi-class classification.\\rLoss Function\\r- Cross-Entropy Loss for classification tasks.\\r\\r- Mean Squared Error (MSE) for regression tasks or continuous data predictions.\\rTraining Phase\\rThe network processes sequences of data step by step, updating weights by minimizing the loss using gradient descent techniques (e.g., SGD, Adam).\\rTesting Phase\\rThe model is evaluated on unseen sequences to assess how well it generalizes to new sequential data by using past information from the training phase.\\rEvaluation Metrics\\r- Accuracy (for classification tasks).\\r\\n- Perplexity (for language models).\\r\\n- Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) (for regression or time-series forecasting).\\rAdvantages\\r- Can handle variable-length input sequences.\\r\\n- Remembers previous inputs, making it suitable for tasks like language modeling, machine translation, and time-series forecasting.\\rDisadvantages\\r- Vanishing/exploding gradient problem, which makes it hard to learn long-term dependencies.\\r\\n- Slower training due to sequential processing.\\r\\n- Prone to overfitting, especially on small datasets.\\rVariants\\r- LSTMs (Long Short-Term Memory networks): Designed to combat the vanishing gradient problem by learning long-term dependencies.\\r\\n- GRUs (Gated Recurrent Units): A simpler version of LSTMs with fewer parameters.\\rUse Cases\\r- Time-Series Forecasting: (e.g., stock price prediction, weather forecasting).\\r\\n- Natural Language Processing (NLP): (e.g., language translation, text generation).\\r\\n- Speech Recognition and Video Processing.\\r\\r\\n\\r\\nThe key concepts of Generative Adversarial Networks (GANs) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rGenerative Adversarial Networks (GANs) are a type of deep learning model composed of two networks, a generator and a discriminator, that compete with each other. The generator creates new data instances, while the discriminator evaluates them.\\rGoal\\rThe goal of GANs is to generate new, realistic data samples (e.g., images, audio, or text) that are indistinguishable from real data. The generator aims to fool the discriminator, while the discriminator learns to distinguish between real and fake data.\\rInput Data\\rInput to the generator is typically random noise (latent space vectors), while the discriminator takes both real data (from the training set) and fake data (generated by the generator).\\rOutput\\r- Generator Output: A data instance (e.g., an image) generated from random noise.\\r\\n- Discriminator Output: A probability score indicating whether the input data is real or fake.\\rArchitecture\\rGANs consist of two networks:\\r\\n- Generator Network: Learns to create realistic data by transforming random noise into meaningful samples.\\r\\n- Discriminator Network: Learns to distinguish between real and generated data, providing feedback to the generator.\\rTraining Process\\rGANs are trained in a two-step adversarial process:\\r\\n1. Discriminator Training: The discriminator is trained to correctly classify real vs. fake data.\\r\\n2. Generator Training: The generator is trained to produce data that fools the discriminator. Both networks are optimized iteratively.\\rLoss Function\\r- Binary Cross-Entropy Loss is commonly used for both the generator and the discriminator. The generator\\'s objective is to maximize the probability that the discriminator is mistaken.\\rAdversarial Process\\r- The discriminator and generator play a minimax game:\\r\\n- The generator tries to minimize the discriminator\\'s ability to correctly classify fake samples.\\r\\n- The discriminator tries to maximize its accuracy in distinguishing real from fake data.\\rTraining Challenges\\r- Mode collapse: The generator may learn to produce a limited variety of samples.\\r\\n- Training instability: GANs are challenging to train because the generator and discriminator must remain balanced.\\r\\n- Sensitive to hyperparameters and learning rate.\\rEvaluation Metrics\\r- Inception Score (IS): Measures the quality and diversity of generated samples.\\r\\n- Frechet Inception Distance (FID): Quantifies the similarity between real and generated data distributions.\\rAdvantages\\r- Capable of generating highly realistic data.\\r\\n- Applicable to various tasks like image generation, style transfer, and data augmentation.\\r\\n- GANs can learn unsupervised from unlabeled data.\\rDisadvantages\\r- Difficult to train and often requires fine-tuning of hyperparameters.\\r\\n- Susceptible to mode collapse, where the generator produces limited variations of data.\\r\\n- Computationally expensive, often requiring substantial resources.\\rVariants\\r- DCGAN (Deep Convolutional GAN): Uses CNNs for both the generator and discriminator, suited for image generation.\\r\\n- CycleGAN: Used for image-to-image translation without paired data (e.g., converting images from one domain to another).\\r\\n- WGAN (Wasserstein GAN): Improves GAN stability by using a different loss function based on Wasserstein distance.\\rUse Cases\\r- Image Generation: (e.g., generating realistic human faces, art, or synthetic images).\\r\\n- Style Transfer: (e.g., applying artistic styles to photos).\\r\\n- Data Augmentation: Generating new training samples for machine learning tasks.\\r\\n- Video Prediction, Text-to-Image Synthesis, and Super-Resolution.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Transformer Neural Networks Models (TNN) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rTransformer Networks are a deep learning model architecture that relies on a self-attention mechanism, making them highly effective for processing sequential data (e.g., text, time-series) without requiring recurrent connections.\\rGoal\\rTo efficiently model long-range dependencies in sequential data by focusing on the most relevant parts of the input through the use of self-attention. Transformers are mainly used for tasks like machine translation, text generation, and natural language understanding.\\rInput Data\\rTypically, sequential data such as text, time-series, or even images (with modifications). Text data is often tokenized into word embeddings (e.g., Word2Vec, BERT embeddings).\\rOutput\\r- Many-to-One: Outputting a single prediction for the entire sequence (e.g., sentiment classification).\\r\\n- Many-to-Many: Outputting a sequence (e.g., machine translation, text generation).\\rArchitecture\\r- Encoder-Decoder Architecture: Consists of two main parts:\\r\\n- Encoder: Processes input data and generates a context-rich representation.\\r\\n- Decoder: Uses this representation to produce the output sequence.\\rSelf-Attention Mechanism\\rEach token (word, element) in the input sequence can \"attend\" to every other token in the sequence to capture dependencies, regardless of their distance from each other. This allows the model to understand contextual relationships effectively.\\rPositional Encoding\\rSince Transformers don\\'t have a built-in understanding of the order of sequence (like RNNs), positional encoding is added to the input embeddings to provide information about the order of tokens.\\rMulti-Head Attention\\rA key feature where the model applies the attention mechanism multiple times in parallel (heads), allowing the network to focus on different parts of the sequence simultaneously.\\rFeedforward Layers\\rAfter attention is applied, each output from the attention heads passes through fully connected feedforward layers, followed by normalization and residual connections to ensure effective learning.\\rTraining Process\\rTransformers are trained using backpropagation and gradient descent (often using the Adam optimizer). The loss function (typically Cross-Entropy Loss) is minimized for tasks like machine translation or text generation.\\rLoss Function\\r- Cross-Entropy Loss is most commonly used for language tasks such as text generation, translation, and sequence prediction.\\rAttention Heads\\rDifferent attention heads allow the network to focus on multiple parts of the input sequence at once. They compute scaled dot-product attention in parallel.\\rEvaluation Metrics\\r- Perplexity: Commonly used to measure how well the model predicts a sequence of text.\\r\\n- BLEU Score: Used for evaluating machine translation by comparing the predicted translation to a reference translation.\\rAdvantages\\r- Can model long-range dependencies effectively without the limitations of sequential processing (like RNNs).\\r\\n- Faster to train due to parallelization capabilities (self-attention is parallelizable).\\r\\n- Highly effective for tasks involving large datasets like NLP, translation, and summarization.\\rDisadvantages\\r- Computationally expensive, especially for long sequences due to the quadratic complexity of self-attention.\\r\\n- Requires large amounts of data to perform well (pre-training helps mitigate this).\\r\\n- Sensitive to hyperparameters like the number of layers and attention heads.\\rVariants\\r- BERT (Bidirectional Encoder Representations from Transformers): Focuses on pre-training using a masked language model approach for NLP tasks.\\r\\n- GPT (Generative Pre-trained Transformer): A generative model for text generation.\\r\\n- T5 (Text-to-Text Transfer Transformer): Converts all NLP tasks into a text-to-text framework.\\r\\n- Vision Transformer (ViT): Uses transformers for image recognition tasks.\\rUse Cases\\r- Machine Translation: (e.g., translating languages like English to French).\\r\\n- Text Summarization: Generating concise summaries of long documents.\\r\\n- Text Generation: (e.g., GPT models used for generating coherent paragraphs or stories).\\r\\n- Question Answering, Sentiment Analysis, Image Recognition (ViT).\\r\\r\\n\\r\\nA comparative table that outlines the architectures Of the Deep Learning AI Models - Programming Ocean Academy\\rArchitecture\\rStructure and Layers\\rInput Type\\rOutput Type\\rKey Features\\rCommon Applications\\rFeedforward Neural Networks (FNN)\\r\\n \\r- Composed of input, hidden, and output layers.\\r\\n- Each layer fully connected to the next.\\rFixed-size vectors\\r\\n \\rFixed-size vectors\\r\\n \\r- Simple structure, no feedback loops.\\r\\n- Activation functions used for non-linearity.\\rClassification, regression, basic pattern recognition.\\r\\n \\rConvolutional Neural Networks (CNN)\\r\\n \\r- Composed of convolutional layers, pooling layers, and fully connected layers.\\r\\n- Convolutional layers use filters (kernels).\\rImages (2D data)\\r\\n \\rClass labels, feature maps\\r\\n \\r- Local receptive fields.\\r\\n- Parameter sharing (weights are shared across space).\\rImage recognition, object detection, image segmentation.\\r\\n \\rRecurrent Neural Networks (RNN)\\r\\n \\r- Composed of recurrent layers where connections between nodes form directed cycles.\\r\\n- Includes LSTM and GRU variants for handling long-term dependencies.\\rSequential data (time series, text)\\r\\n \\rSequences or time-series predictions\\r\\n \\r- Maintains memory of previous inputs through hidden states.\\r\\n- Suitable for variable-length input.\\rNatural language processing, time series forecasting, speech recognition.\\r\\n \\rGenerative Adversarial Networks (GAN)\\r\\n \\r- Composed of two neural networks: generator and discriminator.\\r\\n- These networks compete against each other.\\rRandom noise (for generator)\\r\\n \\rGenerated data (e.g., images)\\r\\n \\r- Adversarial training process.\\r\\n- Generator creates data, discriminator evaluates it.\\rImage generation, data augmentation, unsupervised learning.\\r\\n \\rTransformer Networks (TNN)\\r\\n \\r- Composed of encoder and decoder layers with multi-head self-attention mechanisms.\\r\\n- Uses positional encoding to maintain sequence information.\\rSequences (text, time series)\\r\\n \\rSequences (text generation, translation)\\r\\n \\r- Parallel processing of input data.\\r\\n- Attention mechanism captures contextual relationships.\\rNatural language processing, translation, text summarization.\\r\\n \\r\\r\\nA summarizing the algorithms and techniques used in ChatGPT Part One- Programming Ocean Academy\\rCategory\\rAlgorithm/Technique\\rDescription\\rTransformers\\rSelf-Attention Mechanism\\rWeighs the importance of different words in a sentence for context understanding.\\r \\rMulti-Head Attention\\rAllows the model to focus on various parts of the input simultaneously.\\rOptimization Algorithms\\rAdam Optimizer\\rAdaptive learning rate optimization for weight updates during training.\\r \\rStochastic Gradient Descent (SGD)\\rUpdates weights incrementally based on mini-batches of data.\\rLoss Functions\\rCross-Entropy Loss\\rMeasures the difference between predicted probabilities and actual class labels.\\r \\rLabel Smoothing\\rSoftens target labels to improve generalization and prevent overconfidence.\\rTraining Techniques\\rTransfer Learning\\rPre-training on a large corpus followed by fine-tuning on a smaller dataset.\\r \\rReinforcement Learning from Human Feedback (RLHF)\\rUses human feedback to improve response quality and relevance.\\rRegularization Techniques\\rDropout\\rRandomly sets a portion of neurons to zero during training to prevent overfitting.\\r \\rWeight Decay\\rAdds a penalty based on the magnitude of model weights to control complexity.\\r\\r\\n\\r\\nA summarizing the algorithms and techniques used in ChatGPT Part Two- Programming Ocean Academy\\rCategory\\rAlgorithm/Technique\\rDescription\\rAttention Mechanisms\\rScaled Dot-Product Attention\\rCalculates attention scores based on the dot product of query and key vectors.\\r \\rAdditive Attention\\rCombines query and key vectors using a feedforward network to calculate attention weights.\\rSampling Techniques\\rTop-k Sampling\\rSelects the next word from the top k most probable words for variability.\\r \\rTop-p Sampling (Nucleus Sampling)\\rChooses from the smallest set of words whose cumulative probability exceeds a threshold (p).\\rLanguage Modeling Techniques\\rAutoregressive Modeling\\rPredicts the next word based on previous words for coherent text generation.\\r \\rMasked Language Modeling\\rPredicts missing words in a sentence to enhance pre-training.\\rEmbedding Techniques\\rWord Embeddings\\rConverts words into dense vectors capturing semantic meanings (e.g., Word2Vec, GloVe).\\r \\rPositional Encoding\\rAdds information about the position of words in the sequence.\\rData Augmentation Techniques\\rText Augmentation\\rGenerates additional training examples using techniques like synonym replacement or back-translation.\\rModel Distillation Techniques\\rKnowledge Distillation\\rTransfers knowledge from a large model (teacher) to a smaller model (student).\\r\\r\\n\\r\\nTypes of Models Used in ChatGPT Part One - Programming Ocean Academy\\rModel Type\\rDefinition\\rKey Features / Applications\\rTransformer Model\\rThe foundational architecture that utilizes self-attention mechanisms to process sequences of data.\\rParallel processing, attention mechanisms, and positional encodings.\\rEncoder-Decoder Model\\rA Transformer model with separate encoder and decoder components, originally designed for tasks like translation.\\rAlthough ChatGPT primarily uses the decoder-only version, the encoder-decoder architecture is crucial in understanding the evolution of Transformers.\\rDecoder-Only Model\\rA simplified Transformer model that uses only the decoder component, which is designed for generating text autoregressively.\\rChatGPT operates primarily as a decoder-only model for text generation.\\rFine-Tuned Language Models\\rPre-trained Transformer models that have been further trained on specific datasets for improved performance on targeted tasks.\\rChatGPT is fine-tuned on conversational datasets to enhance its ability to engage in dialogue.\\rLarge Language Models (LLMs)\\rHigh-capacity models that are capable of understanding and generating human-like text based on vast amounts of training data.\\rChatGPT belongs to the class of LLMs, leveraging large datasets to improve fluency and coherence.\\r\\r\\n\\r\\nTypes of Models Used in ChatGPT Part Two - Programming Ocean Academy\\rModel Type\\rDefinition\\rKey Features / Applications\\rPrompt Engineering\\rTechniques used to design and optimize input prompts to elicit better responses from language models.\\rEnhances the effectiveness of ChatGPT by guiding its responses through carefully constructed prompts.\\rReinforcement Learning from Human Feedback (RLHF)\\rA training technique where models are improved based on feedback from human users, often involving ranking or scoring outputs.\\rUsed in ChatGPT to refine its responses and align them with user expectations.\\rZero-Shot, Few-Shot Learning\\rApproaches that allow the model to perform tasks with little to no additional training data (zero-shot) or with a small amount of examples (few-shot).\\rChatGPT can generalize across tasks without explicit training for each one.\\rDistillation Techniques\\rMethods used to create smaller, more efficient versions of large models while retaining performance.\\rMay be applied in future iterations to optimize ChatGPT for deployment on various platforms.\\rMultimodal Models (Future Development)\\rModels that can process and generate multiple types of data (e.g., text, images, audio).\\rFuture versions of ChatGPT may integrate multimodal capabilities.\\r\\r\\nCommon Hyperparameters in Both Machine Learning and Deep Learning - Programming Ocean Academy\\rHyperparameter\\rContext\\rLearning Rate\\rUsed in both contexts to control the step size in optimization algorithms. In deep learning, it often requires careful tuning due to complex architectures.\\rBatch Size\\rRelevant in both contexts; in deep learning, batch size can significantly impact training time and model performance.\\rEpochs\\rApplies to both; refers to the number of times the entire dataset is passed through the model during training.\\rRegularization Strength\\rUsed in both contexts to prevent overfitting through L1 (Lasso) or L2 (Ridge) regularization techniques.\\rOptimizer\\rRelevant in both; different optimization algorithms can be used in both machine learning models and neural networks.\\rMomentum\\rPrimarily used in deep learning to accelerate the optimization process, particularly in stochastic gradient descent (SGD).\\r\\r\\n\\r\\nHyperparameters for Specific Algorithms Part One - Programming Ocean Academy\\rAlgorithm\\rHyperparameter\\rDescription\\rLinear Regression\\r\\n \\rRegularization Type\\r\\nAlpha\\rType of regularization applied (L1 or L2).\\r\\nRegularization strength.\\rDecision Trees\\r\\n \\r\\n \\r\\n \\rMax Depth\\r\\nMin Samples Split\\r\\nMin Samples Leaf\\r\\nMax Features\\rMaximum depth of the tree.\\r\\nMinimum number of samples required to split an internal node.\\r\\nMinimum number of samples required to be at a leaf node.\\r\\nNumber of features to consider when looking for the best split.\\rRandom Forest\\r\\n \\r\\n \\rNumber of Estimators\\r\\nBootstrap\\r\\nMax Features\\rNumber of trees in the forest.\\r\\nWhether to use bootstrap samples when building trees.\\r\\nNumber of features to consider at each split.\\rSupport Vector Machines (SVM)\\r\\n \\r\\n \\rC\\r\\nKernel Type\\r\\nGamma\\rRegularization parameter.\\r\\nType of kernel used (linear, polynomial, RBF, etc.).\\r\\nKernel coefficient for RBF, poly, and sigmoid kernels.\\rK-Nearest Neighbors (KNN)\\r\\n \\rNumber of Neighbors (k)\\r\\nDistance Metric\\rNumber of neighbors to consider for classification or regression.\\r\\nMetric used to measure distance (Euclidean, Manhattan, etc.).\\rNeural Networks\\r\\n \\r\\n \\r\\n \\r\\n \\rNumber of Layers\\r\\nNumber of Neurons\\r\\nActivation Functions\\r\\nDropout Rate\\r\\nWeight Initialization\\rTotal number of hidden layers.\\r\\nNumber of neurons in each layer.\\r\\nFunctions used to introduce non-linearity (ReLU, Sigmoid, Tanh, etc.).\\r\\nFraction of neurons to drop during training to prevent overfitting.\\r\\nMethod for initializing weights (Xavier, He, etc.).\\r\\r\\nHyperparameters for Specific Algorithms Part Two - Programming Ocean Academy\\rAlgorithm\\rHyperparameter\\rDescription\\rGradient Boosting Machines (GBM)\\r\\n \\r\\n \\r\\n \\rNumber of Estimators\\r\\nLearning Rate\\r\\nMax Depth\\r\\nSubsample\\rNumber of boosting stages.\\r\\nStep size shrinkage used to prevent overfitting.\\r\\nMaximum depth of individual trees.\\r\\nFraction of samples used for fitting individual base learners.\\rXGBoost\\r\\n \\r\\n \\r\\n \\rLearning Rate\\r\\nMax Depth\\r\\nSubsample\\r\\nColsample_bytree\\rControls the contribution of each tree.\\r\\nMaximum depth of a tree.\\r\\nProportion of samples used for training each tree.\\r\\nProportion of features used for training each tree.\\rLightGBM\\r\\n \\r\\n \\r\\n \\rLearning Rate\\r\\nNum Leaves\\r\\nBagging Fraction\\r\\nFeature Fraction\\rSimilar to XGBoost.\\r\\nMaximum number of leaves in one tree.\\r\\nProportion of data to use for training.\\r\\nProportion of features to use for training.\\rClustering\\r\\n \\r\\n \\rNumber of Clusters (k)\\r\\nDistance Metric\\r\\nInitialization Method\\rIn k-means clustering.\\r\\nType of distance measure used.\\r\\nMethod to initialize cluster centroids (e.g., k-means++).\\rNatural Language Processing\\r\\n \\r\\n \\r\\n \\rEmbedding Dimension\\r\\nSequence Length\\r\\nDropout Rate\\r\\nAttention Heads\\rDimensionality of word embeddings.\\r\\nLength of input sequences in RNNs.\\r\\nFor regularization in NLP models.\\r\\nIn transformer models, number of attention heads.\\rOther Hyperparameters\\r\\n \\rEarly Stopping\\r\\nGrid Search or Random Search\\rCondition to stop training when validation performance stops improving.\\r\\nMethods for hyperparameter optimization.\\r\\r\\n\\r\\nA comparison between binary classification and multi-class classification in terms of hyperparameters for AI model architecture-Programming Ocean Academy\\rAspect\\rBinary Classification\\rMulti-Class Classification\\rDefinition\\rInvolves classifying data into two distinct categories (e.g., spam vs. not spam).\\rInvolves classifying data into three or more categories (e.g., types of flowers).\\rOutput Layer\\rTypically consists of a single output neuron with a sigmoid activation function.\\rConsists of multiple output neurons (one for each class) with a softmax activation function.\\rLoss Function\\rCommonly uses Binary Cross-Entropy Loss to measure the model\\'s performance.\\rTypically uses Categorical Cross-Entropy Loss for multi-class tasks or Sparse Categorical Cross-Entropy when labels are one-hot encoded.\\rActivation Function\\rUses Sigmoid function in the output layer to produce a probability between 0 and 1.\\rUses Softmax function in the output layer to produce probabilities for each class that sum to 1.\\rEvaluation Metrics\\rCommonly uses accuracy, precision, recall, F1-score, and ROC-AUC for evaluation.\\rUses accuracy, precision, recall, F1-score (macro/micro averages), and confusion matrix for evaluation.\\rData Preparation\\rTypically requires binary labels (0 or 1) in the dataset for each instance.\\rRequires categorical labels (e.g., 0, 1, 2) for each instance in the dataset, often converted to one-hot encoding.\\rTraining Approach\\rOften utilizes simpler models and may converge faster due to fewer output categories.\\rCan be more complex and may require more training data to effectively model multiple classes.\\rRegularization Techniques\\rCommon techniques include L1/L2 regularization and dropout to prevent overfitting.\\rSimilar regularization techniques are applied, but may also include class balancing methods (e.g., class weights) to address imbalances among classes.\\rHyperparameter Tuning\\rTypically involves tuning parameters such as learning rate, batch size, number of epochs, and dropout rates.\\rIn addition to the parameters in binary classification, may include tuning the number of neurons per output layer, learning rate scheduling, and strategies for dealing with class imbalance (e.g., oversampling, undersampling).\\rCommon Algorithms\\rLogistic Regression, Support Vector Machines (SVM), Decision Trees, and Neural Networks.\\rSoftmax Regression, Decision Trees, Random Forests, Gradient Boosting Machines, and Neural Networks.\\r\\r\\n\\r\\nThe mathematical derivatives techniques used in machine learning and deep learning - Programming Ocean Academy\\rDerivative Technique\\rDefinition/Description\\rUse in Machine Learning / Deep Learning\\rGradient Descent\\rAn optimization algorithm that uses the gradient of the loss function to update parameters.\\rCommonly used for minimizing loss functions in various models, including linear regression and neural networks.\\rBackpropagation\\rA method for calculating the gradient of the loss function with respect to each weight in the neural network.\\rEssential for training neural networks by efficiently computing gradients for weight updates.\\rPartial Derivatives\\rDerivatives of multivariable functions with respect to one variable while holding others constant.\\rUsed in optimization problems to understand how changing one parameter affects the output.\\rJacobian Matrix\\rA matrix of all first-order partial derivatives of a vector-valued function.\\rHelps in understanding the sensitivity of outputs to changes in inputs, especially in deep learning models.\\rHessian Matrix\\rA square matrix of second-order partial derivatives of a scalar-valued function.\\rUsed to analyze the curvature of the loss function, aiding in optimization algorithms like Newton\\'s method.\\rChain Rule\\rA formula for computing the derivative of the composition of functions.\\rFundamental in backpropagation to compute gradients of nested functions in neural networks.\\rStochastic Gradient Descent (SGD)\\rA variant of gradient descent that uses a random subset of data to compute the gradient.\\rEfficiently updates model parameters in large datasets, commonly used in deep learning.\\rAdaptive Learning Rate Methods\\rTechniques like Adam, RMSprop that adjust the learning rate based on gradient statistics.\\rEnhances convergence speed and stability in training neural networks.\\rTaylor Series Expansion\\rAn approximation of a function as an infinite sum of terms calculated from the function\\'s derivatives at a single point.\\rHelps in optimizing functions by approximating them locally, useful in methods like Newton\\'s method.\\rSecond Derivative Test\\rA method to determine the concavity of functions and identify local minima or maxima.\\rHelps in understanding the optimization landscape and in implementing more sophisticated optimization algorithms.\\r\\r\\n\\r\\n\\r\\nThe different applications of computer vision across various industries- Programming Ocean Academy\\rApplication\\rDescription\\rContent Organization\\rIdentifies people or objects in photos to organize them, commonly used in photo storage and social media applications.\\rText Extraction\\rUses optical character recognition (OCR) to extract text from images for content discoverability and document processing in robotic process automation.\\rAugmented Reality\\rDetects and tracks physical objects in real-time to place virtual objects realistically in a physical environment.\\rAgriculture\\rAnalyzes images from satellites, drones, or planes to monitor harvests, detect weeds, and identify crop nutrient deficiencies.\\rAutonomous Vehicles\\rUses real-time object identification and tracking to gather information around the car, enabling self-driving cars to navigate and make decisions.\\rHealthcare\\rAnalyzes medical images (e.g., X-rays, MRIs) to assist doctors in identifying problems and making faster, more accurate diagnoses.\\rSports\\rTracks objects, players, and plays to assist with play analysis and strategy development in sports events.\\rManufacturing\\rMonitors machinery for maintenance, product quality, and packaging inspection on production lines using computer vision.\\rSpatial Analysis\\rIdentifies and tracks the movement of people or objects (e.g., cars) within a space for spatial analytics, useful in security or traffic management.\\rFace Recognition\\rIdentifies individuals based on their facial features, commonly used for security, authentication, and personalization applications.\\r\\r\\n\\r\\nAspect\\rGrayscale Images\\rColored Images\\rColor Channels\\r1 channel (intensity values)\\r3 channels (Red, Green, Blue - RGB)\\rPixel Values\\rEach pixel has a single value representing intensity (0-255).\\rEach pixel has three values representing RGB color intensities (0-255).\\rFile Size\\rSmaller file size since only one channel is stored.\\rLarger file size due to the need to store three channels.\\rComplexity\\rSimpler representation, easier for algorithms to process.\\rMore complex data due to the three channels, requiring more computation.\\rApplications\\rSuitable for tasks where color information is not critical (e.g., edge detection, document scanning).\\rEssential for tasks where color is important (e.g., object recognition, art analysis).\\rMemory Usage\\rLower memory usage due to single channel data.\\rHigher memory usage due to multiple channels (3x more than grayscale).\\rVisual Information\\rLimited to shades of gray, no color information.\\rRich in visual information, including a wide range of colors.\\rProcessing Time\\rFaster processing since there\\'s only one channel to analyze.\\rSlower processing due to multiple channels and increased complexity.\\rRepresentation\\rEach pixel represents only brightness or intensity.\\rEach pixel represents color through a combination of RGB intensities.\\rTraining Neural Networks\\rOften used in simpler models or when color is irrelevant.\\rUsed in more complex models where color plays a significant role in classification.\\r\\r\\n\\r\\nColor Channel Type\\rDescription\\rChannels\\rCommon Use Cases\\rAdvantages\\rDisadvantages\\rRGB (Red, Green, Blue)\\rRepresents images using three channels: Red, Green, and Blue. Colors are formed by combining different intensities of these channels.\\r3 (Red, Green, Blue)\\rMost common in digital displays, cameras, and web images.\\rWidely supported, intuitive representation for digital images.\\rLimited in representing real-world colors precisely.\\rRGBA (Red, Green, Blue, Alpha)\\rAdds an alpha channel to RGB to represent transparency. Each pixel has an opacity value.\\r4 (Red, Green, Blue, Alpha)\\rUsed in images where transparency is important (e.g., icons).\\rProvides support for transparency effects.\\rLarger file size due to the additional alpha channel.\\rCMYK (Cyan, Magenta, Yellow, Black)\\rA subtractive color model used in printing. Colors are formed by subtracting colors from white (paper background).\\r4 (Cyan, Magenta, Yellow, Black)\\rPrimarily used in color printing (magazines, posters, etc.).\\rAccurately represents printed colors.\\rNot used for displays, limited support in digital systems.\\rGrayscale\\rRepresents images with shades of gray, from black to white. It only measures light intensity.\\r1 (Gray)\\rUsed in black-and-white photography, document scanning.\\rSimple and efficient for tasks where color is irrelevant.\\rNo color information, limited to shades of gray.\\rYCbCr\\rA color space used in video compression. Y represents luminance, while Cb and Cr represent chrominance (color information).\\r3 (Y, Cb, Cr)\\rCommonly used in video encoding and broadcasting.\\rSeparates brightness and color data, efficient for video compression.\\rNot as intuitive as RGB for direct color manipulation.\\rLAB (CIELAB)\\rA color space that separates lightness (L) from chromaticity (A and B channels).\\r3 (L, A, B)\\rUsed in image editing, color correction, and perception studies.\\rMore perceptually uniform, better for color adjustments.\\rComputationally more complex, less intuitive than RGB.\\rHSB/HSV (Hue, Saturation, Brightness/Value)\\rA cylindrical representation of color, focusing on hue, saturation, and brightness.\\r3 (Hue, Saturation, Brightness/Value)\\rUsed in graphic design, color adjustments, and image editing.\\rMakes it easier to adjust hue and saturation independently of brightness.\\rNot ideal for image storage and transmission.\\rHSL (Hue, Saturation, Lightness)\\rSimilar to HSV but represents color with hue, saturation, and lightness.\\r3 (Hue, Saturation, Lightness)\\rUsed in image editing software, color pickers.\\rIntuitive for adjusting lightness separately from hue and saturation.\\rCan produce non-intuitive results for certain colors.\\r\\r\\n\\r\\n\\r\\n\\r\\nA table summarizing the different CNN architectures - Programming Ocean Academy\\rCNN Architecture\\rDescription\\rKey Features\\rCommon Use Cases\\rLeNet\\rOne of the earliest CNN architectures, developed by Yann LeCun, primarily for digit recognition.\\rSimple architecture with 5 layers, used for handwritten digit classification (MNIST dataset).\\rHandwritten digit recognition, image classification.\\rAlexNet\\rA breakthrough CNN architecture that won the 2012 ImageNet competition, brought CNNs into mainstream attention.\\rDeeper and wider than LeNet, uses ReLU activation, dropout, and data augmentation to reduce overfitting.\\rLarge-scale image classification (ImageNet).\\rVGG\\rKnown for its simplicity and depth, with popular configurations like VGG16 and VGG19.\\rUses small (3x3) convolution filters and a deep architecture (16-19 layers).\\rImage classification, object detection.\\rResNet\\rIntroduces residual connections (skip connections) to solve the vanishing gradient problem in deep networks.\\rEnables training of very deep networks (50+ layers), residual blocks make backpropagation more efficient.\\rImage classification, object detection, segmentation.\\rYOLO (You Only Look Once)\\rA state-of-the-art real-time object detection model that processes the entire image in one pass.\\rFast object detection, treats detection as a regression problem instead of a classification problem.\\rReal-time object detection, autonomous vehicles, video surveillance.\\r\\r\\n\\r\\nThe OpenAI Microscope for different vision models - Programming Ocean Academy\\rModel\\rDescription\\rNodes\\rAlexNet\\rLandmark model that won the 2012 ImageNet competition, widely cited in computer vision.\\r26\\rAlexNet (Places)\\rSame architecture as AlexNet, but trained on the Places365 dataset.\\r26\\rInception v1 (GoogLeNet)\\rSet the state of the art in ImageNet classification in 2014.\\r84\\rInception v1 (Places)\\rSame architecture as Inception v1, but trained on the Places365 dataset.\\r84\\rVGG 19\\rSimpler network introduced in 2014, uses only 3x3 convolutions and no branches.\\r27\\rInception v3\\rReleased in 2015, an improved and more efficient version of the Inception architecture.\\r127\\rInception v4\\rFourth iteration of the Inception architecture, released in 2016 with a focus on uniformity.\\r197\\rResNet v2 50\\rResNet variant with 50 layers, uses skip connections to help gradients propagate in deep networks.\\r58\\rCLIP ResNet 50 v0\\rOlder checkpoint of a ResNet 50 model trained with contrastive loss.\\r67\\rCLIP ResNet 50\\rStandard CLIP ResNet-50 architecture.\\r73\\rCLIP ResNet 101\\rCLIP model based on ResNet-101 architecture.\\r141\\rCLIP ResNet 50 4x\\rScaled-up version of CLIP ResNet-50, 4 times larger.\\r113\\rCLIP ResNet 50 16x\\rScaled-up version of CLIP ResNet-50, 16 times larger.\\r169\\r\\r\\nThe CNN architectures for Different Models-Part One- Programming Ocean Academy\\rCNN Architecture\\rDescription\\rKey Features\\rCommon Use Cases\\rAlexNet\\rLandmark model that won the 2012 ImageNet competition.\\rDeep architecture with 5 convolutional layers and ReLU activation.\\rImage classification, object detection.\\rAlexNet (Places)\\rAlexNet architecture trained on the Places365 dataset for scene recognition.\\rSame structure as AlexNet but optimized for different datasets.\\rScene classification, spatial context analysis.\\rInception v1 (GoogLeNet)\\rSet the state of the art in ImageNet classification in 2014 with a novel architecture.\\rUtilizes inception modules to efficiently handle multiple filter sizes.\\rImage classification, object detection, feature extraction.\\rInception v1 (Places)\\rInception v1 model trained on the Places365 dataset for scene analysis.\\rSame architecture as Inception v1, tailored for scene recognition.\\rScene classification, spatial analysis.\\rVGG 19\\rA simpler network introduced in 2014, known for its uniform architecture.\\rConsists of 16 convolutional layers with 3x3 filters and max pooling.\\rImage classification, transfer learning.\\rInception v3\\rAn improved version of Inception architecture, released in 2015 for enhanced performance.\\rIncreased depth and efficiency with more inception modules.\\rImage classification, fine-grained object detection.\\rInception v4\\rFourth iteration of the Inception architecture, focusing on uniformity and performance.\\rEven deeper with advanced inception modules for better feature extraction.\\rImage classification, advanced object detection.\\r\\r\\n\\r\\nThe CNN architectures for Different Models-Part Two- Programming Ocean Academy\\rCNN Architecture\\rDescription\\rKey Features\\rCommon Use Cases\\rResNet v2 50\\rResNet variant that uses skip connections to enable deeper networks without vanishing gradients.\\r50 layers with residual connections for gradient flow.\\rImage classification, object detection, and segmentation.\\rCLIP ResNet 50 v0\\rEarly checkpoint of ResNet 50 trained with contrastive loss for improved performance in understanding images and text.\\rIncorporates contrastive learning to enhance multimodal understanding.\\rMultimodal tasks, image-text associations.\\rCLIP ResNet 50\\rStandard CLIP model based on ResNet-50 architecture for image and text understanding.\\rSimilar to the ResNet 50 architecture with contrastive loss training.\\rImage and text classification, multimodal tasks.\\rCLIP ResNet 101\\rA deeper version of CLIP model based on ResNet-101 architecture for better performance.\\rMore layers enhance feature extraction capabilities.\\rAdvanced image-text associations, multimodal applications.\\rCLIP ResNet 50 4x\\rScaled-up version of CLIP ResNet-50, designed for improved performance in larger datasets.\\rFour times the original model size, leading to more capacity for learning.\\rLarge-scale image classification, complex tasks.\\rCLIP ResNet 50 16x\\rFurther scaled-up version of CLIP ResNet-50, maximizing model capacity and learning potential.\\rSixteen times the original model size, ideal for extensive datasets.\\rVery large-scale image and text tasks, advanced applications.\\r\\r\\n\\r\\nHow to calculate the number of parameters in the convolutional layer- Programming Ocean Academy\\rComponent\\rCalculation\\rValue\\rExplanation\\rFilter size (kernel)\\r3 x 3\\r9\\rEach filter is a 3x3 matrix, so 9 values per channel\\rInput channels\\r9 (weights per channel) x 3 channels\\r27\\rThe input has 3 color channels (RGB)\\rBias term\\r1 per filter\\r1\\rEach filter has one bias value\\rParameters per filter\\r27 (weights) + 1 (bias)\\r28\\rTotal number of parameters for one filter\\rNumber of filters\\r28 parameters/filter x 32 filters\\r896\\r32 filters, each with 28 parameters\\r\\r\\n\\r\\nLayer Summary - Programming  Ocean Academy\\rLayer\\rPurpose\\rConv2D (32 filters)\\rExtracts low-level features from the input image\\rMaxPooling2D\\rReduces spatial dimensions, retaining important features\\rConv2D (64 filters)\\rExtracts more complex features\\rMaxPooling2D\\rFurther reduces the size of feature maps\\rConv2D (64 filters)\\rExtracts deeper, more abstract features\\rFlatten\\rFlattens the output to a 1D vector for fully connected layers\\rDense (64 neurons, ReLU)\\rCombines features and learns the classification patterns\\rDense (10 neurons)\\rOutput layer that predicts the class (10 possible classes)\\r\\r\\nComparison between different types of Cross-Entropy Loss used in neural networks - Programming Ocean Academy\\rType\\rUse Case\\rInput Format\\rFormula (Loss Calculation)\\rKey Features\\rBinary Cross-Entropy\\r\\n \\r\\n \\rBinary classification (two classes, e.g., cat vs dog)\\r\\n \\r\\n \\rInput: one probability value per sample (output between 0 and 1), corresponding to class 0 or 1\\r\\n \\r\\n \\r- [y*log?(p)+(1-y)*log?(1-p)]\\r- Used for binary classification.\\r\\n- Target labels are 0 or 1.\\r\\n- Output activation function: Sigmoid.\\rCategorical Cross-Entropy\\r\\n \\rMulti-class classification (when classes are mutually exclusive)\\r\\n \\rInput: one-hot encoded vectors for target labels (e.g., [1, 0, 0] for class 0)\\r\\n \\r - ?_(i=1)^NÂ¦?y_I*log??(p_i ? ?)  \\r\\nwhere N is the number of classes\\r- Used for multi-class classification where each instance belongs to one class.\\r\\n- Softmax output layer.\\rSparse Categorical Cross-Entropy\\r\\n \\rMulti-class classification with sparse integer labels\\r\\n \\rInput: integer label (e.g., 0 for class 0) without one-hot encoding\\r\\n \\rSame as categorical cross-entropy, but integer labels are converted internally to one-hot vectors\\r\\n \\r- Works like categorical cross-entropy but without requiring one-hot encoding.\\r\\n- Convenient for large number of classes.\\r\\r\\n\\r\\n\\r\\nparameter of the tf.keras.layers.Conv2D function Part One- Programming Ocean Academy\\rParameter\\rType\\rDefault Value\\rDescription\\rfilters\\rint\\rNone\\rThe number of filters (or kernels) to use in the convolution operation. Each filter learns to detect different features in the input data.\\rkernel_size\\rtuple or int\\r(3, 3)\\rSpecifies the height and width of the 2D convolution window. Can be a single integer (for square kernels) or a tuple of two integers (for rectangular kernels).\\rstrides\\rtuple or int\\r(1, 1)\\rThe strides of the convolution along the height and width. This determines how much the filter moves across the input image. Default is 1 in both dimensions.\\rpadding\\rstr\\r\\'valid\\'\\rSpecifies the padding method: \\'valid\\' (no padding, leading to reduced output dimensions) or \\'same\\' (padding is added to keep output dimensions the same).\\rdata_format\\rstr\\rNone\\rThe format of the input data: \\'channels_last\\' (default, with shape (batch, height, width, channels)) or \\'channels_first\\' (with shape (batch, channels, height, width)).\\rdilation_rate\\rtuple\\r(1, 1)\\rSpecifies the dilation rate for dilated convolution. Dilation expands the kernel size effectively, allowing for a larger receptive field without increasing parameters.\\rgroups\\rint\\r1\\rControls the grouping of the input and output channels. Setting this value greater than 1 creates a grouped convolution, which can be useful for specific architectures like MobileNets.\\ractivation\\rcallable\\rNone\\rA function to apply as an activation function after the convolution. Common options include tf.keras.activations.relu, tf.keras.activations.sigmoid, etc.\\r- \\r\\nparameter of the tf.keras.layers.Conv2D function Part Two - Programming Ocean Academy\\rParameter\\rType\\rDefault Value\\rDescription\\ruse_bias\\rbool\\rTRUE\\rIndicates whether to include a bias term in the convolution operation. If set to False, the bias will not be added.\\rkernel_initializer\\rstr or callable\\r\\'glorot_uniform\\'\\rThe initializer for the kernel weights matrix. Common options include \\'he_normal\\', \\'glorot_uniform\\', etc., which determine how the initial weights are set.\\rbias_initializer\\rstr or callable\\r\\'zeros\\'\\rThe initializer for the bias vector. Default is to initialize biases to zero.\\rkernel_regularizer\\rcallable\\rNone\\rA function used to apply regularization to the kernel weights. Useful for preventing overfitting by adding penalties for large weights.\\rbias_regularizer\\rcallable\\rNone\\rA function used to apply regularization to the bias vector. Similar to kernel regularization, but for biases.\\ractivity_regularizer\\rcallable\\rNone\\rA function used to apply regularization to the output of the layer. This can be used to control the output value\\'s size and prevent overfitting.\\rkernel_constraint\\rcallable\\rNone\\rA function to constrain the kernel weights during optimization. This can be used to enforce weight constraints (e.g., non-negativity).\\rbias_constraint\\rcallable\\rNone\\rA function to constrain the bias during optimization. Similar to kernel constraints, this enforces constraints on the bias weights.\\rkwargs\\rdict\\rNone\\rAdditional keyword arguments passed to the layer. This can include parameters for other functionalities that might be specific to the user\\'s needs.\\r\\r\\n\\r\\nDefinition of Computer Vision and different uses of Computer Vision - Programming Ocean Academy\\rAspect\\rDefinition\\rUses of Computer Vision\\rDefinition\\rComputer Vision (CV) is a field of artificial intelligence (AI) that enables computers to interpret and process visual data (images, videos) similarly to human vision.\\r-\\rImage Classification\\rCV is used to classify images into categories based on their content.\\rExample: Sorting images into categories like cats, dogs, cars, etc. (e.g., Cats vs. Dogs project).\\rObject Detection\\rCV can detect objects within an image or video, often using bounding boxes.\\rExample: Detecting pedestrians, vehicles in autonomous driving, facial recognition, and security cameras.\\rImage Segmentation\\rCV divides an image into segments, identifying objects or regions of interest.\\rExample: Medical imaging to highlight tumors, or background removal in photography.\\rImage Recognition\\rIdentifying specific objects or people in an image.\\rExample: Face recognition systems (security, tagging in social media).\\rVideo Analysis\\rCV processes video data to recognize activities, movements, or changes over time.\\rExample: Surveillance, traffic analysis, or video summarization.\\rOptical Character Recognition (OCR)\\rExtracting and recognizing text from images or documents.\\rExample: Digitizing printed or handwritten text, such as scanning books or recognizing license plates.\\rPose Estimation\\rDetecting human body posture or movement from images or video.\\rExample: Fitness apps to analyze posture, motion capture for movies/games, AR applications.\\r3D Scene Reconstruction\\rReconstructing a 3D model or scene from multiple 2D images.\\rExample: Augmented reality, 3D mapping, or virtual reality environments.\\rAutonomous Vehicles\\rCV helps vehicles understand their surroundings in real time by processing visual data.\\rExample: Self-driving cars detecting traffic signs, pedestrians, lane boundaries, and other vehicles.\\rMedical Imaging\\rEnhancing and analyzing medical images for diagnostic purposes.\\rExample: MRI and CT scan analysis to detect diseases like cancer or fractures.\\rRetail and E-commerce\\rAnalyzing customer behavior, product images, and augmented reality fitting rooms.\\rExample: Virtual try-ons, analyzing product preferences, and in-store activity monitoring (Amazon Go).\\rAgriculture\\rCV helps monitor plant growth, detect pests, and assess crop health.\\rExample: Using drones to monitor large-scale farms, detecting diseases in crops from images.\\rManufacturing\\rAutomated inspection of products for quality control using CV systems.\\rExample: Detecting defects or errors in production lines, robotic assembly.\\rAugmented Reality (AR)\\rIntegrating digital objects into real-world environments using CV.\\rExample: AR filters on social media (e.g., Snapchat), real-time navigation apps, or interactive gaming.\\rRobotics\\rCV allows robots to perceive their environment and perform tasks autonomously.\\rExample: Robots performing warehouse sorting, cleaning, or object manipulation based on visual data.\\rContent Moderation\\rAutomatically moderating visual content on platforms.\\rExample: Detecting inappropriate images or videos for content moderation on social media platforms.\\r\\r\\n\\r\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Nws5Jvv-xmOs",
        "outputId": "71ca3f6a-3424-46fd-80dd-e3654424cddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\r\\n\\r\\n\\r\\nComparisons in Data Science and AI Bootcamp - Programming Ocean Academy-2024\\r\\nProgramming Ocean Academy - Data Storge and Processing\\rAspect\\rWarehouse\\rLake\\rPipeline\\rDatabase\\rData Mart\\rPurpose\\rStores structured data for analysis\\rStores both structured and unstructured data\\rTransfers data from one system to another\\rStores structured data efficiently\\rSubset of a data warehouse tailored for specific business line or department\\rData Type\\rPrimarily structured\\rBoth structured and unstructured\\rTypically structured\\rStructured data\\rStructured data\\rScale\\rCan handle large volumes of data\\rDesigned for massive data storage\\rDesigned for data movement\\rScales according to capacity\\rSmaller subset of a data warehouse\\rProcessing\\rMay involve data transformation\\rCan store data without prior schema definition\\rDirects data flow without altering content\\rMay involve complex querying\\rFocuses on specific business needs\\rAccess Pattern\\rRead-intensive for analytics\\rRead and write-intensive\\rTypically write-intensive, with reads possible\\rMix of read and write operations\\rTailored for specific users or departments\\rLatency\\rGenerally low latency for retrieval\\rLow latency for accessing data\\rLow latency for data transfer\\rGenerally low latency operations\\rOptimized for faster access within a specific domain\\rMaintenance\\rRequires periodic maintenance\\rMay require ongoing optimization\\rMaintenance depends on pipeline complexity\\rRequires regular maintenance\\rRequires maintenance to align with evolving business needs\\rExample\\rA retail company\\'s data warehouse stores transaction records, customer information, and inventory data. It collects and organizes structured data from various sources like point-of-sale systems, online sales platforms, and inventory management software. Analysts use this data warehouse to analyze sales trends, customer behavior, and inventory management strategies.\\rA healthcare organization\\'s data lake stores a vast array of data types, including electronic health records (structured), medical images (unstructured), patient-generated data from wearables (semi-structured), and research publications (unstructured). Researchers and data scientists can access this data lake to perform advanced analytics, such as predictive modeling for disease diagnosis or population health management.\\rAn e-commerce company implements a data pipeline to transfer real-time sales data from its online store to a centralized data warehouse. The pipeline collects transactional data, performs necessary transformations (e.g., cleansing, enrichment), and loads the data into the warehouse for analysis. This pipeline ensures that the data is continuously flowing and available for timely decision-making.\\rA financial institution uses a relational database management system (RDBMS) to store customer banking information, including account balances, transaction history, and loan details. The database supports complex querying for activities such as fraud detection, risk assessment, and customer relationship management. It ensures data integrity, consistency, and security for critical banking operations.\\rWithin a large manufacturing company, the sales department maintains a data mart specific to its operations. This sales data mart contains information on sales transactions, customer demographics, and product performance metrics relevant to the sales team\\'s objectives. By focusing on sales-related data and providing tailored analytics tools, the data mart helps sales managers track performance, forecast sales, and optimize marketing strategies for their business unit.\\r\\r\\n\\r\\nProgramming Ocean Academy -DATA SCIENE BOOTCAMP\\rCriteria\\rHadoop\\rHive\\rSpark\\rCategory\\rBig Data Framework\\rData Warehousing and Query Language\\rUnified Data Processing Framework\\rData Processing Model\\rBatch Processing\\rBatch Processing (SQL-like queries)\\rBatch and Real-Time Processing\\rData Storage\\rHadoop Distributed File System (HDFS)\\rHDFS, Other File Systems\\rIn-Memory Storage (Resilient Distributed Datasets - RDDs)\\rQuery Language\\rHQL (Hive Query Language)\\rHiveQL (SQL-like)\\rSpark SQL, Python, Scala, Java, and more\\rEase of Use\\rComplex, requires Java programming skills\\rSQL-like queries, easier for SQL users\\rProvides APIs in multiple languages for ease of use\\rData Transformation\\rLimited data transformation capabilities\\rETL capabilities for data transformation\\rExtensive data transformation capabilities\\rData Processing Speed\\rSlower due to batch processing\\rFaster than Hadoop, optimized for queries\\rIn-memory processing for high speed\\rUse Cases\\rBig data storage and batch processing\\rData warehousing, ad-hoc querying\\rBatch and real-time data processing, machine learning, graph processing, and more\\rEcosystem\\rPart of the Hadoop ecosystem\\rPart of the Hadoop ecosystem\\rPart of the Hadoop ecosystem, but can be used independently\\rResource Management\\rHadoop\\'s YARN ResourceManager\\rIntegrates with Hadoop\\'s resource management\\rBuilt-in cluster manager, standalone mode available\\rFault Tolerance\\rProvides fault tolerance through replication\\rProvides fault tolerance through replication\\rProvides fault tolerance through lineage information and data recomputation\\rReal-Time Processing\\rNot suitable for real-time processing\\rLimited real-time capabilities\\rSupports real-time processing through streaming and batch modes\\rPerformance Tuning\\rComplex tuning required for optimal performance\\rEasier performance tuning\\rProvides various optimization techniques\\r\\r\\nData Processing with three important Apache Data Science Tools- Programming Ocean Academy\\rAspect\\rApache Hadoop\\rApache Hive\\rApache Spark\\rOverview\\rA collection of tools for distributed storage and processing of big data.\\rA data warehouse built on top of Hadoop for querying and managing large datasets.\\rA distributed data analytics framework for complex data processing in real-time.\\rMain Components\\r- Hadoop Distributed File System (HDFS)\\r- Runs on HDFS or other data storage systems like Apache HBase.\\r- In-memory processing engine with support for multiple programming languages.\\rStorage System\\r- HDFS: Splits large files into blocks and distributes across multiple nodes.\\r- Stores metadata and query results.\\r- Can access data from various sources, including HDFS and Hive.\\rData Handling\\r- Handles structured, semi-structured, and unstructured data.\\r- Handles data stored in HDFS or HBase, optimized for large-scale queries.\\r- Handles interactive analytics, streaming data, machine learning, and ETL tasks.\\rProcessing\\r- Batch processing, with parallel computation on distributed data blocks.\\r- Read-based queries, suitable for ETL and data warehousing but not for high-speed transaction processing.\\r- Real-time processing, in-memory computations, spills to disk only when necessary.\\rScalability\\r- Scales linearly by adding more nodes to the cluster.\\r- Scales with Hadoop\\'s distributed storage system, but queries have high latency.\\r- Scales efficiently with its in-memory processing and can be used with standalone or Hadoop clustering.\\rFault Tolerance\\r- Replicates data blocks on multiple nodes to prevent data loss.\\r- Dependent on Hadoop\\'s fault tolerance, but queries can be slow.\\r- Fault-tolerant with in-memory data recovery.\\rPerformance\\r- Cost-effective and reliable, but can be slow due to disk-based storage and batch processing.\\r- High latency due to batch-oriented processing and large-scale data queries.\\r- High-speed computations with in-memory processing, reducing latency.\\rReal-time Capabilities\\r- Not designed for real-time processing.\\r- Not suitable for real-time processing due to high latency.\\r- Designed for real-time data processing and streaming analytics.\\rUse Cases\\r- Storing large datasets, data warehousing, handling diverse data formats.\\r- ETL, reporting, and data analysis where high latency is acceptable.\\r- Interactive analytics, streaming data processing, machine learning, and ETL tasks.\\rProgramming Languages\\r- Primarily Java-based.\\r- SQL-like query language.\\r- Supports Java, Scala, Python, R, and SQL.\\rIntegration\\r- Can be integrated with various tools and systems for big data processing.\\r- Built on Hadoop, integrates with HDFS and HBase.\\r- Can run on top of Hadoop, and integrates with HDFS, Hive, and other data sources.\\r\\r\\n\\r\\n\\rData Mining Processes - Programming Ocean Academy - Data Science Bootcamp\\rStep\\rDescription\\rEstablishing Data Mining Goals\\rSet clear objectives considering cost-benefit trade-offs, accuracy, and usefulness of results.\\rSelecting Data\\rIdentify quality data sources, considering type, availability, and cost-effectiveness.\\rPreprocessing Data\\rClean and refine raw data, removing errors, handling missing data, and ensuring integrity.\\rTransforming Data\\rModify data to reduce complexity or fit analysis, using techniques like aggregation or conversion.\\rStoring Data\\rStore transformed data securely, allowing unrestricted access for analysis and ensuring privacy.\\rMining Data\\rApply various analysis methods and algorithms, including visualization, to uncover hidden trends.\\rEvaluating Mining Results\\rTest model performance, gather stakeholder feedback, and iteratively refine analysis.\\r\\r\\nProgramming Ocean Academy - Data Science Bootcamp- FEB- 2024\\rAspect\\rRecommender Systems\\rFraud Detection\\rApplication Area\\rUsed in various platforms for recommendations, such as Netflix and Facebook\\rPrimarily applied in retail banking and finance for detecting fraudulent activities\\rObjective\\rRecommending relevant content or products to users based on their preferences and behavior\\rIdentifying potentially fraudulent transactions in real-time\\rTechniques\\rUtilizes algorithms like collaborative filtering, content-based filtering, and matrix factorization\\rEmploys machine learning models such as anomaly detection, supervised learning, and unsupervised learning\\rData Sources\\rRelies on user interactions, ratings, and historical behavior to generate recommendations\\rAnalyzes transaction data, user behavior patterns, and historical fraud cases to train models\\rReal-time\\rRecommendations can be generated in real-time based on user actions and preferences\\rFraud detection operates in real-time to quickly flag suspicious transactions for further investigation\\rBusiness Impact\\rEnhances user experience and increases engagement by providing personalized recommendations\\rHelps prevent financial losses by identifying and mitigating fraudulent activities in the banking sector\\r\\r\\n\\r\\nProgramming Ocean Academy - Data Science Bootcamp- FEB -2024\\rAspect\\rMachine Learning\\rGenerative AI\\rDeep Learning\\rDefinition\\rUtilizes algorithms to learn patterns from data and make predictions or decisions\\rFocuses on generating new data based on learned patterns, mimicking human creativity\\rSubset of machine learning, employing multiple layers of neural networks for complex tasks\\rFocus\\rPredictions, classifications, and clustering\\rCreation of new data, such as images, music, and text\\rComplex pattern recognition, speech recognition, and image processing\\rTechniques\\rIncludes supervised learning, unsupervised learning, and reinforcement learning\\rUtilizes models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)\\rRelies on neural network architectures like convolutional neural networks (CNNs) and recurrent neural networks (RNNs)\\rTraining Data\\rRequires labeled data for supervised learning, and may use unlabeled data for unsupervised learning\\rTrained on large volumes of data to learn underlying patterns and distributions\\rTrained on extensive datasets to optimize model parameters and learn complex features\\rOutput\\rPredictions or classifications based on input data\\rGenerates new data samples based on learned patterns\\rOutputs predictions, classifications, or generated content based on input\\rApplications\\rWidely used in various domains, including finance, healthcare, and marketing\\rApplied in art generation, content creation, and data augmentation\\rUsed in speech recognition, image processing, natural language processing, and autonomous driving\\rComputational Power\\rRequires moderate to high computational resources, depending on model complexity\\rDemands significant computational power, often utilizing GPUs for training\\rNeeds extensive computational resources due to complex network architectures and large datasets\\r\\r\\nProgramming Ocean Academy - Data Science Bootcamp - Feb 2024\\rFile Format\\rDescription\\rCharacteristics\\rDelimited Text File Formats\\rText files where each line has values separated by a delimiter. Comma-separated values (CSVs) and tab-separated values (TSVs) are common examples.\\r- Delimiter separates values - Common delimiters: comma, tab, colon, vertical bar, space - Rows represent records - First row often contains column headers\\rMicrosoft Excel Open XML Spreadsheet (XLSX)\\rXML-based spreadsheet format developed by Microsoft. Can contain multiple worksheets with data organized into rows and columns.\\r- XML-based - Multiple worksheets in a workbook - Rows and columns organize data - Cells contain data - Generally accessible to other applications - Known for security - Cannot save malicious code\\rExtensible Markup Language (XML)\\rMarkup language with set rules for encoding data. Suitable for sending information over the internet.\\r- Markup language - Readable by humans and machines - Designed for internet data transmission - No predefined tags like HTML - Platform and programming language independent - Simplifies data sharing between systems\\rPortable Document Format (PDF)\\rDeveloped by Adobe to present documents independent of software, hardware, and operating systems.\\r- Developed by Adobe - Document format - Independent of software/hardware - Ensures consistent display across devices - Commonly used in legal and financial documents - Can be used for form data entry\\rJavaScript Object Notation (JSON)\\rText-based open standard for transmitting structured data over the web.\\r- Text-based - Open standard - Structured data transmission - Language-independent - Readable in any programming language - Easy to use - Compatible with various browsers - Suitable for sharing data of any size and type, including audio and video - Commonly used in APIs and web services for data exchange\\r\\r\\nProgramming Ocean Academy -Data Science Bootcamp - FEB 2024\\rDifferent Definitions of Data Scientists\\rDefinition\\rKey Points\\rSimon Rogers\\rA data scientist finds solutions to problems by analyzing data (big or small) using appropriate tools and communicates findings effectively.\\rSimon Rogers (Data Science Definition)\\rData science is what data scientists do.\\rProfessor Eric Miller\\rEngineering is what engineers do; similarly, data science is what data scientists do.\\rDr. Vincent Granville\\rDefines a data scientist as one who can process large datasets efficiently, distrusts statistical models, and requires strong mathematical background.\\rDr. Vincent Granville (Data Science Definition)\\rData science is distinguished from statistics, but requires a background in algebra, calculus, and probability/statistics.\\rDr. Patil\\rA data scientist is a blend of skills that can unlock insights from data and tell compelling stories with data.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rCharacteristic\\rStructured Data\\rSemi-Structured Data\\rUnstructured Data\\rDefinition\\rData with a well-defined structure or schema.\\rData with some organizational properties, lacks a fixed schema.\\rData without a discernible structure or schema.\\rStorage Format\\rStored in well-defined schemas (databases).\\rCannot be stored in rows and columns like databases.\\rTypically stored in files or NoSQL databases.\\rExamples\\rSQL databases, spreadsheets, online forms.\\rXML, JSON, emails, binary executables.\\rWeb pages, social media feeds, images, documents.\\rAccessibility\\rEasily accessible and analyzed with standard tools.\\rRequires parsing and interpretation for analysis.\\rOften requires specialized tools for analysis.\\rAnalysis\\rCan be analyzed using standard data analysis methods.\\rRequires customized parsing and processing.\\rMay involve manual analysis or specialized tools.\\rData Model\\rAdheres to a specified data model.\\rHas some organizational properties but lacks a rigid schema.\\rNo identifiable structure or schema.\\rHierarchy\\rFollows a strict hierarchy.\\rContains tags and elements for grouping data.\\rNo inherent hierarchy; data may be disorganized.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rData Source\\rDescription\\rExamples\\rRelational Databases\\rUsed for storing structured data with well-defined schemas.\\rSQL Server, Oracle, MySQL, IBM DB2\\rFlat Files\\rStore data in plain text format with records separated by delimiters.\\rCSV files\\rXML Datasets\\rContain data marked up using tags, suitable for hierarchical data representation.\\rOnline surveys, bank statements\\rAPIs and Web Services\\rProvide interfaces for accessing and retrieving data from various sources.\\rTwitter API, Facebook API, stock market APIs\\rWeb Scraping\\rExtracts relevant data from unstructured web sources based on defined parameters.\\rBeautifulSoup, Scrapy, Pandas, Selenium\\rData Streams\\rAggregates constant streams of data from sources like IoT devices, applications, and social media.\\rApache Kafka, Apache Spark Streaming, Apache Storm\\rRSS Feeds\\rCaptures updated data from online forums and news sites for streaming to user devices.\\rNews websites, online forums\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rKey Points\\rSummary\\rRelational Databases\\r- Flexibility of relational databases for structured data. - Challenges include moving data between different databases and dealing with versioning issues.\\rEvolution of Unstructured Data\\r- Rise of unstructured data (logs, documents, XML, JSON). - Relational databases face challenges with heavy write-intensive applications like IoT and social media data.\\rVariety of Data Sources\\r- Data engineers need to work with standard formats (CSV, JSON, XML) and proprietary formats. - Data sources include relational databases, NoSQL databases, big data repositories, data at rest, streaming data, and data in motion.\\rChallenges with Data Formats\\r- Log data presents challenges due to its unstructured nature, often requiring custom tools for analysis. - XML, while popular in the past, can be resource-intensive due to its tags. - JSON gained popularity for its simplicity and is widely used in RESTful APIs. - Newer formats like Apache Avro are gaining popularity for their efficiency.\\rHandling Data Conversion Challenges\\r- Example of converting data from Db2 to SQL Server. - Challenges include differences in import/export processes and handling special characters in the data. - Different tables may require different delimiters due to varying special characters.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rMetadata Overview\\rMetadata is data that describes other data, crucial in databases, data warehousing, and business intelligence systems. Three main types: technical, process, and business metadata.\\rTechnical Metadata\\rDefines data structures, stored in system catalogs. Includes tables, data catalogs, and information on databases, tables, and columns.\\rProcess Metadata\\rDescribes processes in business systems. Monitors system performance, data movement, and user access.\\rBusiness Metadata\\rHelps users discover and understand data. Includes acquisition details, data descriptions, and data source connections.\\rMetadata Management\\rInvolves developing policies for accessing and integrating data. Focuses on creating a reliable data catalog for efficient data organization. Enhances data discovery, repeatability, and governance.\\rImportance of Metadata Management\\rEnhances data governance by providing context and lineage. Ensures data quality, consistency, and security. Facilitates effective data management and usage across the organization.\\rPopular Metadata Management Tools\\r- IBM InfoSphere Information Server - CA Erwin Data Modeler - Oracle Warehouse Builder - SAS Data Integration Server - Talend Data Fabric - Alation Data Catalog - SAP Information Steward - Microsoft Azure Data Catalog - IBM Watson Knowledge Catalog - Oracle Enterprise Metadata Management (OEMM) - Adaptive Metadata Manager - Unifi Data Catalog - data.world - Informatica Enterprise Data Catalog\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTerm\\rDefinition\\rComma-separated values (CSVs)\\rDelimited text files where the delimiter is a comma. Used to store structured data.\\rDelimited text file formats\\rText files are used to store data where each line or row has values separated by a delimiter. A delimiter is a sequence of one or more characters specifying the boundary between values. Common delimiters include comma, tab, colon, vertical bar, and space.\\rNoSQL databases\\rDatabases are designed to store and manage unstructured data and provide analysis tools for examining this type of data.\\rOnline Transaction Processing (OLTP) Systems\\rSystems that focus on handling business transactions and storing structured data.\\rRelational databases\\rDatabases are designed to store structured data with well-defined schemas and support standard data analysis methods and tools.\\rSensors\\rDevices such as Global Positioning Systems (GPS) and Radio Frequency Identification (RFID) tags generate structured data.\\rSpreadsheets\\rSoftware applications like Excel and Google Spreadsheets are used for organizing and analyzing structured data.\\rSQL Databases\\rDatabases that use Structured Query Language (SQL) for defining, manipulating, and querying data in structured formats.\\rTab-separated values (TSVs)\\rDelimited text files where the delimiter is a tab. Used as an alternative to CSV when literal commas are present in text data.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTerm\\rDefinition\\rData Repository\\rA general term used to refer to data that has been collected, organized, and isolated so that it can be used for business operations or mined for reporting and data analysis. It can be a small or large database infrastructure with one or more databases that collect, manage, and store data sets. In this video, we will provide an overview of the different types of repositories your data might reside in, such as databases, data warehouses, and big data stores, and examine them in greater detail in further videos.\\rDatabase\\rA collection of data, or information, designed for the input, storage, search and retrieval, and modification of data. A Database Management System, or DBMS, is a set of programs that creates and maintains the database. It allows you to store, modify, and extract information from the database using a function called querying.\\rRelational Databases\\rAlso referred to as RDBMSes, relational databases build on the organizational principles of flat files, with data organized into a tabular format with rows and columns following a well-defined structure and schema. Unlike flat files, RDBMSes are optimized for data operations and querying involving many tables and much larger data volumes. Structured Query Language, or SQL, is the standard querying language for relational databases.\\rNon-Relational Databases\\rAlso known as NoSQL, or \"Not Only SQL\", non-relational databases emerged in response to the volume, diversity, and speed at which data is being generated today, mainly influenced by advances in cloud computing, the Internet of Things, and social media proliferation. Built for speed, flexibility, and scale, non-relational databases made it possible to store data in a schema-less or free-form fashion. NoSQL is widely used for processing big data.\\rData Warehouse\\rA central repository that merges information coming from disparate sources and consolidates it through the extract, transform, and load process (ETL process) into one comprehensive database for analytics and business intelligence. At a very high-level, the ETL process helps you to extract data from different data sources, transform the data into a clean and usable state, and load the data into the enterprise\\'s data repository.\\rData Marts\\rData marts are subsets of data warehouses that focus on specific business lines or departments.\\rData Lakes\\rData lakes are repositories that store vast amounts of raw data in its native format until it is needed.\\rBig Data Stores\\rDistributed computational and storage infrastructure to store, scale, and process very large data sets.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTerm\\rDefinition\\rRelational Database\\rA collection of data organized into a table structure, where the tables can be linked based on common data. Tables consist of rows and columns, with rows representing records and columns representing attributes. Tables can be related based on shared data, allowing for the retrieval of new tables with a single query. Relational databases use Structured Query Language (SQL) for querying data. They minimize data redundancy, offer data integrity through data type constraints, and provide security features for controlled access to data.\\rExamples of Relational Databases\\r- IBM DB2 - Microsoft SQL Server - MySQL - Oracle Database - PostgreSQL - Amazon Relational Database Service (RDS) - Google Cloud SQL - IBM DB2 on Cloud - Oracle Cloud - SQL Azure\\rAdvantages of Relational Databases\\r- Flexibility: Changes can be made to the database structure while it\\'s running. - Reduced redundancy: Minimizes data duplication. - Ease of backup and disaster recovery: Offers easy export/import options and continuous mirroring in cloud-based solutions. - ACID-compliance: Ensures data accuracy and consistency. - Well-documented and mature technology.\\rUse Cases for Relational Databases\\r- Online Transaction Processing (OLTP): Well-suited for transaction-oriented tasks with frequent queries and updates. - Data Warehouses: Optimized for Online Analytical Processing (OLAP) for historical data analysis. - IoT Solutions: Lightweight database solution for collecting and processing data from edge devices.\\rLimitations of Relational Databases\\r- Not suitable for semi-structured and unstructured data analytics. - Migration between RDBMSs requires identical schemas and data types. - Limits on the length of data fields.\\rContinued Relevance of RDBMS\\rDespite the evolution of data technologies, such as big data and IoT, relational databases continue to be the predominant technology for working with structured data\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rData Science Task Categories\\rData Asset Management\\rCode Asset Management\\rExecution Environments\\rDevelopment Environments\\rData Management\\r- Collect, persist, and retrieve data securely, efficiently, and cost-effectively from various sources like Twitter, Flipkart, Media, and Sensors.\\r- Organize and manage important data collected from different sources in a central location.\\r- Provides system resources to execute and verify the code.\\r- Provides a workspace and tools to develop, implement, execute, test, and deploy source code.\\rData Integration and Transformation\\r- Extract, Transform, and Load (ETL) data from multiple repositories into a central Data Warehouse.\\r- Version control and collaboration for managing changes to software projects\\' code.\\r- Libraries to compile the source code.\\r- IDEs like IBM Watson Studio for developing, testing, and deploying source code.\\rData Visualization\\r- Graphical representation of data and information using charts, plots, maps, etc.\\r- Organizing and managing data with versioning and collaboration support.\\r- Tools for compiling and executing code.\\r- Testing and simulation tools provided by IDEs to emulate real-world behavior.\\rModel Building\\r- Train data and analyze patterns using machine learning algorithms.\\r- Unified view for managing an inventory of assets.\\r- System resources for executing and verifying code.\\r- Cloud-based execution environments like IBM Watson Studio for preprocessing, training, and deploying models.\\rModel Deployment\\r- Integrate developed models into production environments via APIs.\\r- Share, collaborate, and manage code files simultaneously.\\r- Tools for compiling and executing code.\\r- Integrated tools like IBM Watson Studio and IBM Cognos Dashboard Embedded for developing deep learning and machine learning models.\\rModel Monitoring and Assessment\\r- Continuous quality checks to ensure model accuracy, fairness, and robustness.\\r- N/A\\r- Libraries for compiling and executing code.\\r- N/A\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rData Management Tools\\rData Integration and Transformation Tools\\rData Visualization Tools\\rModel Tools\\rCode Asset Management Tools\\rData Asset Management Tools\\rMySQL, PostgreSQL, MongoDB, Apache CouchDB, Apache Cassandra, Hadoop File System, Ceph, Elastic search\\rApache AirFlow, KubeFlow, Apache Kafka, Apache Nifi, Apache SparkSQL, NodeRED\\rPixie Dust, Hue, Kibana, Apache Superset\\rApache PredictionIO, Seldon, Kubernetes, Redhat OpenShift, MLeap, TensorFlow service, TensorFlow lite, TensorFlow dot JS\\rGit, GitHub, GitLab, Bitbucket\\rApache Atlas, ODPi Egeria, Kylo\\r\\r\\n\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rDevelopment Environments\\rDescription\\rJupyter\\r- Interactive Python programming tool.  - Supports over a hundred different programming languages through kernels.  - Unifies documentation, code, output, shell commands, and visualizations in a single document.  - Jupyter Lab is the next version of Jupyter Notebooks, offering more modern and modular architecture.\\rApache Zeppelin\\r- Inspired by Jupyter Notebooks, provides a similar experience.  - Integrated plotting capability without requiring external libraries.  - Allows extension of capabilities using additional libraries.\\rRStudio\\r- Development environment for statistics and data science, exclusively for R and associated R libraries.  - Offers Python development within the R environment.  - Unifies programming, execution, debugging, remote data access, exploration, and visualization into one tool.\\rSpyder\\r- Mimics the functionality of RStudio for the Python world.  - Integrates code, documentation, and visualizations into a single canvas.\\rCluster Execution Environments\\rDescription\\rApache Spark\\r- Batch data processing engine with linear scalability.  - Used across various industries, offering parallel processing capabilities.\\rApache Flink\\r- Stream-processing engine focusing on real-time data streams.  - Supports both batch and stream processing paradigms.\\rRay\\r- Focuses on large-scale deep learning model training.  - Latest development in data science execution environments.\\rFully Integrated and Visual Tools\\rDescription\\rKNIME\\r- Visual user interface with drag-and-drop capabilities.  - Built-in visualization and extension capabilities with R, Python, and connectors to Apache Spark.\\rOrange\\r- Less flexible than KNIME but easier to use.  - Offers visual tools for data integration, transformation, visualization, and model building.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTool Category\\rCommercial Tools\\rData Management\\r- Oracle Database  - Microsoft SQL Server  - IBM Db2\\rData Integration and Transformation\\r- Informatica PowerCenter - IBM InfoSphere DataStage - SAP products - Oracle products - SAS products - Talend products - Microsoft products - Watson Studio Desktop (includes Data Refinery)\\rData Visualization\\r- Tableau - Microsoft Power BI - IBM Cognos Analytics - Watson Studio Desktop\\rModel Tools for Building\\r- SPSS Modeler - SAS Enterprise Miner - Watson Studio Desktop (includes SPSS Modeler)\\rModel Tools for Deployment\\r- SPSS Collaboration and Deployment Services\\rModel Tools for Monitoring and Assessment\\rNo relevant commercial tools available; open source preferred\\rCode Asset Management\\rOpen-source tools like Git and GitHub are standard\\rData Asset Management\\r- Informatica Enterprise Data Governance - IBM Information Governance Catalog\\rDevelopment Environment\\r- Watson Studio (cloud-based and desktop version) - H2O Driverless AI\\rFully Integrated Visual Tools\\r- Watson Studio (together with Watson Open Scale) - H2O Driverless AI\\r\\r\\n\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTool Category\\rCommercial Cloud Tools\\rFully Integrated Visual Platform\\r- Watson Studio and Watson Open Scale  - Microsoft Azure Machine Learning  - H2O Driverless AI\\rData Management\\r- Amazon Web Services DynamoDB  - Cloudant  - IBM Db2 as a service\\rData Integration and Transformation\\r- Informatica Cloud Data Integration - IBM Data Refinery (part of Watson Studio)\\rData Visualization\\r- Datameer  - IBM Cognos Business Intelligence suite  - IBM Data Refinery (part of Watson Studio)\\rModel Building\\r- Watson Machine Learning  - Google AI Platform Training\\rModel Deployment\\r- SPSS Collaboration and Deployment Services  - Amazon SageMaker Model Monitor  - Watson Machine Learning\\rModel Monitoring\\r- Watson OpenScale\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rCriteria for Determining Language to Learn\\rPopular Languages\\rConsider your needs and the problems you are trying to solve\\rPython, R, SQL\\rAssess who you are solving these problems for\\rScala, Java, C++, Julia\\r\\rJavaScript, PHP, Go, Ruby, Visual Basic\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rUsers of Python\\rBenefits of Using Python\\rDiversity and Inclusion Efforts of the Python Community\\rExperienced Programmers\\r- Clear and readable syntax\\r- Code of Conduct ensuring safety and inclusion for all members\\rBeginners\\r- Huge global community and wealth of documentation\\r- Community initiatives like PyLadies creating safe and inclusive spaces\\rData Professionals\\r- Versatility for data science, AI, machine learning, web development, etc.\\r- Mentorship programs like PyLadies helping underrepresented groups become active participants and leaders in the Python community\\r\\r- Rich ecosystem of libraries and frameworks for various tasks\\r\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rUsers of the R Language\\rBenefits of Using R\\rGlobal Communities for Connecting with Other R Users\\r- Statisticians\\rFree software, allowing for private, commercial, and public use\\r- useR\\r- Mathematicians\\r- Supported by a wide global community of people who want to solve big problems\\r- WhyR\\r- Data miners\\r- Array-oriented syntax makes it easier to translate from math to code for learners with no or minimal programming background\\r- SatRdays\\r\\r- Large repository of statistical knowledge\\r- R-ladies\\r\\r- Integration with other languages like C++, Java, Python, etc.\\r- R project website for R conferences and events\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp- Feb 2024\\rKey Points\\rSQL is officially pronounced as \"ess cue el\" or \"sequel\".\\rIt stands for \"Structured Query Language\" and is non-procedural.\\rSQL is simple and powerful, making it commonly used by data scientists.\\rDeveloped by IBM in 1974, making it older than Python and R.\\rSQL is designed for managing data in relational databases.\\rRelational databases consist of two-dimensional tables with fixed columns and variable rows.\\rSQL language elements include Clauses, Expressions, Predicates, Queries, and Statements.\\rLearning SQL is valuable for various data science roles, such as business/data analysts and data engineers.\\rSQL enables direct access to data, speeding up workflow executions.\\rIt is an ANSI standard, allowing for portability across different databases.\\rVarious SQL databases are available, including MySQL, PostgreSQL, Oracle, etc.\\rSQL syntax may vary depending on the relational database management system being used.\\rFocus on learning SQL for a specific database and plug into the community for support.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp- Feb 2024\\rLanguage\\rDescription\\rPopular Tools/Frameworks\\rJava\\rGeneral-purpose object-oriented programming language. Widely adopted in the enterprise space.\\rWeka, Java-ML, Apache MLlib, Deeplearning4, Hadoop\\rScala\\rGeneral-purpose language supporting functional programming and strong static typing. Interoperable with Java.\\rApache Spark (including Shark, MLlib, GraphX, Spark Streaming)\\rC++\\rGeneral-purpose programming language, extension of C. Provides faster processing, system programming capabilities.\\rTensorFlow, MongoDB, Caffe\\rJavaScript\\rGeneral-purpose language extending beyond browsers with Node.js. Not related to Java.\\rTensorFlow.js, R-js\\rJulia\\rDesigned for high-performance numerical analysis and computational science. Produces programs as fast as C or Fortran.\\rJuliaDB\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rOpen Source\\rFree Software\\rChampioned by the Open-Source Initiative (OSI)\\rDefined by the Free Software Foundation (FSF)\\rMore business-focused\\rMore focused on a set of values\\rExamples: Python, Apache Spark, TensorFlow\\rExamples: R, GNU Compiler Collection (GCC)\\rFree to use\\rFree to use\\rOften uses the General Public License (GNU)\\rOften uses licenses endorsed by the FSF\\rSupports collaboration\\rSupports collaboration\\rCan be used interchangeably with free software in many cases\\r-\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rCategory\\rLibraries\\rScientific Computing Libraries (Python)\\r- Pandas- NumPy\\rVisualization Libraries (Python)\\r- Matplotlib- Seaborn\\rHigh-Level ML/DL Libraries (Python)\\r- Scikit-learn- Keras\\rDeep Learning Libraries (Python)\\r- TensorFlow- Pytorch\\rLibraries in Other Languages\\r- Apache Spark (Python, R, Scala, SQL)- Vegas (Scala)- BigDL (Scala)- ggplot2 (R)\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rLibrary\\rDefinition\\rBenefits\\rCommon Uses\\rPandas\\rPython library providing data structures and functions for efficient data manipulation and analysis.\\r- Offers DataFrame object for easy manipulation of structured data.- Provides tools for data cleaning, reshaping, and merging.- Supports handling of missing data and time-series data.- Integration with other Python libraries such as NumPy and Matplotlib.\\rData cleaning, manipulation, analysis, and preparation.\\rNumPy\\rPython library for numerical computing, providing support for large, multi-dimensional arrays and matrices.\\r- Efficient handling of large arrays and matrices.- Extensive mathematical functions for array manipulation and operations.- Integration with other Python libraries.- Supports linear algebra, Fourier transform, and random number generation.\\rScientific computing, numerical analysis, linear algebra, statistical analysis.\\rMatplotlib\\rPython library for creating static, interactive, and animated visualizations in Python.\\r- Wide range of plot types including line plots, bar charts, scatter plots, histograms, etc.- Highly customizable plot appearance.- Support for multiple data formats and output types.- Seamless integration with Pandas and NumPy.\\rData visualization, exploratory data analysis, presentation of research findings.\\rSeaborn\\rStatistical data visualization library based on Matplotlib, providing a high-level interface for creating attractive and informative plots.\\r- Simplifies complex visualizations with concise syntax.- Supports statistical plotting with built-in themes and color palettes.- Integration with Pandas DataFrames.- Additional plot types and features compared to Matplotlib.\\rStatistical data visualization, exploratory data analysis, presentation-ready plots.\\rScikit-learn\\rPython library providing simple and efficient tools for data mining and data analysis.\\r- Simple and consistent API for building and evaluating machine learning models.- Comprehensive documentation and user-friendly interface.- Wide range of algorithms for classification, regression, clustering, and dimensionality reduction.- Integration with other Python libraries such as Pandas and NumPy.\\rMachine learning algorithms development and evaluation, predictive modeling, data mining.\\rKeras\\rHigh-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit.\\r- Simplifies the construction, training, and evaluation of deep learning models.- User-friendly interface for building complex neural networks.- Supports both CPU and GPU acceleration.- Seamless integration with TensorFlow and other deep learning frameworks.- Extensive community support and documentation.\\rDeep learning model development, prototyping, experimentation, research, production deployment.\\rTensorFlow\\rOpen-source machine learning framework developed by Google Brain, capable of running on CPUs, GPUs, and TPUs.\\r- Scalable and flexible architecture suitable for a wide range of applications.- High-performance computation with support for distributed computing.- Extensive library ecosystem for machine learning and deep learning tasks.- Integration with other frameworks such as Keras and PyTorch.\\rDeep learning model development, research, production deployment, scalable machine learning tasks.\\rPyTorch\\rOpen-source deep learning framework developed by Facebook\\'s AI Research lab, emphasizing flexibility and ease of use.\\r- Dynamic computation graph allowing for dynamic neural network architectures.- Easier debugging and prototyping with Pythonic syntax.- Support for GPU acceleration and distributed training.- Large and active community contributing to the framework\\'s development and ecosystem.- Integration with other Python libraries and frameworks.\\rDeep learning model development, research, experimentation, prototyping, production deployment.\\rApache Spark\\rGeneral-purpose cluster computing framework designed for fast and efficient large-scale data processing.\\r- In-memory computing for rapid data processing.- Fault-tolerance and reliability in distributed computing environments.- Support for various programming languages including Python, Scala, Java, and SQL.- Integration with other big data tools and frameworks.- Scalability and performance optimization for large-scale data analytics.\\rLarge-scale data processing, data analytics, machine learning on big data, distributed computing.\\rVegas (Scala)\\rScala library for creating statistical data visualizations, providing an API for generating charts and plots from Spark DataFrames.\\r- Seamless integration with Spark DataFrames for data visualization on distributed datasets.- Support for various chart types and styles, including bar charts, histograms, and scatter plots.- Extensible and customizable chart configuration options.- Ability to work with both local and distributed data sources.\\rStatistical data visualization, exploratory data analysis, big data visualization.\\rBigDL (Scala)\\rDistributed deep learning library for Apache Spark, enabling deep learning on distributed datasets.\\r- Enables deep learning model training and inference directly on Spark clusters.- Supports a wide range of deep learning models and algorithms.- Integration with Spark MLlib and DataFrame APIs.- Scalability for training models on large-scale datasets.- Compatibility with existing Spark ecosystem tools and libraries.\\rDistributed deep learning model training, large-scale deep learning tasks, big data analytics with deep learning.\\rggplot2 (R)\\rR package for creating elegant and complex data visualizations, inspired by the Grammar of Graphics.\\r- Provides a flexible and powerful syntax for creating sophisticated plots and charts.- Extensive customization options for plot appearance and layout.- Seamless integration with R\\'s data manipulation and analysis capabilities.- Support for complex data structures and statistical transformations.\\r\\r\\r\\n\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTerm\\rDefinition\\rAPI\\rAllows communication between two pieces of software by providing an interface for inputs and outputs without exposing the backend details.\\rPandas\\rExample of a library with an API that allows communication with software components for data processing without knowledge of the backend.\\rTensorFlow\\rBackend written in C++ with APIs available for Python, JavaScript, C++, Java, and Go, allowing communication and utilization of TensorFlow\\'s functionalities in different programming languages.\\rREST API\\rStands for Representational State Transfer API, enabling communication over the internet to utilize resources such as storage, data, and algorithms.\\rClient\\rYour program, which communicates with the web service through the REST API.\\rResource\\rThe web service accessed via the REST API, providing functionalities or data.\\rEndpoint\\rThe location where the client accesses the web service to send requests and receive responses.\\rRequest\\rSent by the client to the resource via HTTP methods, containing instructions for the operation to be performed.\\rResponse\\rReturned by the resource to the client via HTTP methods, containing information or data requested by the client.\\rHTTP Message\\rUsed to transmit data over the internet between the client and the resource.\\rJSON\\rData format used in HTTP messages for requests and responses, containing instructions or information in a structured manner.\\rExample: Text to Speech API\\rConverts speech to text; client sends an audio file via a POST request, and the API returns the text transcription in a response via a GET request.\\rExample: Language Translator API\\rTranslates text from one language to another; client sends text to be translated, and the API returns the translated text.\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rData Set Definition\\rA structured collection of data representing information in various formats such as text, numbers, images, audio, or video files. Tabular data sets are organized into rows and columns, often stored in formats like CSV (Comma Separated Values).\\rTypes of Data Ownership\\rTraditionally, data sets were private, containing proprietary or confidential information, and were not shared publicly. However, many entities now provide data sets as \"open data,\" making them available for free to the public.\\rSources of Data\\rData sets are sourced from various entities including scientific institutions, governments, organizations, and companies. Governments and organizations worldwide publish data sets on topics such as the economy, society, healthcare, transportation, and the environment.\\rCommunity Data License Agreement (CDLA)\\rCDLA was created by the Linux Foundation to address licensing concerns related to the distribution and use of open data. It includes two licenses: CDLA-Sharing and CDLA-Permissive. These licenses define terms for using and modifying data sets, with CDLA-Permissive allowing modifications without the requirement to share changes.\\rCDLA-Sharing License\\rAllows users to use and modify data sets, with the requirement to share any modifications under the same license terms as the original data.\\rCDLA-Permissive License\\rAllows users to use and modify data sets without the requirement to share changes. Users are not obligated to share modifications to the data.\\rImpact of Open Data on Data Science\\rOpen data has played a significant role in the growth of data science, machine learning, and artificial intelligence by providing practitioners with access to a wide range of data sets.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rData Asset eXchange (DAX)\\rIBM\\'s open data repository, providing a curated collection of high-quality open data sets from IBM Research and trusted third-party sources.\\rPurpose of DAX\\rTo provide developers with access to unique, high-quality data sets for use in enterprise applications.\\rData Sets Available on DAX\\rDAX offers various data sets, including images, video, text, and audio data.\\rCommunity Data License Agreement (CDLA)\\rData sets on DAX are made available under the CDLA to foster data sharing and collaboration, ensuring clear license and usage terms.\\rNotebook Tutorials\\rDAX provides tutorial notebooks that guide developers through data cleaning, pre-processing, exploratory analysis, and more. Some data sets include advanced notebooks for tasks such as creating charts, training machine learning models, and performing statistical and time-series analysis.\\rAccessing DAX\\rDevelopers can access DAX on the IBM Developer website under \"Open Source at IBM\" and then selecting \"Data Asset eXchange\" from the dropdown menu.\\rExploring Data Sets on DAX\\rDevelopers can explore various open data sets on DAX, such as the \"NOAA Weather Data - JFK Airport\" data set, which contains weather data from John F. Kennedy Airport in New York.\\rAccessing Notebooks in Watson Studio\\rNotebooks associated with data sets on DAX can be accessed and executed in Watson Studio, allowing developers to perform data cleaning, pre-processing, exploratory analysis, and more.\\rData Files Available on DAX\\rDAX provides access to one or more data files associated with each data set. Developers can view and access these data files to use in their projects.\\rIntegration with IBM Cloud and Watson Studio\\rDevelopers can log into their IBM Cloud account, create a project, and load DAX notebooks into the project in Watson Studio. Notebooks can be executed in Watson Studio for data analysis and exploration\\r\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rMachine Learning (ML)\\rUses algorithms (models) to identify patterns in data.\\rModel Training\\rProcess by which the model learns data patterns.\\rTypes of Machine Learning\\r- Supervised Learning: Requires input data and correct outputs for training. Includes regression and classification models. - Unsupervised Learning: Analyzes unlabeled data to identify patterns and structure. Includes clustering models. - Reinforcement Learning: Learns through trial and error to maximize rewards.\\rSupervised Learning Types\\r- Regression Models: Predict numeric values. - Classification Models: Predict categories or classes.\\rDeep Learning\\rSpecialized type of ML that emulates human brain functioning. Used for analyzing natural language, images, audio, video, and more.\\rTraining Deep Learning Models\\rRequires large labeled datasets and is computationally intensive. Frameworks: TensorFlow, PyTorch, Keras. Pre-trained models available in model zoos.\\rBuilding a Model Example\\r- Collect and prepare data. - Label raw training data. - Build or select a model. - Train the model on prepared data. - Analyze training results and iterate. - Deploy the trained model.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rModel Asset eXchange (MAX)\\r- Free open-source repository for deep learning models. - Provides ready-to-use and customizable deep learning microservices. - Offers models tested and deployable in local and cloud environments. - Models available under permissive open-source licenses.\\rBenefits of Using Pre-trained Models\\r- Reduces time to value. - Saves resources, labor, and time required for training from scratch.\\rComponents of Model-serving Microservice\\r- Pre-trained deep learning model. - Input pre-processing code. - Output post-processing code. - Standardized public API.\\rDistribution of MAX Microservices\\r- Built and distributed as open-source Docker images on GitHub. - Can be customized for personal and commercial use.\\rDeployment Automation Platforms\\r- Kubernetes: Automates deployment, scaling, and management of Docker images. - Red Hat OpenShift: Enterprise-grade Kubernetes platform available on multiple cloud platforms.\\rExploring Predefined Models\\r- Visit ml-exchange.org to view and use predefined models. - Example: Object detector model.\\rUsing CodePen for Model Interaction\\r- CodePen: Online tool for editing front-end languages. - Allows interaction with MAX TensorFlow.js model for object detection.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rNotes\\rDefinition of Jupyter Notebooks\\r- Originally known as \"iPython,\" developed for Python programming.\\r\\r- Renamed Jupyter to support additional languages: Julia, Python, and R, among others.\\r\\r- Browser-based application allowing creation and sharing of documents with code, equations, visualizations, and narrative text.\\rFeatures of Jupyter Notebooks\\r- Combines descriptive text, code blocks, and code output in a single file.\\r\\r- Generates output, including plots and tables, within the notebook file.\\r\\r- Exportable to PDF or HTML formats for sharing.\\rIntroduction to JupyterLab\\r- Browser-based application for accessing multiple Jupyter Notebook files and other code/data files.\\r\\r- Extends functionalities of Jupyter Notebooks with features like multiple notebooks, text editors, terminals, etc.\\r\\r- Compatible with various file formats such as CSV, JSON, PDF, Vega, and others.\\r\\r- Open-source and supports integration with cloud-based services like IBM and Google Colab.\\rInstallation and Usage\\r- Can be installed via command line using pip install.\\r\\r- Can be downloaded locally through Anaconda Platform from Anaconda.com.\\r\\r- Anaconda distribution includes Jupyter and JupyterLab.\\r\\r- Hosted version available in Skills Network Labs, eliminating the need for local installations.\\r\\r\\n\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rActions\\rRunning, Inserting, Deleting\\r- Click \"Run\" button or use Shift + Enter shortcut to execute cells.\\rCells\\r- Add new cell with the plus symbol in the toolbar.\\r\\rDelete cell by clicking \"Edit\" on the main menu bar, then \"Delete Cells,\" or using shortcut D x2.\\rWorking with Multiple Notebooks\\r- Open new notebook with plus button or \"File\" > \"Open a new launcher\" or \"Open a new notebook.\"\\r\\r- Move notebooks, place side by side, or on separate tabs.\\rPresenting Results\\r- Use Markdown cells for titles and text descriptions.\\r\\r- Convert cells and outputs into slides for presentation.\\r\\r- Slides functionality delivers code, visualizations, text, and outputs as part of a project.\\rShutting Down Notebooks\\r- Click stop icon on the sidebar to shut down notebooks.\\r\\r- Terminate all sessions at once or shut down individually.\\r\\r- \"No kernel\" at top right indicates inactive session. Close tabs.\\r\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rTopic\\rDescription\\rDefinition of Kernel\\r- A computational engine executing code in a Notebook file.\\r\\rVarious Jupyter Kernels available for different languages.\\rWorking with Kernels\\r- When a Notebook opens, the related kernel launches automatically.\\r\\r- Kernels perform computations and produce results when Notebook is executed.\\r\\r- Some languages pre-installed in Skills Network lab environment: Python, Apache, Julia, R, Swift.\\r\\r- Select language for Data Science project upon notebook launch.\\r\\r- Python kernel enables execution of Python cells, producing output.\\r\\r- Other kernels available: Apache, Julia, R, Swift.\\r\\r- Select kernel on launch page or from dropdown menu in top right corner of Notebook.\\r\\r- Manually install languages through command line interface (CLI) on local machine if not pre-installed.\\r\\r\\n\\r\\nProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rAspect\\rDescription\\rJupyter Architecture\\r- Implements a two-process model with a kernel and a client.\\r\\r- Client: Interface (e.g., browser in Jupyter Notebook) for sending code to the kernel.\\r\\r- Kernel: Executes code and returns results to the client for display.\\rJupyter Notebooks\\r- Represent code, metadata, contents, and outputs.\\r\\r\\t- Saved from browser to Notebook server as JSON file with .ipynb extension.\\rNotebook Server\\r- Responsible for saving and loading notebooks.\\rKernel Execution\\r- Executes code cells in the Notebook when user runs them.\\rConversion Process\\r- Uses NBConvert tool to convert files to other formats.\\r\\rConversion involves preprocessor modifying notebook, exporter converting to new format, and postprocessor finalizing output.\\rExample\\r- Converting notebook file to HTML involves preprocessor, exporter, and postprocessor, resulting in an HTML file that can be displayed via its URL.\\r\\r\\n\\rProgramming Ocean Academy- Data Science Bootcamp - Feb 2024\\rAspect\\rDescription\\rComputational Notebooks\\r- Combine code, computational output, explanatory text, and multimedia resources into a single document.\\rJupyter Notebook\\r- Popular type of computational notebook supporting dozens of programming languages.\\rJupyterLab and VS Code\\r- Popular environments for creating and modifying Jupyter Notebooks on a local device.\\rJupyterLab\\r- Web-based application allowing creation of code, interactive visualizations, text, and equations. Includes extensive pre-installed Python libraries like NumPy, Pandas, and Matplotlib.\\rAnaconda\\r- Free and open-source distributor for Python and R, featuring over 1500 libraries. Supports data science and machine learning tasks. Offers free community support and Anaconda Navigator GUI.\\rAnaconda Navigator\\r- Graphical user interface facilitating installation of new packages without CLI. Allows launching JupyterLab and installing additional environments.\\rJupyter Notebook (Anaconda3)\\r- Command to start Jupyter Notebook in Anaconda environment.\\rJupyterLab Dashboard\\r- Accessed via localhost in browser. Manages Jupyter Notebooks.\\rNotebook Creation\\r- Create new Jupyter Notebook by selecting Python 3 from the dropdown menu. Rename notebook from \\'Untitled\\' to desired name.\\rCell Types\\r- Code cells contain code to be executed in kernel and display output. Markdown cells contain rich text and display formatted output.\\rDownloading Notebooks\\r- Access download options via File menu. Select desired download format.\\rVisual Studio Code (VS Code)\\r- Free, open-source code editor supporting debugging and task-running operations. Compatible with Linux, Windows, and macOS. Offers syntax highlighting, auto-indentation, and more.\\rExtensions for VS Code\\r- Install Python extensions to execute Python code in VS Code. Access extensions via Extensions menu or Ctrl + Shift + X keys. Search for \"Python\" and install related extensions.\\rUsing VS Code with Jupyter\\r- Open VS Code from Anaconda Navigator or download from code.visualstudio.com. Configure extensions to execute Python code. Write and execute code in Jupyter Notebook format.\\rSaving in VS Code\\r- Save files by navigating to File menu and selecting Save.\\r\\r\\n\\r\\nAspect\\rData Analysis\\rData Analytics\\rDefinition\\rExtracting meaning from data for decision-making.\\rCollection, analysis, and interpretation of data to support decision-making and goal achievement.\\rScope\\rNarrow, focused on extracting insights from data.\\rBroad, encompasses data collection, analysis, interpretation, and decision support.\\rFocus\\rUnderstanding past events and trends.\\rUnderstanding past events, predicting future outcomes, and prescribing actions.\\rMethods\\rQuantitative and qualitative data analysis techniques.\\rData gathering, preparation, exploration, management, storage, evaluation, and sharing of insights.\\rPurpose\\rFacilitating decision-making based on historical data.\\rInforming decision-making across various organizational functions and predicting future outcomes.\\rTypes\\rQuantitative analysis, qualitative analysis.\\rDescriptive, diagnostic, predictive, prescriptive analytics.\\rTechniques\\rHypothesis analysis, regression analysis, content analysis.\\rVisualization, statistical analysis, machine learning, optimization.\\rExamples\\rAnalyzing sales data to identify trends.\\rPredicting customer behavior to optimize marketing strategies.\\rGoal\\rGain insights from data to inform decisions.\\rImprove decision-making, enhance efficiency, and achieve organizational objectives.\\rImpact\\rImmediate insights for specific questions.\\rLong-term strategic planning and continuous improvement.\\rTools\\rSpreadsheet software, statistical software, data visualization tools.\\rBusiness intelligence platforms, advanced analytics tools, machine learning algorithms.\\r\\r\\n\\r\\nAspect\\rData Analysis\\rData Analytics\\rMeaning\\rExtracting insights from data for decision-making.\\rDevelopment of data pipelines from raw data ingestion to delivery of data products.\\rScope\\rConcentrated on characterizing data and producing predictions.\\rBroad, covering data collection, organization, cleaning, reporting, visualization, and delivery.\\rFocus\\rUnderstanding data patterns and trends.\\rManaging data pipelines and optimizing data processes.\\rRoles\\rData analysts.\\rData engineers for infrastructure and logistics, data analysts for analysis components.\\rSkills\\rStatistical analysis, modeling, SQL, visualization.\\rData storage, modeling, communication, documentation.\\rDifference\\rData analysis is a component of data analytics, but not vice versa.\\rAll data analysis is a component of data analytics, but not all analytics processes involve analysis.\\rTypes\\rDescriptive, diagnostic, predictive, prescriptive analysis.\\rBusiness intelligence, financial modeling, marketing research.\\rTechniques\\rStatistical analysis, modeling, SQL, visualization.\\rData preparation, statistical analysis, modeling, data visualization.\\rBenefits\\rEnhanced decision-making based on historical data.\\rImproved processes, future predictions, and management actions.\\rDescription\\rConcentrated activity within the data analytics pipeline.\\rComprehensive process involving various roles and skills in managing data pipelines.\\r\\r\\nCloud computing - Programming Ocean Academy\\rAspect\\rDescription\\rDefinition\\rDelivery of on-demand computing resources (networks, servers, storage, applications) over the Internet, typically on a pay-as-you-go basis.\\rUses\\r- Web-based applications (e.g., Google Drive, Dropbox)\\r\\n - Business software (e.g., Salesforce)\\r\\n- Data storage and backup\\r\\n- Hosting websites and applications\\rBenefits\\r- Cost savings (no need for expensive infrastructure)\\r\\n- Scalability (expand resources as needed)\\r\\n- Accessibility (access from any device with internet connection)\\rStrengths\\r- High flexibility and scalability\\r\\n- Reduces hardware costs\\r\\n- Collaboration through real-time access\\r\\n- Regular updates and maintenance by providers\\rWeaknesses\\r- Security risks (data breaches, hacking)\\r\\n- Dependency on internet connectivity\\r\\n- Potential for downtime\\r\\n- Limited control over infrastructure management\\r\\r\\nA comparison table between IaaS, PaaS, and SaaS - Programming Ocean Academy For Data Science and Artificial Intelligence\\rAspect\\rIaaS (Infrastructure as a Service)\\rPaaS (Platform as a Service)\\rSaaS (Software as a Service)\\rDefinition\\rProvides virtualized computing resources like servers, storage, and networking\\rProvides platforms for developers to build, test, and deploy applications\\rDelivers software applications over the internet, hosted by a provider\\rUser Responsibility\\rUsers manage the infrastructure (OS, storage, networking, etc.)\\rUsers manage applications and data, but the platform is maintained by the provider\\rUsers only manage software settings, with the provider managing infrastructure and platform\\rUse Case\\rBest for IT admins or companies needing complete control over their infrastructure\\rIdeal for developers needing a ready-to-use platform for building apps\\rBest for end-users requiring access to cloud-based software applications\\rExamples\\rAmazon Web Services (AWS), Microsoft Azure, Google Compute Engine\\rGoogle App Engine, Microsoft Azure PaaS, Heroku\\rGoogle Workspace (Gmail, Docs), Salesforce, Dropbox\\rScalability\\rHigh scalability, users can increase/decrease resources as needed\\rScalable development environment\\rSoftware scalability based on subscription or licensing\\rControl\\rHighest level of control over resources and infrastructure\\rModerate control over applications but less over the underlying infrastructure\\rLeast control, users focus on using the software without worrying about infrastructure\\rCost Model\\rPay for virtualized hardware (servers, storage)\\rPay for platform and tools (databases, middleware)\\rSubscription-based pricing for software access\\rAdvantages\\r- Full control over infrastructure\\r- Simplifies development\\r- Easy to use\\r\\r- Flexible\\r- Reduces time to deploy\\r- Accessible from anywhere\\r\\r- Customizable\\r- No need to manage underlying infrastructure\\r- No maintenance or installation required\\rDisadvantages\\r- Complex to manage\\r- Limited customization of platform\\r- Less control over software and its configuration\\r\\r- Requires specialized knowledge for configuration\\r- Less control over infrastructure\\r- Dependent on the provider for updates\\r\\r\\nThe five essential characteristics of cloud computing- Programming Ocean Academy\\rCharacteristic\\rDescription\\rOn-demand Self-service\\rUsers can access computing resources (e.g., processing power, storage, network) as needed, without requiring human intervention from the service provider.\\rBroad Network Access\\rCloud services can be accessed over the network from a wide range of devices such as smartphones, tablets, laptops, and desktops.\\rResource Pooling\\rComputing resources are pooled to serve multiple users using a multi-tenant model, with dynamic resource allocation based on demand. Users are unaware of locations.\\rRapid Elasticity\\rResources can be quickly scaled up or down to meet user demands, providing flexible and scalable resource provisioning.\\rMeasured Service\\rResource usage is monitored and measured, allowing users to be charged based on actual consumption, ensuring cost efficiency and transparency.\\r\\r\\n\\r\\nA comparison of the three cloud deployment models- Programming Ocean Academy\\rDeployment Model\\rDescription\\rAdvantages\\rDisadvantages\\rPublic Cloud\\rCloud services are provided over the public internet and shared among multiple organizations.\\rCost-effective, scalable, no need for in-house hardware, easy to access.\\rLess control over security, potential for data privacy concerns, shared resources with other organizations.\\rPrivate Cloud\\rCloud infrastructure is exclusively used by a single organization, either managed internally or by a third-party, hosted on-premises or off-site.\\rGreater control over security, customized for organizational needs, enhanced privacy.\\rHigher costs due to dedicated infrastructure, requires more management, less scalable than public cloud.\\rHybrid Cloud\\rCombines public and private clouds, allowing data and applications to be shared between them.\\rFlexibility, optimized resource utilization, allows sensitive data to remain in private cloud while leveraging the scalability of the public cloud.\\rComplex management, higher cost than pure public cloud, potential integration issues between public and private clouds.\\r\\r\\n\\r\\nA comparison of the three cloud service models- Programming Ocean Academy\\rService Model\\rDescription\\rKey Features\\rExamples\\rAdvantages\\rDisadvantages\\rInfrastructure as a Service (IaaS)\\rProvides virtualized computing resources over the internet, including servers, storage, and networking.\\rControl over infrastructure, pay-as-you-go model, scalable.\\rAmazon Web Services (AWS), Microsoft Azure, Google Cloud\\rFlexibility to manage and control infrastructure, scalability, cost-effective for large workloads.\\rRequires technical expertise to manage, higher responsibility for security, system updates, and maintenance.\\rPlatform as a Service (PaaS)\\rProvides a platform that allows developers to build, deploy, and manage applications without dealing with the underlying infrastructure.\\rPre-configured platform, application development tools, scaling.\\rGoogle App Engine, Heroku, Microsoft Azure App Services\\rReduces time for app development, easy deployment, no infrastructure management needed, scalable.\\rLimited control over infrastructure, less flexibility for certain configurations, potential vendor lock-in.\\rSoftware as a Service (SaaS)\\rDelivers software applications over the internet on a subscription basis, with the provider managing infrastructure, platform, and software.\\rHosted software, accessible via browser, no maintenance required.\\rGoogle Workspace, Salesforce, Dropbox\\rNo need for installation or updates, accessible from any device, predictable subscription costs, provider manages everything.\\rLimited customization, data security concerns, dependency on internet connectivity, less control over software updates/features.\\r\\r\\nA comparison table for the 5 Vs of Big Data- Programming Ocean Academy for Data Science and Artificial Intelligence\\rBig Data\\rDefinition\\rExample\\rChallenges\\rOpportunities\\rVelocity\\rThe speed at which data is generated and processed.\\rYouTube uploads hours of video every minute.\\rProcessing and analyzing data in real-time.\\rReal-time insights for fast decision-making.\\rVolume\\rThe large scale or quantity of data generated.\\r2.5 quintillion bytes of data are produced daily.\\rStoring and managing massive datasets.\\rAbility to extract valuable insights from large datasets.\\rVariety\\rThe diversity of data types (structured, unstructured).\\rText, images, videos, sensor data from IoT devices.\\rIntegrating and processing different types of data.\\rMore comprehensive analysis by combining various data types.\\rVeracity\\rThe accuracy and reliability of the data.\\r80% of data is unstructured, which may lack consistency.\\rEnsuring data quality and eliminating false information.\\rBetter decision-making with accurate data.\\rValue\\rThe ability to derive meaningful insights from data.\\rBusiness insights, customer behavior, risk management.\\rExtracting value from vast, complex datasets.\\rIncreased profit, efficiency, and innovation through data-driven decisions.\\r\\r\\n\\r\\n\\rTraditional and new way in Processing of Big Data- Programming Ocean Academy\\rAspect\\rTraditional Data Processing\\rBig Data Processing\\rBig Data Technologies\\rIntegration of Techniques\\rData Science and Analytics Growth\\rEmerging Trends\\rData Handling\\rData brought to the computer for processing.\\rData is sliced, distributed, and processed across multiple computers.\\rHadoop framework for big data.\\rCombines traditional methods (e.g., statistics) with new techniques.\\rRapid growth and increasing relevance.\\rEvolution of deep learning and neural networks.\\rProcessing Approach\\rProgram runs on the complete data set.\\rMapReduce: Map process distributes data, Reduce process aggregates results.\\rDeveloped from Google\\'s approach.\\rUse of machine learning to analyze large data sets.\\rData science field was less known five years ago.\\rSignificant advancements in recent years.\\rScalability\\rLimited by single computer capacity.\\rLinear scaling: Double the servers, double the performance/data handling.\\rHadoop scales with the data.\\rScalability achieved through integration of computational techniques.\\rData science gaining traction rapidly.\\rIncreased application by major tech companies.\\rTechnology Impact\\rStandard databases and programming techniques.\\rRevolutionary impact on data processing and handling.\\rMajor impact on big data management.\\rCombines traditional statistics with machine learning advances.\\rGrowing importance in business and technology.\\rDeep learning technology expanding rapidly.\\rField Evolution\\rTraditional techniques in use for decades.\\rNew techniques developed to handle large datasets (e.g., Hadoop).\\rOriginated from Google\\'s innovation.\\rModern computational capabilities enhance traditional methods.\\rSignificant development in the last few years.\\rRapid development in neural network capabilities.\\rApplications\\rLimited by the capabilities of traditional systems.\\rEnables analysis of vast and complex data sets.\\rUsed by many companies for big data.\\rApplied in diverse areas from business to research.\\rEmerging as a critical field in analytics.\\rUsed extensively by tech giants.\\r\\r\\n\\r\\nFeature\\rAdam\\rSGD (Stochastic Gradient Descent)\\rFull Name\\rAdaptive Moment Estimation\\rStochastic Gradient Descent\\rLearning Rate\\rAdaptive (updates the learning rate for each parameter)\\rFixed or manually scheduled (can decay over time)\\rUpdate Rule\\rCombines momentum (first moment) and adaptive learning rates (second moment)\\rDirectly updates the weights using the gradient and learning rate\\rMomentum\\rImplicitly incorporates momentum through first and second moments (biased corrections)\\rRequires separate manual momentum term (optional)\\rConvergence Speed\\rGenerally faster due to adaptive learning rates\\rSlower, especially when learning rate is not well-tuned\\rMemory Usage\\rRequires more memory (stores first and second moments)\\rLess memory-intensive (only stores gradients)\\rUse Cases\\rWorks well for noisy and sparse data, often used in deep learning\\rSuitable for simpler or well-tuned tasks, traditional machine learning\\rHyperparameters\\rRequires tuning \\rMainly the learning rate (and momentum if used)\\rBias Correction\\rYes, bias-corrected estimates of first and second moments\\rNo bias correction\\rSensitivity to Learning Rate\\rLess sensitive to the initial learning rate (due to adaptivity)\\rHighly sensitive; requires careful tuning\\rApplicability\\rEffective for a wide range of tasks, especially deep neural networks\\rSuitable for smaller models and simpler optimization tasks\\rStrengths\\rAdaptive learning rates; works well for large datasets and models\\rSimple and efficient for convex optimization problems\\rWeaknesses\\rMore computationally intensive, may overfit on certain problems\\rProne to slow convergence or getting stuck in local minima\\rTypical Use\\rDeep learning tasks, especially CNNs, RNNs, NLP, etc.\\rBasic gradient-based optimization, small models\\r\\r\\n\\r\\n\\rProgramming Ocean Academy for Data Science and AI\\rThe factors contributing to the cost of running a Large Language Model (LLM)\\rCost Factor\\rDescription\\rModel Size\\rLarger models with more parameters require more memory (RAM/VRAM) and computational resources.\\rHardware Costs\\rGPUs or TPUs for training/inference are expensive; infrastructure includes servers, cooling, and power.\\rEnergy Consumption\\rHigh energy use during training and inference increases operational costs.\\rTraining Costs\\rLong training times, massive datasets, and hyperparameter tuning raise compute and energy costs.\\rInference Costs\\rQuery complexity, batch size, and low-latency requirements affect resource usage during real-time inference.\\rCloud Hosting vs. On-Premise\\rCloud costs include compute hours, storage, and scaling; on-premise costs cover infrastructure and maintenance.\\rSoftware & Licensing Costs\\rAdditional costs for optimization tools, distributed training, and API usage.\\rOperational & Maintenance Costs\\rIncludes model updating, retraining, and staffing costs for engineers and data scientists.\\rSecurity & Compliance\\rEncryption, secure storage, and compliance with data privacy regulations add to the operational overhead.\\rNetworking & Bandwidth\\rData transfer, especially across regions or clouds, and ensuring low-latency response times increase costs.\\r\\r\\nThe major four types in the Machine learning algorithm-Programming Ocean Academy\\rAspect\\rClassification\\rRegression\\rClustering\\rDimensionality Reduction\\rPurpose\\rCategorize data into discrete classes\\rPredict continuous values\\rGroup similar data points without labels\\rReduce the number of features while preserving the variance\\rKey Algorithms\\r\\n \\r\\n \\r\\n \\r- SVC\\r- Lasso\\r- KMeans\\r- PCA\\r\\r- KNeighbors\\r- ElasticNet\\r- Spectral Clustering\\r- Isomap\\r\\r- Naive Bayes\\r- Ridge\\r- GMM\\r- LLE\\r\\r- SGD Classifier\\r- SVR\\r \\r- Spectral Embedding\\rSample Size Considerations\\r\\n \\r- Works with both small (<100K) and large datasets\\r- For smaller datasets (<100K), suitable methods like SGD Regressor\\r- KMeans is good for small datasets (<10K)\\r- Works with smaller datasets (<10K) with algorithms like PCA, Isomap\\r\\r \\r- SVR for small and medium datasets\\r- MiniBatch KMeans for large datasets\\r \\rLabeled Data Requirement\\rRequires labeled data for training\\rRequires labeled data (features and target values)\\rDoes not require labeled data\\rTypically unsupervised; does not need labels\\rTypes of Predictions\\rPredict categories (e.g., spam or not spam)\\rPredict continuous quantities (e.g., house price)\\rFind groupings in data (e.g., customer segments)\\rTransform data to lower dimensions (e.g., visualize high-dimensional data)\\rCommon Challenges\\r\\n \\r- Choosing between linear and non-linear classifiers\\r- Overfitting with complex models\\r- Determining the optimal number of clusters\\r- Loss of information when reducing dimensions\\r\\r- Handling imbalanced data\\r- Feature importance\\r- Scalability with large data\\r- Choosing the correct number of components\\rKey Decision Factors\\r\\n \\r\\n \\r- Size of dataset\\r- Importance of features\\r- Number of clusters\\r- Number of features\\r\\r- Text data suitability\\r- Number of samples\\r- Size of dataset\\r- Variance preservation\\r\\r- Number of classes\\r- Whether features are correlated\\r- Number of known categories\\r- Computational efficiency\\rKey Use Cases\\r\\n \\r\\n \\r- Image or text classification\\r- Predicting stock prices\\r- Market segmentation\\r- Data visualization\\r\\r- Spam detection\\r- Forecasting sales\\r- Social network analysis\\r- Preprocessing for other algorithms\\r\\r- Disease diagnosis\\r- House price prediction\\r- Image segmentation\\r- Noise reduction\\r\\r\\n\\r\\nDimensionality Reduction techniques- Programming Ocean Academy\\rTechnique\\rDescription\\rUses\\rPros\\rCons\\rIdeal Scenarios\\rPrincipal Component Analysis (PCA)\\r\\n \\r\\n \\rA linear technique that projects data onto lower-dimensional spaces while maximizing variance.\\r\\n \\r\\n \\r- Data compression\\r\\n- Visualization\\r\\n- Noise reduction\\r- Simple to implement\\r\\n- Good for large datasets\\r\\n \\r- Assumes linearity\\r\\n- Can lose interpretability\\r\\n \\r- When the data has a large number of features with redundancy or correlation\\r\\n \\r\\n \\rLinear Discriminant Analysis (LDA)\\r\\n \\rProjects data onto lower dimensions while maximizing the separation between classes.\\r\\n \\r- Feature extraction for classification problems\\r\\n \\r- Improves class separation\\r\\n- Computationally efficient\\r- Only works for labeled data\\r\\n- Assumes Gaussian distribution\\r- When class separation is important for classification tasks\\r\\n \\rKernel PCA\\r\\n \\rNon-linear version of PCA using kernel methods to capture non-linear relationships.\\r\\n \\r- Non-linear feature reduction\\r\\n- Preprocessing for SVMs\\r- Captures complex relationships\\r\\n- Flexible for non-linear data\\r- Computationally expensive\\r\\n- Requires careful choice of kernels\\r- When the dataset has complex, non-linear relationships and patterns\\r\\n \\rt-Distributed Stochastic Neighbor Embedding (t-SNE)\\r\\n \\rNon-linear technique that preserves the local structure of data and is mainly used for visualization.\\r\\n \\r- Data visualization\\r\\n- Exploring high-dimensional data\\r- Excellent for visualizing complex, high-dimensional data\\r\\n \\r- Computationally expensive\\r\\n- Not suitable for large datasets\\r- For visualizing complex datasets in 2D or 3D, such as in exploratory data analysis\\r\\n \\rIsomap\\r\\n \\rA manifold learning technique that seeks to preserve the global structure of the data.\\r\\n \\r- Non-linear dimensionality reduction\\r\\n- Visualizing manifolds\\r- Captures global structure\\r\\n- Effective for manifold data\\r- Struggles with noise\\r\\n- Computationally expensive\\r- When the data lies on a low-dimensional manifold within a high-dimensional space\\r\\n \\rLocally Linear Embedding (LLE)\\r\\n \\rA manifold learning algorithm that preserves local neighborhoods in lower-dimensional projections.\\r\\n \\r- Non-linear dimensionality reduction\\r\\n- Visualization\\r- Preserves local structures\\r\\n- Effective for high-dimensional data\\r- Sensitive to noise\\r\\n- Does not preserve global structure\\r- When the goal is to preserve local relationships in high-dimensional data\\r\\n \\rFactor Analysis (FA)\\r\\n \\rA statistical method that models observed variables as linear combinations of potential latent factors.\\r\\n \\r- Feature extraction\\r\\n- Data exploration\\r- Simplifies data interpretation\\r\\n- Good for correlated variables\\r- Assumes linear relationships\\r\\n- Interpretability can be complex\\r- When the objective is to identify underlying factors explaining correlations between features\\r\\n \\rAutoencoders\\r\\n \\r\\n \\rNeural network-based technique that learns efficient representations of data.\\r\\n \\r\\n \\r- Feature learning\\r\\n- Image compression\\r\\n- Denoising\\r- Can model complex non-linear functions\\r\\n- Highly customizable\\r\\n \\r- Requires a lot of data and tuning\\r\\n- Computationally expensive\\r\\n \\r- When working with large datasets and non-linear data, especially in deep learning contexts\\r\\n \\r\\n \\rIndependent Component Analysis (ICA)\\r\\n \\rA technique that separates a multivariate signal into additive, independent components.\\r\\n \\r- Blind source separation\\r\\n- Signal processing\\r- Finds independent features\\r\\n- Good for non-Gaussian data\\r- Assumes statistical independence\\r\\n- Sensitive to noise\\r- When the task is to extract independent signals or sources, such as in audio processing\\r\\n \\rMultidimensional Scaling (MDS)\\r\\n \\rA non-linear technique that projects data into lower dimensions by preserving pairwise distances.\\r\\n \\r- Visualizing similarity or dissimilarity between data points\\r\\n \\r- Preserves distances\\r\\n- Effective for dissimilarity data\\r- Computationally expensive\\r\\n- Limited scalability\\r- When preserving the distance between data points is important, such as in perceptual mapping\\r\\n \\rRandomized PCA\\r\\n \\rA faster approximation of PCA for large datasets that uses random projections.\\r\\n \\r- Fast data compression\\r\\n- Initial feature reduction\\r- Efficient for very large datasets\\r\\n- Retains most variance\\r- Only an approximation of true PCA\\r\\n- Loss of precision\\r- When computational resources are limited and the dataset is extremely large\\r\\n \\rTruncated Singular Value Decomposition (SVD)\\r\\n \\r\\n \\rA matrix factorization technique used for dimensionality reduction, particularly in sparse data.\\r\\n \\r\\n \\r- Sparse data\\r\\n- Text analysis\\r\\n- Matrix factorization\\r- Handles sparse data well\\r\\n- Useful for text data\\r\\n \\r- Loses interpretability\\r\\n- Assumes linear relationships\\r\\n \\r- When dealing with sparse data, such as text (e.g., TF-IDF matrices)\\r\\n \\r\\n \\r\\r\\n\\r\\nClustering Algorithms Table- Programming Ocean Academy\\rAlgorithm\\rUse Case\\rKMeans\\rGeneral-purpose clustering for datasets where clusters are spherical in shape.\\rMiniBatch KMeans\\rFor large-scale datasets with performance constraints, a faster version of KMeans.\\rGaussian Mixture (GMM)\\rWhen clusters have varying sizes and elliptical shapes, and you want probabilistic cluster membership.\\rSpectral Clustering\\rUseful for non-convex clusters, e.g., graph-based clustering or when clusters are not easily separable.\\rMeanShift\\rFinds the center of clusters, good for data with an unknown number of clusters and arbitrarily shaped clusters.\\rAgglomerative Clustering\\rHierarchical clustering, effective for small datasets where relationships between data points are important.\\rDBSCAN\\rBest for data with noise, capable of finding clusters of varying shapes and sizes without specifying the number of clusters.\\rBirch\\rEfficient clustering method for large datasets, works well with data that is incrementally added.\\rAffinity Propagation\\rAutomatically determines the number of clusters based on message-passing between data points.\\rSpectral Embedding\\rDimensionality reduction method that can be used before clustering algorithms for better performance on structured data.\\rVBGMM (Variational Bayesian Gaussian Mixture Model)\\rProbabilistic clustering method where the number of clusters is not fixed; suitable for uncertain data.\\r\\r\\nA comprehensive comparison between Data Science, Data Analysis, and Data Engineering- Programming Ocean Academy\\rAspect\\rData Science\\rData Analysis\\rData Engineering\\rDefinition\\rThe field focused on extracting meaningful insights and predictions from large datasets using advanced statistical, machine learning, and AI techniques.\\rThe process of inspecting, cleaning, transforming, and modeling data to discover useful information and support decision-making.\\rThe practice of building and maintaining the infrastructure (pipelines, databases, etc.) that enables the collection, storage, and processing of large datasets for analysis and machine learning.\\rKey Responsibilities\\r\\n \\r\\n \\r\\n \\r\\n \\r- Develop machine learning models and algorithms.\\r\\n- Perform predictive analytics and AI model training.\\r\\n- Perform experimental design for A/B testing.\\r\\n- Extract actionable insights for business or research.\\r\\n- Communicate findings to stakeholders.\\r- Query, clean, and visualize datasets.\\r\\n- Conduct descriptive analysis of current and historical trends.\\r\\n- Build dashboards and reports.\\r\\n- Identify patterns and relationships in data.\\r\\n- Help decision-makers understand the implications of data.\\r- Design and build data pipelines.\\r\\n- Manage data storage systems (e.g., data warehouses, lakes).\\r\\n- Ensure data quality and availability.\\r\\n- Implement ETL (Extract, Transform, Load) processes.\\r\\n- Support data analysts and data scientists with clean and usable datasets.\\rGoal\\rGenerate predictions and insights using data-driven models, solving complex business problems with advanced analytics, and creating innovative solutions.\\rHelp businesses or researchers understand current data trends, identify key insights, and guide decisions with clear, accurate reports based on data.\\rEnsure that data is available, accessible, and organized properly, enabling smooth analysis and data science processes. Provide a solid infrastructure for handling large-scale datasets.\\rSkills Required\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r- Statistical analysis\\r\\n- Machine learning & AI\\r\\n- Programming (Python, R, SQL)\\r\\n- Data wrangling\\r\\n- Advanced mathematics\\r\\n- Data visualization\\r\\n- Knowledge of cloud platforms (AWS, Azure, GCP)\\r\\n- Model evaluation & optimization techniques\\r- Strong analytical skills\\r\\n- Data cleaning & transformation\\r\\n- Data visualization (e.g., Tableau, Power BI, Excel)\\r\\n- SQL querying\\r\\n- Basic statistical knowledge\\r\\n- Communication and reporting skills\\r\\n \\r\\n \\r- Programming (Python, Java, Scala, SQL)\\r\\n- Database design and management\\r\\n- ETL tools (e.g., Apache Airflow, Apache NiFi)\\r\\n- Cloud data storage (e.g., AWS Redshift, Google BigQuery)\\r\\n- Big data tools (Hadoop, Spark)\\r\\n- Data pipeline automation and orchestration\\r\\n \\r\\n \\rCommon Tools\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r- Jupyter Notebook\\r\\n- Python (pandas, NumPy, scikit-learn, TensorFlow, PyTorch)\\r\\n- R\\r\\n- SQL\\r\\n- Hadoop, Spark\\r\\n- Visualization tools (Matplotlib, Seaborn, Plotly, etc.)\\r\\n- Cloud computing platforms\\r\\n- GitHub, Docker\\r- Excel\\r\\n- SQL\\r\\n- Tableau, Power BI\\r\\n- Python (pandas, Matplotlib)\\r\\n- Google Analytics\\r\\n- Business Intelligence (BI) tools\\r\\n- SAS\\r\\n- R\\r- SQL databases (PostgreSQL, MySQL, Oracle)\\r\\n- NoSQL databases (MongoDB, Cassandra)\\r\\n- ETL tools (Apache Airflow, Talend)\\r\\n- Apache Hadoop, Apache Spark\\r\\n- Cloud platforms (AWS, Google Cloud, Azure)\\r\\n- Data orchestration tools (Kubernetes, Docker)\\r\\n \\r\\n \\rOutput\\rPredictive models, AI-driven insights, recommendations, optimized algorithms for decision-making, automation systems, or research findings.\\rReports, dashboards, KPIs, descriptive insights, and visual representations of data to guide business decisions.\\rOptimized, structured, and processed datasets, data pipelines, data lakes, and data warehouses ready for analysis or modeling.\\rEducational Background\\r\\n \\r\\n \\r- Advanced degrees (Master\\'s or PhD) in Computer Science, Statistics, Mathematics, or related fields.\\r\\n- Specialized certifications in data science and machine learning.\\r\\n- Strong theoretical knowledge in AI and ML concepts.\\r- Bachelor\\'s degree in statistics, economics, or a business-related field.\\r\\n- Certifications in analytics tools (Tableau, Power BI).\\r\\n- Basic understanding of statistical techniques and tools.\\r- Bachelor\\'s or Master\\'s degree in Computer Science, Engineering, or related fields.\\r\\n- Certifications in big data technologies, cloud data management (AWS, Google Cloud, etc.), and ETL systems.\\r\\n- Strong software engineering background.\\rMain Challenges\\r\\n \\r\\n \\r\\n \\r\\n \\r- Dealing with unstructured and large datasets.\\r\\n- Model overfitting/underfitting.\\r\\n- Interpreting complex algorithms.\\r\\n- Keeping up with fast-evolving AI and ML technologies.\\r\\n- Communication of complex models to non-technical stakeholders.\\r- Data quality issues (incomplete or inaccurate data).\\r\\n- Visualizing complex data in understandable ways.\\r\\n- Avoiding bias in data interpretation.\\r\\n- Ensuring that reports align with business objectives.\\r\\n- Managing diverse data sources.\\r- Managing big data storage and retrieval at scale.\\r\\n- Ensuring data security and integrity.\\r\\n- Dealing with data silos and complex data architectures.\\r\\n- Real-time data processing and ensuring low latency in large-scale systems.\\r\\n \\rCareer Path\\rData Scientist, AI/ML Engineer, Research Scientist, Business Intelligence Architect.\\rData Analyst, Business Analyst, BI Analyst, Operations Analyst.\\rData Engineer, Data Architect, Database Administrator, ETL Developer, Big Data Engineer.\\rAverage Salary\\r$120,000 - $160,000 per year (US).\\r$60,000 - $90,000 per year (US).\\r$110,000 - $150,000 per year (US).\\rIndustry Demand\\rHigh demand due to AI and automation trends. Especially sought after in tech, finance, healthcare, and research fields.\\rStrong demand in industries requiring reporting, trend analysis, business insights, e-commerce, and finance.\\rHigh demand in industries that rely on big data processing, cloud infrastructure, and data-driven decision-making, including tech, finance, logistics, and retail.\\r\\r\\n\\r\\n\\r\\nA comparison between Data Analysis, Data Visualization, and Data Modeling- Programming Ocean Academy\\rAspect\\rData Analysis\\rData Visualization\\rData Modeling\\rDefinition\\rThe process of inspecting, cleaning, transforming, and interpreting data to discover patterns, trends, and insights to support decision-making.\\rThe graphical representation of data using visual elements like charts, graphs, and maps to help stakeholders easily understand complex datasets.\\rThe process of defining and organizing the structure of a dataset, typically by creating relationships between different data entities to represent business processes.\\rKey Responsibilities\\r- Analyze data to find trends, correlations, and anomalies.\\r\\n- Perform statistical analysis and hypothesis testing.\\r\\n- Identify actionable insights.\\r- Design and create visual representations of data.\\r\\n- Transform raw data into meaningful charts and graphs.\\r\\n- Present data clearly and effectively to stakeholders.\\r- Design the structure of data systems (relational, hierarchical, or network models).\\r\\n- Define data entities, attributes, and relationships.\\r\\n- Optimize databases for usage.\\rGoal\\rExtract meaningful insights from raw data to inform decision-making and help businesses make data-driven decisions.\\rConvey insights, trends, and patterns through visual elements to facilitate better understanding and quicker decision-making by non-technical stakeholders.\\rCreate a structured framework for how data is stored, accessed, and used within systems to ensure consistency, accuracy, and scalability.\\rSkills Required\\r- Statistical knowledge (hypothesis testing, regression, etc.).\\r\\n- Data cleaning and preprocessing.\\r\\n- Proficiency in querying (SQL).\\r\\n- Python/R for analysis.\\r- Knowledge of design principles (color, layout, clarity).\\r\\n- Proficiency in visualization tools.\\r\\n- Understanding of different chart types and when to use them.\\r- Database design (ER diagrams, schema design).\\r\\n- Knowledge of SQL, NoSQL.\\r\\n- Data normalization and optimization techniques.\\r\\n- Entity-relationship modeling (ERMs).\\rCommon Tools\\r- Excel\\r\\n- SQL\\r\\n- Python (pandas, NumPy)\\r\\n- R\\r\\n- SAS\\r\\n- Google Analytics\\r- Tableau\\r\\n- Power BI\\r\\n- Matplotlib, Seaborn (Python)\\r\\n- Excel\\r\\n- Google Data Studio\\r- SQL databases (PostgreSQL, MySQL)\\r\\n- ER diagram tools (Lucidchart, draw.io)\\r\\n- Data modeling tools (Erwin, IBM InfoSphere Data Architect)\\r\\n- NoSQL databases (MongoDB).\\rOutput\\rStatistical reports, data summaries, trend analyses, dashboards, and key performance indicators (KPIs).\\rCharts, graphs, dashboards, infographics, and interactive visuals that clearly display key metrics, patterns, and trends.\\rEntity-relationship diagrams (ERD), schema definitions, database tables, and a structured data model for further analysis or system usage.\\rFocus\\rUnderstanding the \"what\" and \"why\" behind the data, performing both descriptive and inferential analysis.\\rSimplifying complex data and presenting it visually to communicate insights in a more digestible format to a broad audience.\\rStructuring the \"how\" data will be stored, organized, and related, creating a clear blueprint for efficient and accurate data management and usage.\\rEnd Users\\rData analysts, decision-makers, business strategists, and researchers who need to make informed decisions based on historical and real-time data.\\rBusiness executives, product managers, marketing teams, and non-technical stakeholders who need to quickly grasp trends, insights, and data-driven stories.\\rData engineers, database administrators, developers, and architects who implement and maintain the data structure in databases or data warehouses.\\rOutput Examples\\r- Descriptive statistics (mean, median, mode).\\r\\n- Correlation analysis.\\r\\n- Trend analysis over time.\\r\\n- Predictive models (e.g., forecasting future trends).\\r- Line charts for trends.\\r\\n- Bar charts for comparisons.\\r\\n- Pie charts for proportions.\\r\\n- Heatmaps for density.\\r\\n- Interactive dashboards for real-time data.\\r- ER diagrams to represent relationships.\\r\\n- Database schema definition (tables, columns).\\r\\n- Logical and physical data models.\\r\\n- Data normalization schemas.\\rMain Challenges\\r- Dealing with incomplete or messy datasets.\\r\\n- Misinterpretation of statistical results.\\r\\n- Ensuring that insights are actionable and relevant to the business goals.\\r- Choosing the right type of visualization for the data.\\r\\n- Ensuring that visualizations are not misleading.\\r\\n- Simplifying complex data without losing key insights.\\r- Ensuring that the data model is scalable, consistent, and supports all business operations.\\r\\n- Handling large, complex datasets while optimizing performance.\\rCareer Path\\rData Analyst, Business Analyst, Financial Analyst, Operations Analyst.\\rData Visualization Specialist, BI Analyst, Dashboard Designer, UX/UI Data Visualization Designer.\\rData Modeler, Database Architect, Data Engineer, Data Warehouse Architect.\\rEducational Background\\rTypically a Bachelor\\'s in Statistics, Computer Science, Economics, or Business, often supplemented by data analysis certifications.\\rA background in Data Analytics, Computer Science, Information Design, or Graphic Design, with an emphasis on communication and data visualization tools.\\rDegrees in Computer Science, Information Systems, or Database Management, with specializations in database design and data architecture.\\rIndustry Demand\\rStrong demand across industries that need actionable insights from data to improve operations, marketing, and finance.\\rGrowing demand, especially in businesses that require intuitive data dashboards and visuals to inform decision-making in areas like marketing, sales, and product development.\\rHigh demand in industries with complex data architectures and systems, such as tech, finance, logistics, and e-commerce, where data management is critical to operations.\\r\\r\\n\\r\\n\\r\\nA Convolutional Neural Network (CNN)- Programming Ocean Academy\\rStage\\rDescription\\rInput Image\\rThe network starts with an input image, and the goal is to classify or identify objects or patterns within the image.\\rConvolution\\rA small filter (called a kernel) moves over the image, looking at small sections at a time. It detects basic patterns like edges, textures, or color gradients.\\rFeature Map\\rThe output of the convolution operation is a feature map, which highlights important features from the image in a simplified, spatially structured format.\\rPooling\\rPooling reduces the size of the feature map by summarizing the most important information, helping the model become more efficient and less sensitive to small variations.\\rPooled Feature Map\\rThe result after pooling is a down-sampled version of the feature map that retains the essential characteristics of the image.\\rConvolution & Pooling (again)\\rThese steps are repeated multiple times to extract more complex and higher-level features from the image.\\rFlatten Layer\\rThe pooled feature maps are flattened into a one-dimensional vector, which can be passed into the next layers for further processing or classification.\\rFully Connected Layer\\rThe flattened feature vector is connected to a fully connected layer, where all features are combined and analyzed to make a final prediction.\\rOutput\\rThe network outputs a set of probabilities for each possible class. The class with the highest probability is selected as the model\\'s prediction.\\r\\r\\n\\r\\nAspect\\rConvolutional Layers\\rFlatten Layers\\rFully Connected Layers\\rDefinition\\rPerforms a convolution operation by sliding filters (kernels) over the input to extract features.\\rConverts multi-dimensional arrays (e.g., 2D or 3D) into a 1D vector.\\rA traditional neural network layer where every neuron is connected to every neuron in the previous layer.\\rPrimary Function\\rExtracts local features like edges, textures, and patterns from the input data, preserving spatial relationships.\\rPrepares multi-dimensional data (e.g., feature maps from convolutional layers) for input into fully connected layers.\\rCombines all features to make final predictions or decisions, performing global feature aggregation.\\rInput Shape\\rMulti-dimensional input (e.g., 2D for images: HÃ—WH \\\\times WHÃ—W, 3D with channels: HÃ—WÃ—CH \\\\times W \\\\times CHÃ—WÃ—C).\\rMulti-dimensional input (2D, 3D, etc.), flattened into a 1D vector.\\r1D vector as input (output from the Flatten layer or a previous fully connected layer).\\rOutput Shape\\rMulti-dimensional output (reduced height and width after convolution, but retains multiple channels).\\r1D vector (flattened representation of the input).\\r1D vector of neuron activations, size depends on the number of neurons in the layer.\\rUse Case\\rFeature extraction, especially in image and video processing tasks.\\rInterface between convolutional layers and fully connected layers.\\rFinal classification, regression, or prediction tasks in deep learning models.\\rParameters\\rSmall number of parameters (weights are shared across the input using kernels).\\rNo parameters-just reshapes the data.\\rLarge number of parameters, as each neuron connects to every input from the previous layer (dense connections).\\rSpatial Information\\rPreserves spatial relationships (position and structure) in the input data.\\rDiscards spatial information and reshapes data into a flat array.\\rDoes not preserve spatial relationships, instead focuses on global feature representation.\\rCommon Applications\\r\\n \\r- CNNs for image classification and object detection.\\r\\n- NLP with text representations (e.g., 1D convolutions).\\rTransitioning from feature maps (from CNNs) to fully connected layers for final classification.\\r\\n \\r- Final prediction in CNNs.\\r\\n- Used in classical neural networks for classification or regression.\\rRole in the Network\\rDetects low-level to high-level features hierarchically by applying multiple filters.\\rPrepares the extracted features for classification or prediction by reshaping them.\\rCombines all features and performs the final decision-making (output layer in classification/regression).\\rParameter Sharing\\rYes, weights are shared across the input space to reduce the number of parameters.\\rNo sharing simply reshapes data.\\rNo parameter sharing; each neuron has its own set of weights, leading to a large number of parameters.\\rComputation Cost\\rGenerally lower compared to fully connected layers due to weight sharing, but can be high for deep layers.\\rVery low, as it only reshapes the input data.\\rHigh computational cost due to dense connections (each neuron connected to all inputs).\\rExample\\r3x3 convolution filter applied to an image to detect edges.\\rFlattening a 32Ã—32Ã—3 times feature map into a vector of size 3072.\\rFully connected layer with 1000 neurons receiving input from a flattened vector.\\r \\r\\n\\r\\nDense (Fully Connected) Layers and Convolutional Layers - Programming Ocean Academy\\rAspect\\rDense (Fully Connected) Layers\\rConvolutional Layers\\rStructure\\rEvery neuron is connected to every other neuron in the next layer.\\rOnly a subset of neurons (kernel or filter) is connected to the input data, sliding over it spatially.\\rInput Shape\\rFlattened vector (e.g., a 1D array of pixel values from an image).\\r2D or 3D data (e.g., images with height, width, and channels).\\rWeights\\rEach connection has its own weight, leading to a large number of parameters.\\rShared weights within a kernel/filter across spatial locations.\\rNumber of Parameters\\rLarger number of parameters as each input is connected to every output.\\rFewer parameters due to weight sharing (e.g., a 3x3 filter for an entire image).\\rPrimary Function\\rLearns global features by treating the entire input as a whole.\\rLearns local features like edges, textures, or patterns in small regions.\\rDimensionality\\rTypically used for 1D input/output (after flattening).\\rWorks with 2D (e.g., image data) or 3D (e.g., video data).\\rActivation Functions\\rUsually followed by a non-linear activation like ReLU or sigmoid.\\rOften followed by an activation function like ReLU, and pooling layers for downsampling.\\rUse Cases\\rUsed in final classification or regression tasks.\\rUsed for feature extraction, especially with spatial data like images or video.\\rInterpretability\\rHarder to interpret; no clear link between input space and learned features.\\rMore interpretable; filters can reveal patterns like edges or textures learned from the input.\\rComputational Efficiency\\rLess efficient due to large number of parameters and lack of spatial awareness.\\rMore efficient due to local connections and weight sharing, which reduces computational cost.\\rData Requirements\\rRequires flattened data (2D images need to be reshaped to 1D vectors).\\rDirectly works with structured input like images, preserving spatial relationships.\\rExample Usage\\rOften used in final layers of a model to output probabilities or scores.\\rUsed in early layers for extracting spatial features from images or signals.\\r\\r\\nConvolutional Layer, Pooling Layer, Flatten Layer, and Dense Layer - Programming Ocean Academy\\rFeature\\rConvolutional Layer (Conv2D)\\rPooling Layer (MaxPooling2D)\\rFlatten Layer\\rDense Layer (Fully Connected Layer)\\rPurpose\\rExtracts features from the input by using filters (kernels) to scan and detect patterns like edges and textures.\\rReduces the spatial dimensions by downsampling, improving computational efficiency and reducing overfitting.\\rConverts a multidimensional tensor into a 1D vector to prepare it for fully connected (dense) layers.\\rProcesses extracted features to classify inputs or make predictions, where each neuron connects to all neurons in the previous and next layers.\\rInput/Output Shape\\rInput: 3D (height, width, channels)\\rInput: 3D (height, width, channels)\\rInput: 3D (height, width, channels)\\rInput: 1D vector (from flatten layer)\\r \\rOutput: Reduced 3D (height, width, filters)\\rOutput: Further reduced 3D (height, width, channels)\\rOutput: 1D vector\\rOutput: 1D vector (number of neurons in the dense layer)\\rKey Operations\\rApplies filters that convolve across the input. Uses strides, padding, and activation functions (e.g., ReLU).\\rPerforms max-pooling or average-pooling to summarize information within regions (e.g., 2x2 pooling).\\rFlattens the output of the previous layer by collapsing spatial dimensions into a single vector.\\rApplies a weighted sum of inputs and biases, followed by an activation function (e.g., ReLU, softmax) to make predictions.\\rParameters\\rNumber of filters, filter size (kernel), strides, activation function, padding\\rNo trainable parameters (just reduces input dimensions).\\rNo trainable parameters (reshapes data).\\rNumber of neurons, activation function, weights, and biases\\rCommon Use Cases\\rFeature extraction from images, audio, or spatial data, commonly used in CNNs for tasks like image recognition.\\rTypically used after convolutional layers to reduce spatial dimensions while preserving important features.\\rActs as a bridge between convolutional/pooling layers and dense layers, preparing the data for final classification or regression tasks.\\rFinal decision-making layer(s) of a model, typically used after flattening in CNNs. Responsible for classification or regression.\\r\\r\\nComparison of Different CNN Architectures and Their Use Cases Part One- Programming Ocean Academy\\rType\\rDescription\\rUse Case\\rExample\\rStandard CNN (Sequential CNN)\\rBasic form of CNN where layers are stacked in sequence. Includes Convolutional, Pooling, Flatten, and Dense Layers.\\rImage classification, object detection\\rLeNet-5 for digit classification\\rFully Convolutional Networks (FCN)\\rReplaces fully connected layers with convolutional layers to handle varying input sizes. Outputs pixel-wise predictions.\\rImage segmentation, semantic segmentation\\rFCN for semantic segmentation\\rDeep CNN (Very Deep CNN)\\rNetworks with many layers (dozens or hundreds) to capture complex patterns. Improves accuracy but requires more computational power.\\rComplex tasks like image recognition, object detection\\rVGGNet, ResNet (up to 152 layers)\\rRecurrent CNN (RCNN)\\rCombines CNNs with RNNs for handling sequential data like video frames. CNN for feature extraction, RNN for temporal dependencies.\\rVideo classification, action recognition\\rRCNN for video analysis\\rMultistream CNN\\rMultiple CNN streams process inputs from different sources, such as RGB and depth images in parallel, improving accuracy.\\rMultimodal data processing, combining inputs from different sensors\\rTwo-stream CNN for action recognition\\r\\r\\nComparison of Different CNN Architectures and Their Use Cases Part Two- Programming Ocean Academy\\rType\\rDescription\\rUse Case\\rExample\\rResidual Networks (ResNet)\\rUses residual blocks to solve the vanishing gradient problem in deep networks. Adds shortcuts (residual connections) to improve training efficiency.\\rImage classification, object detection\\rResNet-50, ResNet-152 for large-scale tasks\\rDilated (Atrous) CNN\\rUses dilated convolutions to expand the receptive field without losing resolution, capturing global context in images.\\rSemantic segmentation, speech analysis\\rDeepLab for semantic segmentation\\rRegion-Based CNN (R-CNN)\\rCombines CNNs with region proposal methods for object detection. More advanced versions like Fast and Faster R-CNN improve speed and accuracy.\\rObject detection, image segmentation\\rR-CNN, Fast R-CNN, Faster R-CNN\\rU-Net\\rEncoder-decoder architecture used for segmentation. Contracting path captures context, expanding path enables precise localization.\\rBiomedical image segmentation, medical imaging\\rU-Net for medical image segmentation\\rCapsule Networks (CapsNet)\\rUses capsules (groups of neurons) to store positional information and relationships between features, preserving spatial hierarchies.\\rImage classification, object detection, spatial relationships\\rCapsule Network for recognizing overlapping digits\\r\\r\\nTable that captures the entire CNN process from the image input to the final output-Part One - Programming Ocean Academy\\rStep\\rLayer Type\\rProcess Description\\rOutput\\r1. Input Image\\rInput Layer\\rThe image (e.g., of a character like Tweety) is fed into the network. The input has a certain width, height, and depth (e.g., 32x32x3 for a color image with 3 RGB channels).\\rRaw image data of size (e.g., 32x32x3).\\r2. Convolution\\rConvolutional Layer\\rFilters (kernels) slide over the image, detecting simple patterns like edges and corners. Each filter produces a feature map by applying convolution over the image.\\rFeature maps of size (e.g., 30x30x32) where 32 is the number of filters and 30x30 is the reduced spatial dimension (due to no padding).\\r3. Feature Maps\\rOutput of Convolution\\rThe output of the convolutional layer is multiple feature maps that represent different patterns detected from the input image.\\rFeature maps showing edges, corners, and basic features extracted from the image.\\r4. Pooling\\rPooling Layer\\rPooling layers (usually MaxPooling) reduce the spatial dimensions (width and height) of the feature maps while keeping the depth (number of filters) intact.\\rPooled feature maps of smaller size (e.g., 15x15x32), where the spatial dimensions have been halved but the depth remains the same.\\r5. Second Convolution\\rConvolutional Layer\\rAnother set of filters is applied, detecting more complex features by sliding over the pooled feature maps. These filters detect patterns of higher complexity (e.g., shapes).\\rNew feature maps with more complex patterns detected (e.g., 13x13x64, where 64 is the number of filters).\\r\\r\\nTable that captures the entire CNN process from the image input to the final output-Part Two - Programming Ocean Academy\\rStep\\rLayer Type\\rProcess Description\\rOutput\\r6. Second Pooling\\rPooling Layer\\rAnother pooling layer reduces the spatial dimensions further, simplifying the data while retaining essential features.\\rSmaller pooled feature maps (e.g., 6x6x64).\\r7. Third Convolution\\rConvolutional Layer\\rA deeper convolution layer with more filters is applied, detecting even more abstract patterns, refining the learned features.\\rMore complex feature maps (e.g., 4x4x64) representing more detailed patterns.\\r8. Flattening\\rFlatten Layer\\rThe 3D feature maps are flattened into a 1D vector so that they can be used by the fully connected (Dense) layers for classification.\\rA flattened vector (e.g., size 1024) representing all learned features from the previous layers.\\r9. Fully Connected Layer\\rDense Layer\\rThe flattened features are passed through a fully connected (dense) layer where each node is connected to every feature. These connections have weights and biases.\\rA vector of activations (e.g., size 64), where each node\\'s activation depends on the learned weights and biases.\\r10. Output Layer (Classification)\\rDense Layer\\rThe final dense layer produces a probabilistic output for each class using an activation function like softmax. Each output represents the probability of the image belonging to a specific class.\\rA probabilistic distribution for classification (e.g., 0.2 for Donald, 0.1 for Goofy, 0.7 for Tweety, indicating Tweety as the highest probability).\\r11. Final Prediction\\rOutput\\rThe highest probability is selected as the predicted class (e.g., Tweety), based on the probabilistic distribution produced in the final layer.\\rPredicted class for the input image (e.g., Tweety).\\r\\r\\n\\r\\n\\r\\nThe receptive field VS kernel (or filter) - Programming Ocean Academy\\rConcept\\rReceptive Field\\rKernel/Filter\\rDefinition\\rThe portion of the input image that a specific neuron in a layer \"sees\" or is influenced by. It grows as you move deeper into the network.\\rA small matrix (usually 3x3 or 5x5) used to slide over the input image and compute features like edges, textures, or patterns.\\rScope\\rRefers to the area of the input image contributing to a neuron\\'s activation, and it expands as the CNN layers stack.\\rA set of weights used in convolution to extract specific features from the input.\\rSize\\rGrows as layers are added. For example, after several layers of convolutions, a neuron may \"see\" a larger part of the original image.\\rFixed size (e.g., 3x3 or 5x5). Determines how many pixels in the input are processed at each convolution step.\\rRole\\rHelps capture increasingly larger patterns or structures in the input as you go deeper into the network.\\rExtracts local patterns (like edges, textures) from the input by computing weighted sums in small regions.\\rInteraction\\rDetermined by how many layers the image has passed through. Each convolution layer with filters adds to the size of the receptive field.\\rApplies locally across the input to compute feature maps. The number of filters determines the depth of the feature maps.\\rCalculation\\rDependent on the kernel size, stride, padding, and number of layers in the network. A neuron in deeper layers has a larger receptive field.\\rThe kernel slides over the input image, applying weights at each position to compute feature maps at that layer.\\r\\r\\n\\r\\nA comparison Between Flattening Matrices and Transforming them into Higher Dimensions - Programming Ocean Academy\\rAspect\\rFlattening Matrices\\rTransforming Matrices into Higher Dimensions\\rDefinition\\rFlattening refers to converting a matrix (or higher-dimensional tensor) into a single vector by rearranging its elements into one continuous sequence.\\rTransforming into higher dimensions means converting data into a more complex space with additional features or dimensions, often through techniques like kernel functions or feature extraction.\\rPurpose\\rSimplifies data for algorithms that require a 1D input (e.g., feeding image data into a neural network).\\rAllows algorithms (like SVM) to separate data more effectively by mapping it to a higher-dimensional space.\\rResulting Shape\\rConverts a matrix of shape (m , n) into a vector of shape (m , n).\\rConverts an input of nnn-dimensions into an input of more than nnn-dimensions (e.g., mapping 2D data into 3D or higher).\\rUse Case\\r- Image processing: flattening 2D images for input into a fully connected neural network.\\r\\n- Simplifying input for models that expect vectors.\\r- Used in non-linear machine learning models (e.g., SVM with kernel trick) to make data linearly separable.\\r\\n- Feature engineering or dimensionality increase in deep learning.\\rImpact on Data\\rThe structure of the matrix is lost since it is now a 1D vector, but all the values are preserved.\\rThe data is enriched by adding new dimensions, allowing for more complex relationships to be learned.\\rCommon Techniques\\rReshape function in libraries like NumPy or TensorFlow (e.g., reshape((-1,))).\\rKernel transformations (e.g., polynomial kernel, RBF kernel), feature expansion, neural network layers (embedding layers).\\rComputational Complexity\\rGenerally straightforward and computationally cheap (just a rearrangement of elements).\\rMay increase the computational cost significantly due to the added complexity of more dimensions (higher-dimensional operations).\\rLoss of Information\\rNo loss of information occurs; all elements are retained in the vector. However, the spatial relationship of the matrix elements may be lost.\\rNo loss of information occurs. Instead, it often adds more information by embedding data into a higher-dimensional space.\\rExamples\\r- Flattening a 28Ã—28 image into a 784-length vector.\\r\\n- Flattening tensors for fully connected layers in neural networks.\\r- Mapping 2D input data into a 3D space using polynomial or RBF kernels in SVM.\\r\\n- Convolution layers in CNNs can map 2D data into higher-dimensional feature spaces.\\r\\r\\n\\r\\nHow SVM calculates the similarity between vectors using different approaches, including linear and non-linear kernels- Programming Ocean Academy\\rType of Kernel\\rMathematical Formula\\rExplanation\\rLinear Kernel\\r\\n \\r\\n \\rk=(X_(i  ,  )  X_(j  )= X_(i  )*   X_j)\\r- The dot product of two vectors.\\r\\n- Measures the similarity based on the angle between the vectors.\\r\\n- High similarity if vectors point in the same direction; low or negative if orthogonal or opposite.\\rPolynomial Kernel\\r\\n \\r\\n \\rK(xi,xj)=(xiÂ·xj+c)dK(\\\\mathbf{x}_i, \\\\mathbf{x}_j) = (\\\\mathbf{x}_i \\\\cdot \\\\mathbf{x}_j + c)^dK(xi?,xj?)=(xi?Â·xj?+c)d\\r\\n \\r\\n \\r- Raises the dot product to a power ddd and adds a constant ccc.\\r\\n- Allows SVM to fit more complex decision boundaries.\\r\\n- Higher degree ddd increases flexibility of the model.\\rRadial Basis Function (RBF) Kernel / Gaussian Kernel\\r\\n \\r\\n \\rK(xi,xj)=exp?(-?xi-xj?22s2)K(\\\\mathbf{x}_i, \\\\mathbf{x}_j) = \\\\exp\\\\left(-\\\\frac{\\\\|\\\\mathbf{x}_i - \\\\mathbf{x}_j\\\\|^2}{2\\\\sigma^2}\\\\right)K(xi?,xj?)=exp(-2s2?xi?-xj??2?)\\r\\n \\r\\n \\r- Measures similarity based on the Euclidean distance between vectors.\\r\\n- High similarity for vectors close in space; low similarity for distant vectors.\\r\\n- s\\\\sigmas controls the spread of the Gaussian curve.\\rSigmoid Kernel\\r\\n \\r\\n \\rK(xi,xj)=tanh?(a(xiÂ·xj)+c)K(\\\\mathbf{x}_i, \\\\mathbf{x}_j) = \\\\tanh(\\\\alpha (\\\\mathbf{x}_i \\\\cdot \\\\mathbf{x}_j) + c)K(xi?,xj?)=tanh(a(xi?Â·xj?)+c)\\r\\n \\r\\n \\r- Similar to the activation function in neural networks.\\r\\n- Uses the tanh function to compute similarity.\\r\\n- Parameters a\\\\alphaa and ccc control the shape of the similarity function.\\r\\r\\n\\r\\n\\r\\nA comprehensive list of feature types used in AI and machine learning - Part One - Programming Ocean Academy\\rFeature Type\\rDefinition\\rExample\\rApplication\\rSpatial Features\\rCaptures positional or locational data\\rLocation of edges in images\\rImage classification, object detection\\rGlobal Features\\rSummarizes overall structure of data\\rAverage pixel intensity\\rScene recognition, sentiment analysis\\rLocal Features\\rDescribes characteristics of smaller regions\\rPixel patch representing a corner\\rFace recognition, texture analysis\\rTemporal Features\\rCaptures time-based changes\\rStock prices over time\\rVideo analysis, speech recognition\\rFrequency Features\\rBased on frequency domain\\rFourier coefficients\\rAudio processing, sensor data\\rContextual Features\\rCaptures surrounding environment or context\\rWord meaning from surrounding words\\rNLP, recommendation systems\\rStructural Features\\rDescribes underlying structure or relationships\\rConnections in social network graph\\rGraph analysis, chemical modeling\\rSemantic Features\\rCarries conceptual meaning from data\\rWord embeddings like BERT\\rNLP, machine translation\\rStatistical Features\\rDerived from statistical properties\\rMean, variance\\rAnomaly detection, feature engineering\\rHierarchical Features\\rCaptures patterns at different abstraction levels\\rEdges in lower CNN layers, objects in higher layers\\rDeep learning, object detection\\r\\r\\n\\r\\nA comprehensive list of feature types used in AI and machine learning - Part Two - Programming Ocean Academy\\rFeature Type\\rDefinition\\rExample\\rApplication\\rTexture Features\\rDescribes surface properties or patterns\\rHaralick texture features\\rMedical imaging, material classification\\rColor Features\\rDescribes color properties\\rRGB values, color histograms\\rImage retrieval, object detection\\rShape Features\\rCaptures geometric properties\\rContour descriptors, HOG\\rObject detection, handwriting recognition\\rDerived Features\\rEngineered from transformations\\rPolynomial features\\rFeature engineering, model optimization\\rLatent Features\\rHidden features learned by models\\rLatent factors in matrix factorization\\rDeep learning, recommendation systems\\rCategorical Features\\rRepresents discrete categories\\rGender, product category\\rClassification, recommendation systems\\rNumerical Features\\rRepresents quantitative values\\rAge, income\\rRegression, predictive modeling\\rBinary Features\\rHas only two possible values\\rYes/No, True/False\\rClassification, anomaly detection\\rOrdinal Features\\rOrdered but without fixed intervals\\rEducation level\\rClassification, ranking systems\\rSparse Features\\rContains many zeros or missing values\\rOne-hot encoded vectors\\rText classification, NLP\\rTime-Series Features\\rIndexed by time, captures sequential dependencies\\rAutocorrelation in stock prices\\rFinancial forecasting, predictive maintenance\\rCorrelation Features\\rQuantifies relationship between variables\\rPearson correlation coefficient\\rFeature selection, multicollinearity checking\\rInteraction Features\\rCreated by combining original features\\rBMI from height and weight\\rFeature engineering, non-linear models\\rDimensionality-Reduced Features\\rReduced dimensionality while retaining info\\rPCA components, t-SNE\\rHigh-dimensional data analysis\\rSpectral Features\\rDerived from spectral representation\\rPower spectral density, MFCC\\rAudio processing, speech recognition\\r\\r\\n\\r\\n\\r\\nA timeline in table format tracing the evolution of the term \"Data Science\" the source : Forbes- Programming Ocean Academy\\rYear\\rEvent Description\\r1962\\rJohn W. Tukey writes \"The Future of Data Analysis,\" emphasizing data analysis as an empirical science rather than merely mathematics. Tukey also coined the term \"bit\" in 1947.\\r1974\\rPeter Naur publishes \"Concise Survey of Computer Methods,\" introducing the term \"data science\" and defining it as the science of dealing with established data.\\r1977\\rThe International Association for Statistical Computing (IASC) is established, aiming to link statistical methodology with modern computer technology.\\r1989\\rGregory Piatetsky-Shapiro organizes the first Knowledge Discovery in Databases (KDD) workshop, which evolves into the annual ACM SIGKDD Conference on Knowledge Discovery and Data Mining.\\r1994\\rBusinessWeek publishes a cover story on \"Database Marketing,\" discussing companies\\' use of data to predict consumer behavior.\\r1996\\rThe term \"data science\" is included in the title of the International Federation of Classification Societies conference for the first time.\\r1996\\rUsama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth publish \"From Data Mining to Knowledge Discovery in Databases,\" defining data mining as a step in the Knowledge Discovery process.\\r1997\\rC. F. Jeff Wu calls for the renaming of statistics as data science in his inaugural lecture at the University of Michigan.\\r1997\\rThe journal \"Data Mining and Knowledge Discovery\" is launched, reflecting the rising popularity of data mining.\\r1999\\rJacob Zahavi comments on the challenges of data mining in \"Mining Data for Nuggets of Knowledge.\"\\r2001\\rWilliam S. Cleveland publishes \"Data Science: An Action Plan,\" proposing the establishment of data science as a new discipline.\\r2001\\rLeo Breiman discusses two cultures of statistical modeling, advocating for algorithmic modeling to solve problems with data.\\r2002\\rLaunch of the \"Data Science Journal,\" focusing on data management in science and technology.\\r2003\\rLaunch of the \"Journal of Data Science,\" aimed at presenting applications of statistical methods.\\r2005\\rThomas H. Davenport and others describe the emergence of competition based on analytics in \"Competing on Analytics.\"\\r2005\\rThe National Science Board calls for the development of a career path for data scientists in their report on digital data collections.\\r2007\\rThe Research Center for Dataology and Data Science is established at Fudan University, Shanghai.\\r2008\\rJISC publishes a study defining the role and career development of data scientists and curators.\\r2009\\rThe report \"Harnessing the Power of Digital Data for Science and Society\" emphasizes the need for data science experts.\\r2009\\rHal Varian describes statisticians as the \"sexy job\" of the next decade due to their ability to understand and extract value from data.\\r2009\\rKirk D. Borne and others advocate for training in data science for both specialists and non-specialists.\\r2009\\rMike Driscoll discusses the skills needed for data professionals in \"The Three Sexy Skills of Data Geeks.\"\\r2010\\rKenneth Cukier writes about the emergence of data scientists who combine skills in programming, statistics, and storytelling.\\r2010\\rMike Loukides describes data scientists as inherently interdisciplinary, capable of tackling various aspects of data problems.\\r2010\\rHilary Mason and Chris Wiggins propose a taxonomy of data science activities.\\r2010\\rDrew Conway introduces the \"Data Science Venn Diagram,\" illustrating the skill set required for data scientists.\\r2011\\rPete Warden highlights the ambiguity in defining data science, suggesting it involves interdisciplinary work across various data-related fields.\\r2011\\rDavid Smith discusses the rising popularity and application of the terms \"Data Science\" and \"Data Scientist.\"\\r2012\\rTom Davenport and D.J. Patil publish \"Data Scientist: The Sexiest Job of the 21st Century\" in the Harvard Business Review, highlighting the growing demand and importance of data scientists in the modern workforce.\\r\\r\\n\\r\\n\\r\\nA table summarizing the key milestones in the history of big data- the source: Forbes - Programming Ocean Academy\\rYear\\rMilestone\\r1941\\rThe term \"information explosion\" is first used.\\r1944\\rFremont Rider estimates U.S. university libraries will double in size every 16 years, predicting vast future storage needs.\\r1961\\rDerek Price publishes \"Science Since Babylon,\" noting exponential growth in scientific journals and knowledge.\\r1967\\rMarron and de Maine discuss the need for automatic data compression due to the information explosion.\\r1971\\rArthur Miller comments on measuring individuals by their data storage capacity.\\r1975\\rJapan begins the Information Flow Census, tracking information volume across media.\\r1980\\rI.A. Tjomsland discusses how data expands to fill available storage.\\r1981\\rHungary starts a research project on information industries, measuring information volume.\\r1983\\rIthiel de Sola Pool publishes findings on the growth of communication media and information flow.\\r1986\\rHal B. Becker estimates significant increases in data recording density.\\r1990\\rPeter J. Denning discusses the challenges posed by the overwhelming volume of information.\\r1996\\rDigital storage becomes more cost-effective than paper.\\r1997\\rMichael Cox and David Ellsworth introduce the term \"big data\" in a publication on out-of-core visualization.\\r1998\\rJohn R. Mashey presents on \"Big Data\" at a USENIX meeting.\\r2000\\rPeter Lyman and Hal R. Varian publish a study quantifying new information creation, estimating 1.5 exabytes produced in 1999.\\r2001\\rDoug Laney publishes on 3D data management, establishing volume, velocity, and variety as dimensions of big data.\\r2005\\rTim O\\'Reilly discusses the significance of data in Web 2.0.\\r2007\\rIDC forecasts that digital data will grow significantly, estimating 988 exabytes by 2010.\\r2008\\rCisco predicts IP traffic will double every two years, reaching half a zettabyte in 2012.\\r2009\\rA report finds Americans consumed information for 1.3 trillion hours in 2008, totaling 3.6 zettabytes.\\r2011\\rMartin Hilbert and Priscila Lopez estimate a shift from analog to digital storage, with 94% of storage capacity being digital by 2007.\\r2011\\rMcKinsey Global Institute reports on the rapid growth of stored data across sectors, estimating 7.4 exabytes of new data stored by enterprises in 2010.\\r2012\\rInternational Journal of Communications publishes a special section on information capacity.\\r\\r\\n\\r\\nA Short History of Information Technology (IT)- The source : Forbes- Programming Ocean Academy\\rYear\\rEvent\\r6/30/1945\\rJohn Von Neumann publishes the First Draft of a Report on the EDVAC, introducing the stored-program concept, which became the foundation of modern computer architecture.\\r5/22/1973 \\rBob Metcalfe invents Ethernet at Xerox PARC, enabling Local Area Networks (LANs) and revolutionizing computer connectivity in offices.\\r3/1/1989 \\rTim Berners-Lee circulates \"Information Management: A Proposal\" at CERN, which outlines the basic framework for the World Wide Web, leading to a global hypertext system for sharing information.\\r1980s\\rThe rise of the personal computer (PC) allows individuals to work digitally, but true impact emerges when PCs are connected via LANs, making email the \"killer app\" and increasing data generation exponentially.\\r1990s\\rThe World Wide Web gains popularity, facilitating the rapid spread of information beyond enterprise-focused activities and leading to the digitization of many aspects of daily life. New IT companies like Intel, Microsoft, Oracle, Cisco, Dell, and EMC emerge as dominant players.\\r6/15/1905\\rCERN releases the World Wide Web to the public, making the internet widely accessible and paving the way for a global network of connected computers.\\r2000s\\rThe IT industry shifts towards specialization, with horizontal players focusing on key components like semiconductors (Intel), operating systems (Microsoft), databases (Oracle), and networking (Cisco).\\r2000s-2010s\\rThe cloud computing revolution and the rise of \"big data\" change the IT landscape. The Internet of Things (IoT) connects billions of devices, vastly increasing the amount of data generated and shared.\\r6/12/1905\\rPeter J. Denning discusses the challenges posed by the overwhelming volume of information.\\r6/18/1905\\rDigital storage becomes more cost-effective than paper.\\r6/19/1905\\rMichael Cox and David Ellsworth introduce the term \"big data\" in a publication on out-of-core visualization.\\r6/20/1905\\rJohn R. Mashey presents on \"Big Data\" at a USENIX meeting.\\r6/22/1905\\rPeter Lyman and Hal R. Varian publish a study quantifying new information creation, estimating 1.5 exabytes produced in 1999.\\r6/23/1905\\rDoug Laney publishes on 3D data management, establishing volume, velocity, and variety as dimensions of big data.\\r6/27/1905\\rTim O\\'Reilly discusses the significance of data in Web 2.0.\\r6/29/1905\\rIDC forecasts that digital data will grow significantly, estimating 988 exabytes by 2010.\\r6/30/1905\\rCisco predicts IP traffic will double every two years, reaching half a zettabyte in 2012.\\r7/1/1905\\rA report finds Americans consumed information for 1.3 trillion hours in 2008, totaling 3.6 zettabytes.\\r7/3/1905\\rMartin Hilbert and Priscila Lopez estimate a shift from analog to digital storage, with 94% of storage capacity being digital by 2007.\\r7/3/1905\\rMcKinsey Global Institute reports on the rapid growth of stored data across sectors, estimating 7.4 exabytes of new data stored by enterprises in 2010.\\r7/4/1905\\rInternational Journal of Communications publishes a special section on information capacity.\\r\\r\\n\\r\\nA comparison of Scikit-learn, TensorFlow, and PyTorch- Programming Ocean Academy\\rAspect\\rScikit-learn\\rTensorFlow\\rPyTorch\\rPrimary Use\\rTraditional machine learning (ML) algorithms like classification, regression, clustering, and preprocessing.\\rDeep learning, neural networks, large-scale ML.\\rDeep learning, neural networks, research-oriented deep learning tasks.\\rAPI Style\\rSimple, high-level, easy to use, and well-suited for beginners.\\rFlexible but can be complex, uses high-level Keras API for ease.\\rMore Pythonic and intuitive, especially for researchers and developers.\\rCore Language\\rPython (also supports integration with other languages).\\rPython (primary), also supports C++, JavaScript, Java, Go, and others.\\rPython (primary), also supports C++.\\rType of ML Supported\\rTraditional ML (SVM, Decision Trees, Random Forest, etc.).\\rDeep learning (CNNs, RNNs, Transformers), some support for traditional ML.\\rDeep learning (CNNs, RNNs, Transformers).\\rGPU Support\\rNo direct support for GPU acceleration.\\rYes, strong support for GPU via CUDA and TPU.\\rYes, native GPU support via CUDA.\\rDistributed Computing\\rNot designed for distributed systems, though integration with Spark is possible.\\rSupports distributed training across multiple GPUs and machines.\\rSupports distributed training, including data parallelism and model parallelism.\\rEase of Use\\rVery easy to use for classical ML tasks with consistent APIs.\\rHigh-level API (Keras) is easy, but low-level API can be more complex.\\rIntuitive for deep learning; dynamic computational graphs make it easier to debug.\\rPerformance\\rEfficient for small to medium datasets but not optimized for massive datasets or deep learning.\\rOptimized for large-scale deep learning and high-performance computing.\\rHighly performant for deep learning, particularly on GPUs.\\rModel Deployment\\rLimited deployment options (e.g., via pickle or joblib for ML models).\\rTensorFlow Lite, TensorFlow Serving for production environments. Widely used for mobile and web.\\rSupports TorchServe for model serving, but deployment is less extensive than TensorFlow.\\rVisualization Tools\\rLimited; integrates with Matplotlib or Seaborn for visualization.\\rTensorBoard for model training visualization and debugging.\\rSupports integration with TensorBoard, but fewer built-in tools compared to TensorFlow.\\rCommunity & Ecosystem\\rLarge community for traditional ML; many pre-built algorithms and extensions.\\rVery large community, strong Google backing, extensive ecosystem (TensorFlow Hub, Lite, etc.).\\rGrowing community, strong in academia and research, especially due to dynamic computation graph and flexibility.\\rLearning Curve\\rLow, particularly for beginners in ML.\\rMedium to high; Keras API simplifies deep learning but TensorFlow\\'s full flexibility is complex.\\rLow to medium; dynamic computation graph and Pythonic design make it easier to learn for researchers.\\rDynamic vs Static Graphs\\rStatic computation graph (not suited for dynamic deep learning tasks).\\rPrimarily uses static computation graph (though supports eager execution).\\rDynamic computation graph, allowing more flexibility and ease for debugging.\\rPre-trained Models\\rFew, mostly for traditional ML (limited transfer learning options).\\rLarge model zoo (TensorFlow Hub) for deep learning tasks (e.g., BERT, ResNet).\\rPyTorch Hub offers a growing library of pre-trained models (e.g., ResNet, GPT).\\rPopularity\\rWidely used in industry and academia for traditional ML.\\rDominant in production environments for deep learning; used by many large companies (Google, Uber, etc.).\\rIncreasingly popular in academia and research, especially for cutting-edge deep learning models.\\r\\r\\n\\r\\n\\r\\nA comparison between Matplotlib and Seaborn- Programming Ocean Academy\\rFeature\\rMatplotlib\\rSeaborn\\rPurpose\\rGeneral-purpose plotting library\\rHigh-level interface for statistical plotting\\rEase of Use\\rMore control but requires detailed code\\rEasier to use with built-in themes and defaults\\rCustomization\\rHighly customizable but manual\\rBuilt on Matplotlib, offers automatic styling and improved aesthetics\\rPlot Types\\rBasic plots: line, scatter, bar, histogram, etc.\\rSpecialized for statistical plots like heatmaps, pair plots, violin plots\\rLearning Curve\\rSteeper for complex visualizations\\rEasier for quick, insightful visualizations\\rDefault Aesthetics\\rMinimal by default\\rStylish, colorful, and attractive by default\\rStatistical Visualization\\rRequires manual setup for statistical plots\\rBuilt-in support for statistical plots (e.g., KDE, regression)\\rIntegration\\rWorks well with low-level libraries like NumPy\\rIntegrates seamlessly with Pandas DataFrames and NumPy\\rThemes/Styles\\rCustomizable but requires manual settings\\rPredefined themes (e.g., \"darkgrid\", \"whitegrid\") for easy styling\\rPerformance\\rHighly performant for large datasets\\rSlightly slower due to additional abstraction over Matplotlib\\rAnnotations & Labels\\rRequires manual positioning and adjustments\\rSimplifies labeling with built-in functionality\\rData Handling\\rCan plot data from arrays, lists, DataFrames\\rPrimarily designed for Pandas DataFrames and tidy data\\rCommunity & Support\\rLarge community, extensive documentation\\rSmaller but still active community, based on Matplotlib\\r\\r\\n\\r\\n\\r\\nA comparison between the Training Set, Validation Set, and Testing Set- Programming Ocean Academy\\rAspect\\rTraining Set\\rValidation Set\\rTesting Set\\rPurpose\\rUsed to train the model by adjusting its weights and learning patterns.\\rUsed to tune the model\\'s hyperparameters and prevent overfitting.\\rUsed to evaluate the final model\\'s performance on unseen data.\\rData Composition\\rComprises the largest portion of the dataset (e.g., 70-80%).\\rTypically, smaller than the training set (e.g., 10-20%).\\rAlso, a small portion of the dataset (e.g., 10-20%).\\rModel Feedback\\rThe model learns from this data and updates its parameters based on the training process.\\rThe model receives feedback on its performance to help improve and optimize during training.\\rThe model does not receive feedback during this stage; it measures generalization.\\rUsage in Training Cycle\\rDirectly involved in the training cycle where the model learns the data features.\\rUsed during training to validate model performance and adjust hyperparameters, often via techniques like cross-validation.\\rUsed after the model is fully trained to assess accuracy and performance metrics.\\rImpact on Model\\rDirectly impacts how well the model learns from the data.\\rAffects the choice of hyperparameters, impacting model architecture and generalization ability.\\rIndicates how well the model will perform in real-world applications.\\rExample Ratio\\r70-80% of the total dataset.\\r10-20% of the total dataset.\\r10-20% of the total dataset.\\rData Leakage Risk\\rRisk of overfitting if too few examples are used or if the model is too complex.\\rOverfitting can still occur if validation set is not representative or if hyperparameters are not tuned carefully.\\rShould be completely separate to provide an unbiased evaluation of model performance.\\r\\r\\n\\r\\n\\r\\nA comparison between Overfitting, Underfitting, and a Balanced Model- Programming Ocean Academy\\rAspect\\rOverfitting\\rUnderfitting\\rBalanced Model\\rDefinition\\rThe model learns the training data too well, capturing noise and outliers, resulting in poor generalization to new data.\\rThe model is too simple to capture the underlying patterns in the data, leading to poor performance on both training and testing datasets.\\rThe model adequately captures the underlying patterns in the data, performing well on both training and testing datasets.\\rModel Complexity\\rHighly complex with too many parameters relative to the amount of data.\\rToo simple with too few parameters or features.\\rAppropriate complexity, balancing bias and variance.\\rTraining Performance\\rHigh accuracy on the training set.\\rLow accuracy on both training and testing sets.\\rGood accuracy on both training and testing sets.\\rTesting Performance\\rPoor accuracy on the testing set due to lack of generalization.\\rPoor accuracy on the testing set, similar to training set performance.\\rGood accuracy on the testing set, indicating effective generalization.\\rIndicators\\rLarge difference between training and testing accuracy (high training accuracy, low testing accuracy).\\rLittle difference in performance between training and testing sets, but both are low.\\rSimilar and satisfactory performance metrics for both training and testing sets.\\rCommon Causes\\rExcessive training time, too many features, or lack of regularization.\\rInsufficient training, overly simplistic model, or poor feature selection.\\rProper model selection, sufficient training, and appropriate feature engineering.\\rExample\\rA model that memorizes every data point in the training set, failing to predict new data accurately.\\rA linear model trying to fit a complex, non-linear relationship in the data.\\rA decision tree that finds a reasonable balance between complexity and generalization.\\r\\r\\n\\r\\n\\r\\n\\r\\nA table comparing Gini Index, Entropy, and Noise - Programming Ocean Academy\\rAspect\\rGini Index\\rEntropy\\rNoise\\rDefinition\\rA metric that measures the impurity of a split.\\rA metric from information theory that measures uncertainty or impurity in data.\\rUnwanted random variation or disturbances in data, unrelated to the underlying signal.\\rRange\\r0 to 0.5 (binary classification). 0 = pure node, 0.5 = maximum impurity.\\r0 to 1 (binary classification). 0 = pure node, 1 = maximum uncertainty.\\rDepends on the data; usually ranges from small perturbations to significant disruptions.\\rInterpretation\\rLower values indicate purer nodes (less impurity).\\rLower values indicate less disorder or uncertainty. Higher values show more disorder.\\rRepresents the level of distortion in the data, which can lead to inaccurate predictions.\\rRelation to Data\\rMeasures the probability that a randomly chosen element would be misclassified.\\rMeasures the amount of information needed to classify data points.\\rRepresents data irregularities that may obscure patterns or affect model accuracy.\\rRole in Decision Trees\\rUsed to determine the best split by minimizing impurity in child nodes.\\rUsed to select the best split by maximizing information gain.\\rConsidered a source of error or distortion; decision trees aim to reduce the impact of noise on splitting criteria.\\r\\r\\n\\r\\n\\r\\nHow GridSearch helps in finding the best hyperparameters- Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rGridSearch is an exhaustive search method to find the best combination of hyperparameters for a machine learning model.\\rProcess\\rIt tries every combination of hyperparameter values specified in a predefined grid and evaluates the model for each combination.\\rGrid Creation\\rA grid of hyperparameter values is defined by the user, consisting of various possible values for each hyperparameter.\\rSearch Strategy\\rExhaustive search over the entire grid of hyperparameters; each combination is evaluated independently.\\rEvaluation\\rFor each combination, the model is trained and evaluated (usually using cross-validation) to compute performance metrics.\\rPerformance Metric\\rThe user specifies a performance metric (e.g., accuracy, F1-score, etc.) to evaluate each hyperparameter combination.\\rBest Hyperparameter Selection\\rAfter evaluating all combinations, GridSearch selects the hyperparameter set that maximizes the chosen performance metric.\\rTime Complexity\\rCan be computationally expensive because it evaluates every possible combination of hyperparameters (especially with large grids).\\rAdvantages\\rSimple to implement, ensures all combinations are tested, and guarantees finding the best combination within the given grid.\\rLimitations\\rComputationally expensive, especially with many hyperparameters and large search spaces; doesn\\'t scale well to complex models.\\rUse Case\\rBest suited for models with smaller hyperparameter spaces or when computational resources are sufficient for exhaustive search.\\r\\r\\n\\r\\n\\r\\nA comparison between Logistic Regression, Support Vector Classifier (SVC), Decision Trees, and K-Nearest Neighbors (KNN)- Programming Ocean Academy\\rAspect\\rLogistic Regression\\rSupport Vector Classifier (SVC)\\rDecision Trees\\rK-Nearest Neighbors (KNN)\\rType of Algorithm\\rLinear model, probabilistic classifier\\rNon-linear (can also be linear with certain kernels)\\rNon-parametric, tree-based model\\rInstance-based learning (lazy learning algorithm)\\rMathematical Basis\\rUses a logistic (sigmoid) function to model the probability of binary outcomes\\rMaximizes the margin between data points and decision boundary (hyperplane)\\rUses a tree structure to split data based on feature importance\\rClassifies based on the majority class of nearest neighbors\\rUse Case\\rBinary classification problems (can be extended to multi-class)\\rClassification of both linear and non-linear data\\rSuitable for both classification and regression tasks\\rSuitable for classification tasks, can also be used for regression\\rAssumptions\\rAssumes a linear relationship between features and log-odds of the outcome\\rAssumes data can be separated with a hyperplane in high-dimensional space\\rAssumes no specific distribution, no parametric assumptions\\rNo assumptions about the distribution of data\\rInterpretability\\rHigh, coefficients show the impact of each feature\\rMedium, more difficult to interpret due to kernel transformations\\rMedium, easy to interpret the decision path in the tree\\rLow, as it only relies on the distance metric from the nearest neighbors\\rPerformance on Small Data\\rGood for small to medium-sized datasets\\rWorks well with smaller datasets but can scale with kernel trick\\rWorks well with small datasets, prone to overfitting\\rEffective for small datasets but can be slow on larger ones\\rPerformance on Large Data\\rEfficient for large datasets\\rCan be computationally expensive with large datasets\\rStruggles with large, complex datasets (prone to overfitting)\\rBecomes computationally expensive with large datasets\\rHandling of Non-linearity\\rNot suitable for non-linear problems without feature transformation\\rExcellent for non-linear problems using kernels\\rHandles non-linear data naturally through branching\\rCan handle non-linear problems well depending on the distance metric used\\rOverfitting Tendency\\rLess prone to overfitting if regularization is applied\\rCan overfit if the kernel is too complex\\rHighly prone to overfitting, especially without pruning\\rProne to overfitting if k (number of neighbors) is small\\rHyperparameters\\rRegularization strength (C), solver\\rKernel (linear, polynomial, RBF), C (penalty parameter), gamma\\rMax depth, min samples per split, splitting criterion\\rNumber of neighbors (k), distance metric (Euclidean, Manhattan, etc.)\\rComputational Complexity\\rLow, computationally efficient\\rHigher complexity due to support vectors and kernel computation\\rLow complexity for small trees but grows with tree depth\\rHigh, as it involves computing distances from each point to all other points\\rHandling Outliers\\rCan be sensitive to outliers\\rRobust to outliers if margin is wide\\rSensitive to outliers; can lead to skewed splits\\rSensitive to outliers since distance-based methods are affected by them\\rMemory Usage\\rLow, only requires storing coefficients\\rMedium to high, depends on the number of support vectors\\rLow to moderate, depends on tree depth\\rHigh, since it needs to store all training data for comparison\\rPrediction Speed\\rFast\\rFast after training, but training can be slow for complex kernels\\rFast once the tree is built\\rSlow, as it computes the distance to all points at prediction time\\rExample\\rEmail spam detection, disease diagnosis\\rImage classification, text classification\\rCredit scoring, customer churn prediction\\rHandwriting recognition, recommendation systems\\r\\r\\n\\r\\n\\r\\n\\r\\nA comparison between GridSearch and GridSearchCV- Programming Ocean Academy\\rFeature\\rGridSearch\\rGridSearchCV\\rDefinition\\rA process that evaluates all combinations of hyperparameters over a given set but does not involve cross-validation.\\rA method from sklearn.model_selection that performs exhaustive search over specified hyperparameter values with built-in cross-validation.\\rPrimary Use\\rManually implemented to find the best hyperparameters, usually without automatic cross-validation.\\rUsed to automatically tune hyperparameters with cross-validation built in, ensuring model robustness.\\rCross-Validation\\rDoes not perform cross-validation by default. You must manually split the data or use additional validation techniques.\\rPerforms cross-validation (CV) automatically based on the provided cv parameter (e.g., k-folds).\\rLibrary Support\\rNot directly supported by libraries like scikit-learn. Typically requires manual coding for parameter search.\\rDirectly supported by scikit-learn with the class GridSearchCV.\\rModel Evaluation\\rEvaluates model performance based on a given validation set, not using multiple splits for CV.\\rUses cross-validation, evaluating the model across multiple folds of training data to give a more reliable performance estimate.\\rOverfitting Risk\\rHigher risk of overfitting since it may evaluate the model only on a single validation set.\\rLower risk of overfitting due to cross-validation, as it tests the model across different data folds.\\rEfficiency\\rLess efficient in terms of ensuring generalization since it may focus on a specific dataset split.\\rMore efficient in evaluating the generalization of the model by testing on multiple data splits.\\rOutput\\rProvides the best parameters based on the specified validation set.\\rProvides the best parameters based on cross-validated performance across different folds\\r\\r\\n\\r\\nA table summarizing the processes the model goes through during one epoch- Programming Ocean Academy\\rStep\\rDescription\\r1. Shuffling\\rOptionally shuffle the dataset to randomize the order of samples before processing (helps prevent bias).\\r2. Batch creation\\rDivide the dataset into smaller subsets called batches for efficient processing during training.\\r3. Forward Propagation\\rFor each batch, the model passes the input data through the network layers to produce predictions.\\r4. Loss Calculation\\rCompare the model\\'s predictions to the actual labels using a loss function to quantify the error.\\r5. Backpropagation\\rCompute gradients (partial derivatives of the loss) by propagating the error backward through the network.\\r6. Weight Updates\\rUpdate the model\\'s weights using the computed gradients and an optimization algorithm (e.g., SGD, Adam).\\r7. Evaluation\\rOptionally evaluate the model\\'s performance on metrics like accuracy, using either training or validation data.\\r8. Repeat for All Batches\\rRepeat steps 3 to 7 for each batch in the dataset within the epoch.\\r9. Epoch-End Actions\\rOptional actions like validating on a separate dataset, adjusting the learning rate, or saving model checkpoints.\\r\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nA table explaining the terms related to deep learning - Programming Ocean Academy\\rTerm\\rExplanation\\rEpoch\\rOne complete pass through the entire training dataset. During an epoch, the model processes every data point in the dataset, adjusts its weights, and learns from the data.\\rLearning Rate\\rA hyperparameter that controls how much to adjust the model\\'s weights in response to the calculated gradients. A higher learning rate speeds up training but may overshoot, while a lower rate offers more precise adjustments but slower convergence.\\rActivation\\rThe function applied at each neuron in a neural network layer, introducing non-linearity into the model. Common activation functions include ReLU, sigmoid, and tanh.\\rRegularization\\rA technique used to prevent overfitting by adding a penalty (or constraint) to the loss function. Common methods include L1 (Lasso) and L2 (Ridge) regularization.\\rRegularization Rate\\rA hyperparameter that controls the strength of the regularization applied. A higher regularization rate increases the penalty on larger weights, helping to simplify the model and reduce overfitting.\\rRatio of Training to Test Size\\rThe proportion of the dataset used for training versus testing (or validation). A typical split might be 80% training and 20% testing. The training set is used for learning, while the test set evaluates model performance on unseen data.\\rNoise\\rUnwanted variations or randomness in the data that can distort the learning process. In deep learning, noise can arise from data collection errors, or it can be deliberately introduced as a regularization technique (e.g., dropout, adding Gaussian noise).\\rBatch Size\\rThe number of training examples processed in one iteration. Smaller batch sizes lead to noisier updates but require less memory and can generalize better. Larger batch sizes can be more stable but may lead to slower generalization. Common sizes include 32, 64, and 128.\\r\\r\\n\\r\\n\\r\\nA table explaining the anatomy of a neural network in deep learning- Programming Ocean Academy\\rComponent\\rExplanation\\rInput Layer\\rThe first layer in the neural network that receives the raw data (features) to be processed. Each neuron in this layer corresponds to one feature from the input data.\\rHidden Layers\\rIntermediate layers between the input and output layers, where most of the computations occur. The network can have one or more hidden layers. Each hidden layer applies weights, biases, and activation functions to transform the input into something meaningful for the model.\\rNeurons\\rThe basic units in each layer. Each neuron takes inputs, applies weights and biases, sums them, and passes the result through an activation function. The output is then passed to neurons in the next layer.\\rWeights\\rParameters that determine the strength and direction of the connection between neurons in adjacent layers. Weights are learned during training and adjusted to minimize the loss function.\\rBiases\\rAdditional parameters added to the weighted sum of inputs before applying the activation function. Biases help the model fit the data better by shifting the activation function.\\rActivation Functions\\rFunctions applied at each neuron to introduce non-linearity. Common functions include ReLU, sigmoid, and tanh, which allow the model to capture complex patterns in the data.\\rOutput Layer\\rThe final layer that provides the predictions or output of the model. The number of neurons in the output layer typically corresponds to the number of prediction classes or the target variables in regression.\\rForward Propagation\\rThe process where input data passes through the network from the input layer, through hidden layers, to the output layer, generating predictions based on current weights and biases.\\rLoss Function\\rA function that measures the error or difference between the predicted output and the actual target value. Common loss functions include Mean Squared Error for regression and Cross-Entropy Loss for classification.\\rBackpropagation\\rThe process of calculating the gradient of the loss function with respect to the weights and biases in the network, starting from the output layer and moving backward through the layers. This allows the network to learn by updating the weights to minimize the loss.\\rOptimization Algorithm\\rAn algorithm (such as Stochastic Gradient Descent or Adam) used to update the network\\'s weights and biases based on the gradients computed during backpropagation, to minimize the loss function.\\rLearning Rate\\rA hyperparameter that controls how much to adjust the weights during each update. A lower learning rate makes smaller adjustments and converges slowly, while a higher learning rate can speed up training but may overshoot the optimal solution.\\rEpoch\\rOne full pass through the entire training dataset. During an epoch, the model processes the data, calculates the loss, performs backpropagation, and updates the weights. Multiple epochs are needed for the model to learn effectively.\\rBatch Size\\rThe number of training samples processed in one forward and backward pass. Small batch sizes lead to noisier updates but can help generalize better, while large batch sizes provide more stable updates but may require more memory.\\rRegularization\\rTechniques (such as L1 and L2 regularization or dropout) used to prevent overfitting by adding a penalty to the loss function or randomly disabling neurons during training.\\rDropout\\rA regularization technique where a random set of neurons is ignored (dropped out) during each training iteration. This forces the model to learn more robust features and helps prevent overfitting.\\rSoftmax\\rA function often used in the output layer for multi-class classification tasks. It converts the output scores into probabilities, making it easier to interpret which class the model predicts.\\r\\r\\n\\r\\na comparative table outlining the key differences between Machine Learning and Deep Learning- Programming Ocean Academy\\rAspect\\rMachine Learning\\rDeep Learning\\rDefinition\\rA subset of artificial intelligence that enables systems to learn from data and improve their performance without explicit programming.\\rA subset of machine learning that uses neural networks with many layers to model complex patterns in large datasets.\\rData Requirements\\rTypically requires less data to train models effectively.\\rRequires large amounts of data to achieve high performance, often needing thousands to millions of samples.\\rFeature Engineering\\rOften requires manual feature extraction and selection to identify relevant input features for model training.\\rAutomatically learns features from raw data, eliminating the need for extensive manual feature engineering.\\rComplexity\\rGenerally simpler algorithms (e.g., linear regression, decision trees) that are easier to interpret.\\rMore complex architectures (e.g., CNNs, RNNs) that are often seen as \"black boxes\" due to their complexity.\\rTraining Time\\rTypically faster training times due to simpler algorithms and less data.\\rLonger training times, often requiring powerful hardware (GPUs) to handle large datasets and complex computations.\\rPerformance\\rPerforms well on structured data (like tables) but may struggle with unstructured data (like images and text).\\rExcels at processing unstructured data, making it suitable for tasks such as image recognition, natural language processing, and speech recognition.\\rModel Interpretability\\rModels are generally more interpretable, allowing easier understanding of how decisions are made.\\rModels are less interpretable, making it difficult to understand how decisions are derived from input data.\\rExamples\\rAlgorithms include linear regression, logistic regression, decision trees, random forests, and support vector machines.\\rCommon architectures include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers.\\rApplication Areas\\rCommonly used in applications like recommendation systems, fraud detection, and predictive analytics.\\rWidely used in advanced applications such as computer vision, speech recognition, and natural language processing.\\rHardware Dependency\\rUsually runs efficiently on standard CPUs.\\rOften requires specialized hardware, such as GPUs or TPUs, to accelerate the training of deep neural networks.\\r\\r\\n\\r\\nA table summarizing the breakthroughs in biology, proteins, and human health using AI models- Programming Ocean Academy\\rField\\rBreakthrough\\rImpact\\rProtein Folding\\rAlphaFold by DeepMind\\rAccurate prediction of 3D protein structures, revolutionizing drug discovery, disease understanding, and protein design.\\rDrug Discovery & Development\\rAI-driven drug design (e.g., Atomwise,BenevolentAI)\\rAccelerates the identification of drug candidates and optimization of existing drugs.\\r\\rPredictive toxicology\\rAssesses drug safety and toxicity before clinical trials, reducing costs and time.\\rGenomic Analysis\\rPrecision medicine\\rAI analyzes genetic data to tailor personalized treatments based on an individual\\'s genetic profile.\\r\\rSingle-cell RNA sequencing\\rHelps understand cellular diversity and the role of specific genes in diseases.\\rDisease Diagnosis\\rAI in medical imaging (e.g., radiology, pathology)\\rImproves accuracy in detecting abnormalities such as tumors and lesions in scans, speeding up diagnosis.\\r\\rPredictive analytics\\rAnalyzes health records to predict disease onset and progression (e.g., diabetes, cardiovascular diseases).\\rUnderstanding Disease Mechanisms\\rMolecular dynamics simulations\\rAI-enhanced simulations offer insights into protein interactions and disease pathways.\\r\\rSystems biology\\rIntegrates data from genomics, proteomics, etc., to understand complex disease mechanisms and identify new therapeutic targets.\\rVaccine Development\\rRapid antigen prediction\\rAI predicts vaccine candidates by modeling immune responses, accelerating vaccine development.\\rHealth Monitoring & Management\\rWearable technology\\rAI analyzes real-time health metrics from wearables, aiding early disease detection and personal health insights.\\r\\rTelemedicine and AI chatbots\\rEnhances patient interaction, provides symptom assessment, and facilitates remote healthcare services.\\rMental Health\\rSentiment analysis in social media\\rDetects early signs of mental health issues by analyzing language patterns, enabling timely intervention.\\r\\r\\n\\r\\n\\r\\nA table that outlines the most common optimization algorithms used in deep learning- Programming Ocean Academy\\rAlgorithm\\rDescription\\rCommon Uses\\rStochastic Gradient Descent (SGD)\\rUpdates weights after each training example rather than the entire dataset, making training faster.\\rLarge datasets; simple models; used for faster training, though convergence may be unstable.\\rMini-Batch Gradient Descent\\rUses small batches of data for weight updates, combining the benefits of SGD and full-batch methods.\\rPreferred in most deep learning applications for stable convergence and efficiency.\\rMomentum\\rAccelerates gradient descent by adding a fraction of the previous update to the current one.\\rUsed to avoid local minima and speed up convergence in deep networks.\\rRMSProp\\rAdapts the learning rate by dividing it by a running average of the magnitudes of recent gradients.\\rPopular for recurrent neural networks (RNNs) and noisy data; good for non-stationary environments.\\rAdam (Adaptive Moment Estimation)\\rCombines RMSProp and Momentum; computes adaptive learning rates for each parameter using first and second moments.\\rWidely used in deep learning for fast, efficient training across many tasks, including NLP and vision.\\rNadam\\rAdam with Nesterov momentum; looks ahead and adjusts gradient descent accordingly.\\rMore efficient than Adam in some cases; used for faster convergence in image classification and NLP.\\rAdaGrad\\rAdapts learning rate for each parameter individually based on the sum of historical squared gradients.\\rSuitable for sparse data; used in text classification, recommendation systems, and NLP tasks.\\rAdadelta\\rImproves AdaGrad by limiting the accumulation of past gradients to a fixed window size.\\rHandles decaying learning rates better than AdaGrad; useful in image recognition and speech tasks.\\rL-BFGS\\rA second-order optimization algorithm that approximates the Hessian matrix for faster convergence.\\rSmall datasets; shallow networks; sometimes used in fine-tuning pre-trained models.\\rNAG (Nesterov Accelerated Gradient)\\rA variant of Momentum that uses a look-ahead mechanism to adjust gradients.\\rFaster convergence compared to vanilla momentum; commonly used in deep networks.\\r\\r\\n\\r\\n\\r\\nA comprehensive table that outlines the common optimization algorithms in deep learning, along with their strengths and weaknesses-Programming Ocean Academy\\rAlgorithm\\rStrengths\\rWeaknesses\\rStochastic Gradient Descent (SGD)\\r\\n \\r\\n \\r- Fast updates with each training example\\r\\n- Good for large datasets\\r\\n- Can escape local minima\\r- High variance in updates\\r\\n- May converge slowly or oscillate if learning rate is too high\\r\\n \\rMini-Batch Gradient Descent\\r\\n \\r\\n \\r- Balances speed and convergence\\r\\n- Reduces variance in updates\\r\\n- Faster than full-batch\\r- Requires careful tuning of batch size\\r\\n- May still struggle with local minima\\r\\n \\rMomentum\\r\\n \\r\\n \\r- Speeds up convergence\\r\\n- Reduces oscillation\\r\\n- Helps escape local minima\\r- Can overshoot minima\\r\\n- Requires tuning of momentum parameter\\r\\n \\rRMSProp\\r\\n \\r- Adaptive learning rate helps with varying gradients\\r\\n- Effective for non-stationary objectives\\r- Requires tuning of decay factor\\r\\n- Can be sensitive to hyperparameter choices\\rAdam (Adaptive Moment Estimation)\\r\\n \\r\\n \\r- Combines advantages of RMSProp and Momentum\\r\\n- Fast convergence\\r\\n- Well-suited for large datasets\\r- Can be overly aggressive with learning rates\\r\\n- May lead to suboptimal solutions in some cases\\r\\n \\rNadam\\r\\n \\r- Incorporates Nesterov momentum for faster convergence\\r\\n- Often performs better than Adam\\r- More complex than Adam\\r\\n- Still requires careful tuning of hyperparameters\\rAdaGrad\\r\\n \\r- Good for sparse data and features\\r\\n- Adapts learning rate based on parameter importance\\r- Learning rate can decrease too quickly, leading to premature convergence\\r\\n \\rAdadelta\\r\\n \\r- Addresses learning rate decay issue in AdaGrad\\r\\n- Keeps track of recent gradients\\r- Still requires tuning of decay parameters\\r\\n- May converge slowly for some tasks\\rL-BFGS\\r\\n \\r- Fast convergence with fewer iterations\\r\\n- Good for small datasets\\r- Memory-intensive due to second-order approximation\\r\\n- Less suitable for large datasets\\rNAG (Nesterov Accelerated Gradient)\\r\\n \\r- More responsive to changes in the loss landscape\\r\\n- Generally faster convergence compared to Momentum\\r- More complex implementation\\r\\n- Requires careful tuning of parameters\\r\\r\\n\\r\\nA comparison between learning rate, loss rate, overfitting, and underfitting- Programming Ocean Academy\\rAspect\\rLearning Rate\\rLoss Rate\\rOverfitting\\rUnderfitting\\rDefinition\\rThe learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function.\\rThe loss rate refers to the error or difference between the predicted output and the actual output, typically measured by a loss function (e.g., MSE, Cross-Entropy).\\rOverfitting occurs when a model learns the training data too well, including noise and details, leading to poor generalization on unseen data.\\rUnderfitting happens when a model is too simple and fails to capture the underlying patterns in the data.\\rEffect on Model\\rControls how fast the model updates weights. A high learning rate can cause the model to converge too quickly to a suboptimal solution, while a low learning rate can lead to slow convergence or getting stuck in local minima.\\rA high loss rate indicates poor model performance, while a low loss rate indicates that the model\\'s predictions are closer to the actual values.\\rResults in high accuracy on training data but poor performance on testing data due to memorization of noise rather than learning the general pattern.\\rLeads to both poor training and testing accuracy because the model is too simplistic to capture complex patterns.\\rCauses\\rA high learning rate may skip the optimal solution, and a low learning rate may take too long to converge.\\rHigh loss rate occurs due to a poorly trained model, improper hyperparameters, or incorrect data preprocessing.\\rToo complex models with too many parameters, insufficient regularization, or too much training time.\\rToo simple models with insufficient complexity, inadequate training, or incorrect features.\\rSymptoms\\rHigh learning rate: unstable training and large loss oscillations. Low learning rate: very slow convergence.\\rHigh loss rate: large errors between predictions and true values. Low loss rate: predictions are close to true values.\\rHigh training accuracy and low testing accuracy (large gap).\\rLow accuracy on both training and testing data (small gap between them).\\rRelationship with Model Performance\\rOptimal learning rate ensures efficient convergence and better model performance.\\rA low loss rate indicates good model performance, while a high loss rate indicates poor performance and incorrect predictions.\\rHurts generalization by fitting the model too closely to the training data, capturing noise.\\rReduces performance as the model fails to learn important patterns, leading to low predictive accuracy.\\rPrevention\\rAdjusting the learning rate through techniques like learning rate schedules, adaptive learning rates, or grid search.\\rImprove model training, tune hyperparameters, use better loss functions, and improve data preprocessing.\\rUse techniques such as cross-validation, regularization (L1/L2), dropout, or reducing model complexity.\\rIncrease model complexity, add more features, improve feature engineering, or train for more epochs.\\rExample\\rLearning rate is a key parameter in gradient descent algorithms used in deep learning and machine learning models.\\rLoss rate is measured through loss functions such as Mean Squared Error (MSE) or Cross-Entropy in classification tasks.\\rA neural network with too many layers or a decision tree with too deep branches could overfit the data.\\rA linear regression model trying to fit nonlinear data or a shallow decision tree can lead to underfitting.\\r\\r\\n\\r\\n\\r\\n\\r\\nA comparison between convergence and divergence in the context of AI - Programming Ocean Academy\\rAspect\\rConvergence\\rDivergence\\rDefinition\\rConvergence refers to the process where a machine learning model\\'s training stabilizes over time, meaning the loss function or error decreases and approaches a minimum value.\\rDivergence refers to when a machine learning model\\'s training becomes unstable, where the loss function or error increases or fluctuates indefinitely without stabilizing.\\rBehavior in Training\\rThe model\\'s performance improves with each iteration as it moves toward the optimal solution.\\rThe model fails to learn effectively, resulting in increasing or highly oscillating loss, and the model moves away from the optimal solution.\\rImpact on Model\\rLeads to better generalization and improved predictive accuracy on unseen data.\\rPrevents the model from learning meaningful patterns, resulting in poor accuracy and poor generalization.\\rCauses\\r- Appropriate learning rate\\r\\n- Proper model architecture\\r\\n- Correct data preprocessing and loss function\\r- Learning rate is too high\\r\\n- Model is too complex\\r\\n- Improper data preprocessing or feature scaling\\r\\n- Incorrect gradient descent algorithm\\rSymptoms\\r- Gradual and steady decrease in loss function\\r\\n- Improved accuracy on both training and testing data\\r\\n- Stability during optimization\\r- Increasing or erratic loss function\\r\\n- Decreasing accuracy on both training and testing data\\r\\n- Instability during optimization (fluctuations in loss)\\rRelationship to Learning Rate\\rA proper learning rate helps the model converge to a global or local minimum.\\rA too-high learning rate often causes divergence, where the model overshoots the optimal solution repeatedly.\\rModel\\'s Path\\rThe model follows a path that narrows down to the minimum of the loss function, effectively optimizing its parameters.\\rThe model\\'s path moves erratically or away from the minimum of the loss function, resulting in poor parameter optimization.\\rPrevention\\r- Use an appropriate learning rate (with tuning)\\r\\n- Proper data normalization and regularization\\r\\n- Cross-validation to monitor progress\\r- Reduce learning rate\\r\\n- Simplify model architecture\\r\\n- Ensure correct data scaling and processing\\rExample\\rA well-tuned deep neural network trained using gradient descent that successfully reduces the loss and achieves high accuracy.\\rA deep neural network trained with a very high learning rate, causing the model\\'s loss function to increase drastically after each iteration.\\r\\r\\n\\r\\n\\r\\nA table summarizing the learning rate, loss function behavior, convergence, and divergence cases - Programming Ocean Academy\\rAspect\\rLow Learning Rate (Convergence)\\rHigh Learning Rate (Divergence)\\rOptimal Learning Rate (Convergence)\\rLearning Rate\\rVery small (e.g., 0.001)\\rToo high (e.g., 10)\\rBalanced (e.g., 0.01 - 0.1)\\rLoss Function Behavior\\rDecreases slowly over many iterations\\rLoss fluctuates or increases dramatically\\rDecreases steadily and reaches a minimum\\rConvergence\\rSlow convergence to the minimum, stable but takes longer\\rNo convergence, model fails to find the minimum, unstable\\rQuick convergence with stable optimization\\rDivergence\\rNone, but may take excessive time to optimize\\rHigh, model parameters oscillate or overshoot\\rNone, learning is balanced and efficient\\rTraining Loss\\rGradually decreases but can plateau for long periods\\rFluctuates wildly, increases or stays high\\rDecreases steadily, reaching low values\\rTest Loss\\rLow, but improvement is slow\\rHigh, indicating poor generalization\\rLow, with efficient training and good generalization\\rDecision Boundary\\rSmooth and correct, but may be delayed\\rErratic and inaccurate, model overfits or diverges\\rWell-defined and optimal for the dataset\\rWeight Adjustments\\rSmall, steady adjustments, slow progress\\rLarge adjustments, causing instability\\rAppropriate, balanced adjustments\\rTraining Behavior\\rStable but slow learning process\\rUnstable, unable to learn effectively\\rEfficient and stable learning process\\r\\r\\n\\r\\n\\r\\n\\r\\nA comparison of the most common metrics used to evaluate classification AI models- Programming Ocean Academy\\rMetric\\rDefinition\\rUse Case\\rFormula\\rRange\\rInterpretation\\rAccuracy\\rThe proportion of correctly classified instances (both true positives and true negatives) out of the total instances.\\rGood for balanced datasets.\\rAccuracy=  (TP+TN)/(TP+TN+FP+FN)\\r0 to 1\\rHigher values indicate better performance.\\rPrecision\\rThe proportion of true positive predictions out of all positive predictions. Measures how many selected items are relevant.\\rImportant when false positives are costly.\\rPrecision =  TP/(TP + FP)\\r0 to 1\\rHigh precision means fewer false positives.\\rRecall (Sensitivity)\\rThe proportion of actual positives correctly classified. Measures how many relevant items are selected.\\rImportant when false negatives are costly.\\rRecall (Sensitivity)=  TP/(TP+FN)\\r0 to 1\\rHigh recall means fewer false negatives.\\rF1 Score\\rThe harmonic mean of precision and recall. Provides a balanced measure when classes are imbalanced.\\rUseful in imbalanced datasets.\\rF1 Score=2 x   (Precision x Recall)/(Precision+Recall)\\r0 to 1\\rHigher F1 indicates a better balance between precision and recall.\\rSpecificity\\rThe proportion of true negatives correctly classified. Focuses on how well the model identifies negative instances.\\rCritical when detecting negatives is important.\\rSpecificity=  TN/(TN+FP)\\r0 to 1\\rHigh specificity means fewer false positives.\\r\\r\\n\\r\\n\\r\\nA comparison of the most common metrics used to evaluate classification AI models- Programming Ocean Academy\\rMetric\\rDefinition\\rUse Case\\rFormula\\rRange\\rInterpretation\\rROC-AUC\\rThe Area Under the Receiver Operating Characteristic Curve. Measures the ability of the model to discriminate between classes across various threshold settings.\\rUseful to compare models regardless of thresholds.\\r(ROC-AUC does not have a specific formula for direct calculation in classification, but it measures the area under the ROC curve.)\\r0.5 to 1\\rHigher AUC indicates better discrimination between positive and negative classes.\\rLog Loss (Cross-Entropy Loss)\\rMeasures the uncertainty of predictions by comparing predicted probabilities with actual class labels.\\rImportant in probabilistic classification models.\\rLog Loss=-  1/N  ?Â¦?(y log(p)+?  (1-y)log(1-p)\\r0 to 8\\rLower values indicate more accurate probabilistic predictions.\\rBalanced Accuracy\\rA modification of accuracy, which takes the average of recall obtained on each class, especially useful in imbalanced datasets.\\rBest for imbalanced datasets.\\rBlanaced Accuracy=  (Recall Positive+Recall Negative)/2\\r0 to 1\\rHigher balanced accuracy means better performance across all classes.\\rCohen\\'s Kappa\\rA metric that accounts for chance agreement between true and predicted classifications.\\rUse for assessing agreement beyond chance.\\rK=  (Po-Pe)/(1-Pe)\\r-1 to 1\\rA value of 1 represents perfect agreement; 0 represents no agreement beyond chance; negative values indicate worse-than-random performance.\\r\\r\\n\\r\\n\\r\\nA comparison of the most common metrics used to evaluate classification AI models- Programming Ocean Academy\\rMetric\\rDefinition\\rUse Case\\rFormula\\rRange\\rInterpretation\\rMatthews Correlation Coefficient (MCC)\\rConsiders true and false positives and negatives. Balanced and informative even for imbalanced datasets.\\rUseful for highly imbalanced datasets.\\rMCC=  ((TP x TN)-(FP x FN))/v((TP+FP)(TP+FN)(TN+FP)(TN+FN))\\r-1 to 1\\r1 indicates perfect prediction; 0 is no better than random; -1 indicates total disagreement between prediction and reality.\\rG-Mean\\rThe geometric mean of sensitivity (recall) and specificity. Focuses on both the true positive and true negative rates.\\rImportant in imbalanced datasets.\\rG-MEAN= v(Recall x Sensitivity)\\r0 to 1\\rA higher G-Mean indicates better balance between identifying positives and negatives.\\rF2 Score\\rA variation of the F1 score, giving more weight to recall than precision. Useful when false negatives are more critical than false positives.\\rEmphasizes recall over precision.\\rF2=?(1+2)?^2  x  (Precision x Recall)/(?(2?^(2 ) x Precision)+Recall)\\r0 to 1\\rA higher value indicates a better balance, with more importance on recall.\\rFalse Positive Rate (FPR)\\rThe proportion of actual negatives that are incorrectly classified as positive.\\rUseful when false positives are highly undesirable.\\rFPR=  FP/(FP+TN)\\r0 to 1\\rLower FPR indicates fewer false positives.\\rFalse Negative Rate (FNR)\\rThe proportion of actual positives that are incorrectly classified as negative.\\rUseful when false negatives are costly.\\rFNR =  FN/(FN+TP)\\r0 to 1\\rLower FNR indicates fewer false negatives.\\r\\r\\n\\r\\n\\r\\n\\r\\nA comparison of the most common metrics used to evaluate regression (Predictions) AI models- Programming Ocean Academy\\rMetric\\rDefinition\\rUse Case\\rFormula\\rRange\\rInterpretation\\rMean Absolute Error (MAE)\\rThe average magnitude of absolute errors between predicted and actual values.\\rSuitable for models where error magnitude matters and the direction of errors (positive/negative) is not important.\\rMAE =  (?_(i=1)^nÂ¦?|y_(i -   ) y ~_(i  ) |?)/n\\r[0, 8)\\rLower MAE means the model\\'s predictions are closer to the actual values, indicating better performance.\\r\\nHigher MAE indicates larger average errors, meaning the model is less accurate.\\rMean Squared Error (MSE)\\rThe average of the squared differences between predicted and actual values, giving more weight to larger errors.\\rUse when you want to penalize large errors more heavily, making the model focus on reducing big mistakes.\\rMSE =1/n  ?_(i=1)^nÂ¦?(Y_(i-  ) Y ~_i )^2 ?\\r[0, 8)\\rLower MSE indicates better performance; large errors are more penalized due to squaring.\\rRoot Mean Squared Error (RMSE)\\rThe square root of MSE, providing errors in the same units as the target variable.\\rUse when you want a metric in the same unit as the predicted variable while still penalizing large errors.\\rRMSE =v((?_(I=1)^NÂ¦?(X_(I-    ) (X_I ) ~  )^2 ?)/N)\\r[0, 8)\\rSimilar to MSE but easier to interpret since it\\'s in the same units as the data.\\rR-squared (RÂ²)\\rThe proportion of the variance in the dependent variable explained by the independent variables in the model.\\rUse to understand how well your model explains the variability of the data.\\rR^2  =1-  RSS/TSS\\r\\nR^2=1-  (?_IÂ¦?(Y_(I )- (Y_i ) ~  )^2 ?)/(?_iÂ¦?( Y_(I ) ?- (Y ) ~)^2 )\\r[0, 1]\\rHigher RÂ² values indicate that the model explains more of the variance in the target variable.\\r\\r\\n\\r\\n\\r\\n\\r\\nThe main types of machine learning - Programming Ocean Academy\\rType of Machine Learning\\rDefinition\\rData Requirement\\rUse Cases\\rOutput\\rExamples\\rSupervised Learning\\rThe model is trained on labeled data, where the input and corresponding output (target) are known.\\rRequires large amounts of labeled data.\\rClassification (e.g., spam detection), regression (e.g., house price prediction).\\rPredicts labels or continuous values.\\rLinear Regression, Logistic Regression, Decision Trees, SVM, k-NN\\rUnsupervised Learning\\rThe model learns patterns and structures from data that has no labeled output, finding hidden structures and relationships.\\rDoes not require labeled data.\\rClustering (e.g., customer segmentation), anomaly detection, association rule learning (e.g., market basket analysis).\\rIdentifies patterns, groups, or relationships in the data.\\rK-Means Clustering, PCA (Principal Component Analysis), Hierarchical Clustering, Autoencoders\\rSemi-Supervised Learning\\rThe model is trained on a small amount of labeled data combined with a large amount of unlabeled data, leveraging both types for learning.\\rA mix of labeled and unlabeled data.\\rWeb content classification, image and video recognition, medical imaging analysis.\\rPredicts labels or uncovers patterns in the unlabeled data.\\rSemi-Supervised SVM, Self-training, Label Propagation\\rReinforcement Learning\\rThe model learns through trial and error by interacting with an environment, receiving rewards or penalties for actions, aiming to maximize cumulative reward over time.\\rNo labeled data, feedback through rewards/penalties.\\rGame playing (e.g., AlphaGo), robotics, autonomous vehicles, recommendation systems.\\rDecision-making policy or strategy to maximize rewards.\\rQ-Learning, Deep Q-Networks (DQN), Proximal Policy Optimization (PPO)\\r\\r\\n\\r\\nThe main types of Deep Learning architectures- Programming Ocean Academy\\rDeep Learning Type\\rDefinition\\rUse Cases\\rStrengths\\rExamples\\rFeedforward Neural Networks (FNN)\\rThe simplest type of neural network, where information flows in one direction from input to output.\\rImage classification, speech recognition, basic predictive tasks.\\rEasy to implement, works well for simple tasks.\\rMulti-Layer Perceptron (MLP), Shallow Networks.\\rConvolutional Neural Networks (CNN)\\rNeural networks designed to process and recognize patterns in grid-like data (e.g., images), using convolutional layers.\\rImage and video recognition, object detection, medical image analysis.\\rStrong spatial feature detection, great for image-based tasks.\\rLeNet, AlexNet, VGG, ResNet, Inception.\\rRecurrent Neural Networks (RNN)\\rNetworks with loops allowing information to persist, enabling sequential data processing.\\rTime series prediction, natural language processing (NLP), speech.\\rHandles sequential data and temporal dependencies well.\\rLong Short-Term Memory (LSTM), Gated Recurrent Units (GRU).\\rGenerative Adversarial Networks (GAN)\\rConsists of two neural networks (generator and discriminator) competing against each other to generate new data.\\rImage generation, deepfake creation, data augmentation, art generation.\\rCan create highly realistic data, used in unsupervised learning.\\rDCGAN, StyleGAN, CycleGAN.\\rAutoencoders\\rA type of neural network used for learning efficient codings by compressing and reconstructing data.\\rAnomaly detection, image noise reduction, data compression.\\rGood at dimensionality reduction and unsupervised learning tasks.\\rVariational Autoencoder (VAE), Denoising Autoencoder.\\rTransformer Networks\\rNeural networks designed for processing sequential data, using self-attention mechanisms to capture global dependencies.\\rMachine translation, text generation, NLP tasks, speech recognition.\\rEfficient for long-range dependencies, parallel processing.\\rGPT, BERT, Transformer, T5, Vision Transformer (ViT).\\rDeep Belief Networks (DBN)\\rProbabilistic generative models composed of multiple layers of stochastic, latent variables.\\rFeature extraction, pre-training deep networks, unsupervised learning.\\rGood at unsupervised learning, extracting hierarchical representations.\\rStacked Restricted Boltzmann Machine (RBM).\\rSelf-Organizing Maps (SOM)\\rA type of unsupervised neural network that projects high-dimensional data onto a low-dimensional grid.\\rData visualization, clustering, dimensionality reduction.\\rGood for data visualization and understanding of complex patterns.\\rKohonen Self-Organizing Map.\\r\\r\\n\\r\\n\\r\\nSummary of Common Trade-offs in AI Models Evaluation Metrics - Programming Ocean Academy\\rMetric Pair\\rTrade-off\\rAccuracy vs. Precision/Recall\\rAccuracy might be misleading on imbalanced data; precision and recall may not increase together.\\rPrecision vs. Recall\\rIncreasing one typically decreases the other, depending on the tolerance for false positives vs. false negatives.\\rBias vs. Variance\\rHigh bias leads to underfitting; high variance leads to overfitting. Balancing them is key for generalization.\\rMSE vs. MAE\\rMSE penalizes large errors more, while MAE treats all errors equally; choice depends on how important outliers are.\\rSpeed vs. Accuracy\\rMore accurate models are often slower and more resource-intensive; simpler models are faster but may be less accurate.\\r\\r\\n\\r\\nThe key concepts of Supervised Machine Learning- Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rSupervised learning is a type of machine learning where the model is trained on labeled data (input-output pairs).\\rGoal\\rTo learn a mapping function from input features (X) to the target output (Y) and make accurate predictions on new, unseen data.\\rInput Data\\rLabeled data, meaning each training example consists of both input data and the corresponding correct output label.\\rOutput\\rPredictions or classifications made by the model based on the input data.\\rTraining Process\\rThe model adjusts its internal parameters based on the labeled data to minimize the error (loss function) between predictions and actual labels.\\rTypes of Tasks\\r1. Classification - Predicts discrete categories (e.g., spam or not spam).\\r\\n2. Regression - Predicts continuous values (e.g., house prices).\\rCommon Algorithms\\r- Linear Regression (for regression)\\r\\n- Logistic Regression (for classification)\\r\\n- Support Vector Machine (SVM) \\r\\n- Decision Trees\\r\\n- Random Forest\\r\\n- K-Nearest Neighbors (KNN)\\r\\n- Neural Networks\\rLoss Function\\rMeasures the difference between the predicted output and the actual output. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy for classification.\\rTraining Phase\\rThe model is fed labeled data to learn from. The goal is to minimize the loss function using algorithms like gradient descent.\\rTesting Phase\\rAfter training, the model is evaluated on new data (test data) to see how well it generalizes to unseen examples.\\rEvaluation Metrics\\r- Accuracy (for classification tasks)\\r\\n- Precision and Recall (for imbalanced classification)\\r\\n- Mean Absolute Error (MAE) (for regression)\\r\\n- R-squared (for regression)\\rAdvantages\\r- Models can be highly accurate if trained on sufficient and representative data.\\r\\n- Provides direct feedback by comparing predictions to actual labels.\\rDisadvantages\\r- Requires large amounts of labeled data, which can be expensive to collect.\\r\\n- May not generalize well to new, unseen data if the model overfits.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Unsupervised Machine Learning - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rUnsupervised learning is a type of machine learning where the model is trained on data without explicit labels or supervision.\\rGoal\\rTo identify patterns, structures, or relationships within data without predefined output labels.\\rInput Data\\rUnlabeled data, meaning the training examples consist only of input data without corresponding target labels.\\rOutput\\rPatterns, clusters, or reduced dimensions, providing insights into the structure of the data.\\rTraining Process\\rThe model identifies hidden patterns in the data, like clusters or associations, based on the input features alone.\\rTypes of Tasks\\r1. Clustering - Grouping similar data points (e.g., segmenting customers).\\r\\n2. Dimensionality Reduction - Reducing the number of features while preserving data variability (e.g., PCA).\\rCommon Algorithms\\r- K-Means Clustering\\r\\n- Hierarchical Clustering\\r\\n- DBSCAN\\r\\n- Principal Component Analysis (PCA)\\r\\n- t-SNE (for visualization)\\r\\n- Autoencoders\\rLoss Function\\rNo explicit loss function is used as there are no labels, but models often minimize some distance measure (e.g., Euclidean distance in clustering).\\rTraining Phase\\rThe model explores the structure of the input data, organizing it based on similarity or other characteristics, without needing labels.\\rTesting Phase\\rGenerally, unsupervised models are not evaluated with test data, but the quality of patterns (like cluster coherence) may be assessed using metrics like Silhouette Score.\\rEvaluation Metrics\\r- Silhouette Score (for clustering)\\r\\n- Inertia (sum of squared distances from the cluster center)\\r\\n- Variance Explained (for dimensionality reduction)\\rAdvantages\\r- No need for labeled data, which can be difficult or expensive to obtain.\\r\\n- Useful for exploring hidden structures and gaining insights from raw data.\\rDisadvantages\\r- Hard to evaluate since there is no ground truth for comparison.\\r\\n- Results can be harder to interpret, and performance depends on the specific problem and data.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Semi-Supervised Machine Learning- Programming Ocean Academy\\rAspect\\t\\rExplanation\\rDefinition\\rSemi-supervised learning combines elements of both supervised and unsupervised learning, using a small amount of labeled data and a large amount of unlabeled data.\\rGoal\\rTo leverage both labeled and unlabeled data to improve model performance while reducing the need for extensive labeled datasets.\\rInput Data\\rA combination of labeled data (typically a small subset) and a larger amount of unlabeled data.\\rOutput\\rPredictions or classifications, similar to supervised learning, but informed by both labeled and unlabeled data.\\rTraining Process\\rThe model is initially trained on the labeled data, then leverages patterns in the unlabeled data to refine and improve its predictions.\\rTypes of Tasks\\r1. Classification - Predicting categories using a small labeled dataset with additional unlabeled examples.\\r\\n2. Regression - Predicting continuous values by leveraging both labeled and unlabeled datasets.\\rCommon Algorithms\\r- Self-training\\r\\n- Co-training\\r\\n- Transductive SVM (TSVM)\\r\\n- Generative Models\\r\\n- Graph-based Methods\\rLoss Function\\rCombines supervised loss (e.g., Cross-Entropy for classification) on labeled data with an unsupervised component (e.g., entropy minimization or consistency regularization) for unlabeled data.\\rTraining Phase\\rInitially, the model learns from labeled data. Then it uses the structure in the unlabeled data to generalize and improve performance, often iteratively.\\rTesting Phase\\rLike supervised learning, the model is evaluated on a test set, usually consisting of labeled data, to measure generalization performance.\\rEvaluation Metrics\\r- Accuracy (for classification)\\r\\n- Precision/Recall\\r\\n- Root Mean Squared Error (RMSE) (for regression)\\r\\n- F1 Score\\rAdvantages\\r- Reduces the need for large amounts of labeled data.\\r\\n- Can significantly improve model performance by utilizing large amounts of unlabeled data.\\r\\n- Often more practical in real-world scenarios where labeling is costly or time-consuming.\\rDisadvantages\\r- Requires careful tuning of how labeled and unlabeled data are combined.\\r\\n- Unlabeled data can introduce noise or incorrect patterns if not handled correctly.\\r\\n- Complexity increases due to the hybrid approach.\\r\\r\\n\\r\\n\\r\\n\\r\\nThe key concepts of Reinforcement Machine Learning- Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rReinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties.\\rGoal\\rTo learn a policy that maximizes the cumulative reward by taking actions that lead to favorable outcomes over time.\\rInput Data\\rThe agent receives feedback from the environment in the form of states (representing the current situation) and rewards based on its actions.\\rOutput\\rA sequence of actions or decisions aimed at maximizing cumulative rewards (long-term success).\\rTraining Process\\rThe agent explores the environment and learns through trial and error. It updates its policy (decision-making strategy) based on the rewards or penalties received.\\rTypes of Tasks\\r1. Markov Decision Processes (MDPs) - Standard RL tasks with discrete states and actions.\\r\\r2. Games and Robotics - The agent interacts with dynamic environments, such as playing a game or controlling a robot.\\rCommon Algorithms\\r- Q-Learning\\r\\n- Deep Q-Networks (DQN)\\r\\n- Policy Gradient Methods\\r\\n- Actor-Critic Algorithms\\r\\n- Proximal Policy Optimization (PPO)\\rLoss Function\\rThe reward signal drives learning. The goal is to maximize the cumulative reward (discounted reward over time), often through techniques like Bellman equations.\\rTraining Phase\\rThe agent starts by exploring the environment and experimenting with different actions. Over time, it learns to take actions that maximize rewards using strategies like exploitation (choosing known good actions) and exploration (trying new actions).\\rTesting Phase\\rThe agent is evaluated based on how well it can maximize cumulative rewards in the environment after training. Typically, the agent\\'s performance is measured by the total reward it accumulates during testing.\\rEvaluation Metrics\\r- Total Reward (sum of rewards over time)\\r\\n- Convergence (how quickly the agent learns)\\r\\n- Exploration vs. Exploitation Tradeoff (balance between trying new actions and using learned actions)\\rAdvantages\\r- Useful for dynamic and complex environments where the agent learns through experience.\\r\\n- Can achieve superhuman performance in tasks like games or robotics.\\r\\n- Does not require labeled data, only a reward signal.\\rDisadvantages\\r- Training can be slow and computationally expensive.\\r\\n- Exploration can lead to suboptimal decisions or unsafe actions in real-world environments.\\r\\n- Requires careful tuning of reward structures and exploration strategies.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Feedforward Neural Networks (FNN) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rA Feedforward Neural Network (FNN) is the simplest type of artificial neural network where information flows in one direction-from input nodes, through hidden nodes (if any), to output nodes.\\rGoal\\rTo map input data to the appropriate output using a series of weighted connections, activation functions, and learned parameters.\\rInput Data\\rTypically structured data, such as numerical features, pixel values for images, or word embeddings for text.\\rOutput\\rDepends on the task:\\r\\n- Classification: Probabilities of class membership.\\r\\n- Regression: Continuous values.\\rArchitecture\\rComposed of an input layer, one or more hidden layers, and an output layer. Information moves forward through these layers.\\rTraining Process\\rUses backpropagation and an optimization algorithm (like Stochastic Gradient Descent) to update weights based on the difference between predicted output and actual output (error).\\rActivation Functions\\rCommon activation functions include:\\r\\n- ReLU (Rectified Linear Unit) for hidden layers.\\r\\n- Sigmoid or Softmax for output in classification tasks.\\r\\n- Linear for regression tasks.\\rLoss Function\\r- Mean Squared Error (MSE) for regression.\\r\\n- Cross-Entropy Loss for classification tasks.\\rTraining Phase\\rThe network is initialized with random weights. It learns through multiple epochs, adjusting weights by minimizing the error on the training data using backpropagation.\\rTesting Phase\\rOnce trained, the network is evaluated on a separate test dataset to assess its ability to generalize to new, unseen data.\\rEvaluation Metrics\\r- Accuracy (for classification tasks).\\r\\n- Mean Squared Error (MSE) or RÂ² (for regression).\\r\\n- Precision, Recall, F1-Score (for specific classification needs).\\rAdvantages\\r- Simple and efficient for structured data tasks.\\r\\n- Can approximate any continuous function given enough hidden units (Universal Approximation Theorem).\\rDisadvantages\\r- Prone to overfitting, especially with small datasets.\\r\\n- Not suitable for sequential or spatial data (e.g., images, time-series) without modifications like CNNs or RNNs.\\r\\n- Cannot capture complex relationships in data as effectively as other deep learning architectures.\\rUse Cases\\r- Simple classification tasks (e.g., binary classification or multi-class classification).\\r\\n- Basic regression tasks (e.g., house price prediction).\\r\\n- Feature extraction or dimensionality reduction.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Convolutional Neural Networks (CNN) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rConvolutional Neural Networks (CNN) are a class of deep learning networks designed to process grid-like data structures such as images by automatically learning spatial hierarchies of features.\\rGoal\\rTo efficiently recognize patterns and features in data (especially images) by using convolutional layers to detect spatial structures and reduce the number of parameters compared to fully connected networks.\\rInput Data\\rPrimarily image data (pixel grids), but also applicable to time series, video, and other structured data.\\rOutput\\rDepends on the task:\\r\\n- Classification: Predicts the probability of each class (e.g., image classification).\\r\\n- Localization/Detection: Identifies objects and their locations within images.\\r\\n- Regression: Predicts continuous values.\\rArchitecture\\rConsists of the following layers:\\r\\n- Convolutional Layers to extract features.\\r\\n- Pooling Layers for down sampling.\\r\\n- Fully Connected Layers for final decision-making.\\rConvolutional Layers\\rApplies filters (kernels) to the input data to create feature maps that capture essential features like edges, textures, and patterns.\\rPooling Layers\\rReduces the spatial dimensions of the feature maps (often using Max Pooling) to decrease computational load and focus on the most prominent features.\\rFully Connected Layers\\rConnects every neuron from the previous layer to the next, typically used at the end of the network for classification or regression tasks.\\rTraining Process\\rTrained using backpropagation and an optimizer (e.g., Adam or SGD). The network adjusts weights in both convolutional and fully connected layers by minimizing a loss function.\\rActivation Functions\\r- ReLU (Rectified Linear Unit): Commonly used after convolutional layers to introduce non-linearity.\\r\\n- Softmax: Used in the final layer for classification tasks to convert logits to class probabilities.\\rLoss Function\\r- Cross-Entropy Loss for classification tasks.\\r\\n- Mean Squared Error (MSE) for regression tasks.\\rTraining Phase\\rThe network learns filters and weights by passing data through the layers, minimizing the error over multiple iterations (epochs). Overfitting is often mitigated with dropout and data augmentation.\\rTesting Phase\\rAfter training, the network is evaluated on a separate test dataset to measure how well it generalizes to new, unseen data.\\rEvaluation Metrics\\r- Accuracy (for classification).\\r\\n- Precision, Recall, F1-Score (for object detection or segmentation).\\r\\n- IoU (Intersection over Union) (for localization tasks).\\rAdvantages\\r- Automatically detects important features in images (no manual feature engineering required).\\r\\n- Highly efficient for large image datasets.\\r\\n- Well-suited for image classification, object detection, and segmentation.\\rDisadvantages\\r- Computationally expensive, requiring high-performance GPUs.\\r\\n- Requires large amounts of labeled data for training.\\r\\n- Susceptible to overfitting without proper regularization.\\rUse Cases\\r- Image Classification: (e.g., recognizing objects in photos).\\r\\n- Object Detection: (e.g., identifying multiple objects in an image).\\r\\n- Image Segmentation: (e.g., medical image analysis).\\r\\n- Facial Recognition, Video Processing, and Autonomous Driving.\\r\\r\\n\\r\\n\\r\\n\\r\\nThe key concepts of Recurrent Neural Networks (RNN) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rRecurrent Neural Networks (RNN) are a class of neural networks designed to handle sequential data by maintaining a \"memory\" of previous inputs through loops within the network.\\rGoal\\rTo model time-dependent or sequential relationships in data, such as predicting future values based on past inputs or understanding sequential patterns.\\rInput Data\\rSequential data such as time series, text (sentences), speech, or video frames.\\rOutput\\r- Many-to-One: One output for the entire sequence (e.g., sentiment analysis).\\r\\n- Many-to-Many: One output for each input in the sequence (e.g., machine translation, video frame prediction).\\rArchitecture\\rConsists of recurrent layers where the output of a neuron is fed back into the input for the next step. This creates loops that allow the network to maintain information about past inputs.\\rRecurrent Layers\\rEach recurrent neuron has connections to the next neuron as well as feedback connections to itself, allowing it to retain information from previous time steps.\\rTraining Process\\rTrained using backpropagation through time (BPTT), where the gradient is propagated backward through the sequence to update weights.\\rActivation Functions\\r- Tanh: Often used in hidden layers to introduce non-linearity.\\r\\n- Sigmoid: Used in the output layer for binary classification tasks.\\r\\n- Softmax: Used for multi-class classification.\\rLoss Function\\r- Cross-Entropy Loss for classification tasks.\\r\\r- Mean Squared Error (MSE) for regression tasks or continuous data predictions.\\rTraining Phase\\rThe network processes sequences of data step by step, updating weights by minimizing the loss using gradient descent techniques (e.g., SGD, Adam).\\rTesting Phase\\rThe model is evaluated on unseen sequences to assess how well it generalizes to new sequential data by using past information from the training phase.\\rEvaluation Metrics\\r- Accuracy (for classification tasks).\\r\\n- Perplexity (for language models).\\r\\n- Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) (for regression or time-series forecasting).\\rAdvantages\\r- Can handle variable-length input sequences.\\r\\n- Remembers previous inputs, making it suitable for tasks like language modeling, machine translation, and time-series forecasting.\\rDisadvantages\\r- Vanishing/exploding gradient problem, which makes it hard to learn long-term dependencies.\\r\\n- Slower training due to sequential processing.\\r\\n- Prone to overfitting, especially on small datasets.\\rVariants\\r- LSTMs (Long Short-Term Memory networks): Designed to combat the vanishing gradient problem by learning long-term dependencies.\\r\\n- GRUs (Gated Recurrent Units): A simpler version of LSTMs with fewer parameters.\\rUse Cases\\r- Time-Series Forecasting: (e.g., stock price prediction, weather forecasting).\\r\\n- Natural Language Processing (NLP): (e.g., language translation, text generation).\\r\\n- Speech Recognition and Video Processing.\\r\\r\\n\\r\\nThe key concepts of Generative Adversarial Networks (GANs) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rGenerative Adversarial Networks (GANs) are a type of deep learning model composed of two networks, a generator and a discriminator, that compete with each other. The generator creates new data instances, while the discriminator evaluates them.\\rGoal\\rThe goal of GANs is to generate new, realistic data samples (e.g., images, audio, or text) that are indistinguishable from real data. The generator aims to fool the discriminator, while the discriminator learns to distinguish between real and fake data.\\rInput Data\\rInput to the generator is typically random noise (latent space vectors), while the discriminator takes both real data (from the training set) and fake data (generated by the generator).\\rOutput\\r- Generator Output: A data instance (e.g., an image) generated from random noise.\\r\\n- Discriminator Output: A probability score indicating whether the input data is real or fake.\\rArchitecture\\rGANs consist of two networks:\\r\\n- Generator Network: Learns to create realistic data by transforming random noise into meaningful samples.\\r\\n- Discriminator Network: Learns to distinguish between real and generated data, providing feedback to the generator.\\rTraining Process\\rGANs are trained in a two-step adversarial process:\\r\\n1. Discriminator Training: The discriminator is trained to correctly classify real vs. fake data.\\r\\n2. Generator Training: The generator is trained to produce data that fools the discriminator. Both networks are optimized iteratively.\\rLoss Function\\r- Binary Cross-Entropy Loss is commonly used for both the generator and the discriminator. The generator\\'s objective is to maximize the probability that the discriminator is mistaken.\\rAdversarial Process\\r- The discriminator and generator play a minimax game:\\r\\n- The generator tries to minimize the discriminator\\'s ability to correctly classify fake samples.\\r\\n- The discriminator tries to maximize its accuracy in distinguishing real from fake data.\\rTraining Challenges\\r- Mode collapse: The generator may learn to produce a limited variety of samples.\\r\\n- Training instability: GANs are challenging to train because the generator and discriminator must remain balanced.\\r\\n- Sensitive to hyperparameters and learning rate.\\rEvaluation Metrics\\r- Inception Score (IS): Measures the quality and diversity of generated samples.\\r\\n- Frechet Inception Distance (FID): Quantifies the similarity between real and generated data distributions.\\rAdvantages\\r- Capable of generating highly realistic data.\\r\\n- Applicable to various tasks like image generation, style transfer, and data augmentation.\\r\\n- GANs can learn unsupervised from unlabeled data.\\rDisadvantages\\r- Difficult to train and often requires fine-tuning of hyperparameters.\\r\\n- Susceptible to mode collapse, where the generator produces limited variations of data.\\r\\n- Computationally expensive, often requiring substantial resources.\\rVariants\\r- DCGAN (Deep Convolutional GAN): Uses CNNs for both the generator and discriminator, suited for image generation.\\r\\n- CycleGAN: Used for image-to-image translation without paired data (e.g., converting images from one domain to another).\\r\\n- WGAN (Wasserstein GAN): Improves GAN stability by using a different loss function based on Wasserstein distance.\\rUse Cases\\r- Image Generation: (e.g., generating realistic human faces, art, or synthetic images).\\r\\n- Style Transfer: (e.g., applying artistic styles to photos).\\r\\n- Data Augmentation: Generating new training samples for machine learning tasks.\\r\\n- Video Prediction, Text-to-Image Synthesis, and Super-Resolution.\\r\\r\\n\\r\\n\\r\\nThe key concepts of Transformer Neural Networks Models (TNN) - Programming Ocean Academy\\rAspect\\rExplanation\\rDefinition\\rTransformer Networks are a deep learning model architecture that relies on a self-attention mechanism, making them highly effective for processing sequential data (e.g., text, time-series) without requiring recurrent connections.\\rGoal\\rTo efficiently model long-range dependencies in sequential data by focusing on the most relevant parts of the input through the use of self-attention. Transformers are mainly used for tasks like machine translation, text generation, and natural language understanding.\\rInput Data\\rTypically, sequential data such as text, time-series, or even images (with modifications). Text data is often tokenized into word embeddings (e.g., Word2Vec, BERT embeddings).\\rOutput\\r- Many-to-One: Outputting a single prediction for the entire sequence (e.g., sentiment classification).\\r\\n- Many-to-Many: Outputting a sequence (e.g., machine translation, text generation).\\rArchitecture\\r- Encoder-Decoder Architecture: Consists of two main parts:\\r\\n- Encoder: Processes input data and generates a context-rich representation.\\r\\n- Decoder: Uses this representation to produce the output sequence.\\rSelf-Attention Mechanism\\rEach token (word, element) in the input sequence can \"attend\" to every other token in the sequence to capture dependencies, regardless of their distance from each other. This allows the model to understand contextual relationships effectively.\\rPositional Encoding\\rSince Transformers don\\'t have a built-in understanding of the order of sequence (like RNNs), positional encoding is added to the input embeddings to provide information about the order of tokens.\\rMulti-Head Attention\\rA key feature where the model applies the attention mechanism multiple times in parallel (heads), allowing the network to focus on different parts of the sequence simultaneously.\\rFeedforward Layers\\rAfter attention is applied, each output from the attention heads passes through fully connected feedforward layers, followed by normalization and residual connections to ensure effective learning.\\rTraining Process\\rTransformers are trained using backpropagation and gradient descent (often using the Adam optimizer). The loss function (typically Cross-Entropy Loss) is minimized for tasks like machine translation or text generation.\\rLoss Function\\r- Cross-Entropy Loss is most commonly used for language tasks such as text generation, translation, and sequence prediction.\\rAttention Heads\\rDifferent attention heads allow the network to focus on multiple parts of the input sequence at once. They compute scaled dot-product attention in parallel.\\rEvaluation Metrics\\r- Perplexity: Commonly used to measure how well the model predicts a sequence of text.\\r\\n- BLEU Score: Used for evaluating machine translation by comparing the predicted translation to a reference translation.\\rAdvantages\\r- Can model long-range dependencies effectively without the limitations of sequential processing (like RNNs).\\r\\n- Faster to train due to parallelization capabilities (self-attention is parallelizable).\\r\\n- Highly effective for tasks involving large datasets like NLP, translation, and summarization.\\rDisadvantages\\r- Computationally expensive, especially for long sequences due to the quadratic complexity of self-attention.\\r\\n- Requires large amounts of data to perform well (pre-training helps mitigate this).\\r\\n- Sensitive to hyperparameters like the number of layers and attention heads.\\rVariants\\r- BERT (Bidirectional Encoder Representations from Transformers): Focuses on pre-training using a masked language model approach for NLP tasks.\\r\\n- GPT (Generative Pre-trained Transformer): A generative model for text generation.\\r\\n- T5 (Text-to-Text Transfer Transformer): Converts all NLP tasks into a text-to-text framework.\\r\\n- Vision Transformer (ViT): Uses transformers for image recognition tasks.\\rUse Cases\\r- Machine Translation: (e.g., translating languages like English to French).\\r\\n- Text Summarization: Generating concise summaries of long documents.\\r\\n- Text Generation: (e.g., GPT models used for generating coherent paragraphs or stories).\\r\\n- Question Answering, Sentiment Analysis, Image Recognition (ViT).\\r\\r\\n\\r\\nA comparative table that outlines the architectures Of the Deep Learning AI Models - Programming Ocean Academy\\rArchitecture\\rStructure and Layers\\rInput Type\\rOutput Type\\rKey Features\\rCommon Applications\\rFeedforward Neural Networks (FNN)\\r\\n \\r- Composed of input, hidden, and output layers.\\r\\n- Each layer fully connected to the next.\\rFixed-size vectors\\r\\n \\rFixed-size vectors\\r\\n \\r- Simple structure, no feedback loops.\\r\\n- Activation functions used for non-linearity.\\rClassification, regression, basic pattern recognition.\\r\\n \\rConvolutional Neural Networks (CNN)\\r\\n \\r- Composed of convolutional layers, pooling layers, and fully connected layers.\\r\\n- Convolutional layers use filters (kernels).\\rImages (2D data)\\r\\n \\rClass labels, feature maps\\r\\n \\r- Local receptive fields.\\r\\n- Parameter sharing (weights are shared across space).\\rImage recognition, object detection, image segmentation.\\r\\n \\rRecurrent Neural Networks (RNN)\\r\\n \\r- Composed of recurrent layers where connections between nodes form directed cycles.\\r\\n- Includes LSTM and GRU variants for handling long-term dependencies.\\rSequential data (time series, text)\\r\\n \\rSequences or time-series predictions\\r\\n \\r- Maintains memory of previous inputs through hidden states.\\r\\n- Suitable for variable-length input.\\rNatural language processing, time series forecasting, speech recognition.\\r\\n \\rGenerative Adversarial Networks (GAN)\\r\\n \\r- Composed of two neural networks: generator and discriminator.\\r\\n- These networks compete against each other.\\rRandom noise (for generator)\\r\\n \\rGenerated data (e.g., images)\\r\\n \\r- Adversarial training process.\\r\\n- Generator creates data, discriminator evaluates it.\\rImage generation, data augmentation, unsupervised learning.\\r\\n \\rTransformer Networks (TNN)\\r\\n \\r- Composed of encoder and decoder layers with multi-head self-attention mechanisms.\\r\\n- Uses positional encoding to maintain sequence information.\\rSequences (text, time series)\\r\\n \\rSequences (text generation, translation)\\r\\n \\r- Parallel processing of input data.\\r\\n- Attention mechanism captures contextual relationships.\\rNatural language processing, translation, text summarization.\\r\\n \\r\\r\\nA summarizing the algorithms and techniques used in ChatGPT Part One- Programming Ocean Academy\\rCategory\\rAlgorithm/Technique\\rDescription\\rTransformers\\rSelf-Attention Mechanism\\rWeighs the importance of different words in a sentence for context understanding.\\r \\rMulti-Head Attention\\rAllows the model to focus on various parts of the input simultaneously.\\rOptimization Algorithms\\rAdam Optimizer\\rAdaptive learning rate optimization for weight updates during training.\\r \\rStochastic Gradient Descent (SGD)\\rUpdates weights incrementally based on mini-batches of data.\\rLoss Functions\\rCross-Entropy Loss\\rMeasures the difference between predicted probabilities and actual class labels.\\r \\rLabel Smoothing\\rSoftens target labels to improve generalization and prevent overconfidence.\\rTraining Techniques\\rTransfer Learning\\rPre-training on a large corpus followed by fine-tuning on a smaller dataset.\\r \\rReinforcement Learning from Human Feedback (RLHF)\\rUses human feedback to improve response quality and relevance.\\rRegularization Techniques\\rDropout\\rRandomly sets a portion of neurons to zero during training to prevent overfitting.\\r \\rWeight Decay\\rAdds a penalty based on the magnitude of model weights to control complexity.\\r\\r\\n\\r\\nA summarizing the algorithms and techniques used in ChatGPT Part Two- Programming Ocean Academy\\rCategory\\rAlgorithm/Technique\\rDescription\\rAttention Mechanisms\\rScaled Dot-Product Attention\\rCalculates attention scores based on the dot product of query and key vectors.\\r \\rAdditive Attention\\rCombines query and key vectors using a feedforward network to calculate attention weights.\\rSampling Techniques\\rTop-k Sampling\\rSelects the next word from the top k most probable words for variability.\\r \\rTop-p Sampling (Nucleus Sampling)\\rChooses from the smallest set of words whose cumulative probability exceeds a threshold (p).\\rLanguage Modeling Techniques\\rAutoregressive Modeling\\rPredicts the next word based on previous words for coherent text generation.\\r \\rMasked Language Modeling\\rPredicts missing words in a sentence to enhance pre-training.\\rEmbedding Techniques\\rWord Embeddings\\rConverts words into dense vectors capturing semantic meanings (e.g., Word2Vec, GloVe).\\r \\rPositional Encoding\\rAdds information about the position of words in the sequence.\\rData Augmentation Techniques\\rText Augmentation\\rGenerates additional training examples using techniques like synonym replacement or back-translation.\\rModel Distillation Techniques\\rKnowledge Distillation\\rTransfers knowledge from a large model (teacher) to a smaller model (student).\\r\\r\\n\\r\\nTypes of Models Used in ChatGPT Part One - Programming Ocean Academy\\rModel Type\\rDefinition\\rKey Features / Applications\\rTransformer Model\\rThe foundational architecture that utilizes self-attention mechanisms to process sequences of data.\\rParallel processing, attention mechanisms, and positional encodings.\\rEncoder-Decoder Model\\rA Transformer model with separate encoder and decoder components, originally designed for tasks like translation.\\rAlthough ChatGPT primarily uses the decoder-only version, the encoder-decoder architecture is crucial in understanding the evolution of Transformers.\\rDecoder-Only Model\\rA simplified Transformer model that uses only the decoder component, which is designed for generating text autoregressively.\\rChatGPT operates primarily as a decoder-only model for text generation.\\rFine-Tuned Language Models\\rPre-trained Transformer models that have been further trained on specific datasets for improved performance on targeted tasks.\\rChatGPT is fine-tuned on conversational datasets to enhance its ability to engage in dialogue.\\rLarge Language Models (LLMs)\\rHigh-capacity models that are capable of understanding and generating human-like text based on vast amounts of training data.\\rChatGPT belongs to the class of LLMs, leveraging large datasets to improve fluency and coherence.\\r\\r\\n\\r\\nTypes of Models Used in ChatGPT Part Two - Programming Ocean Academy\\rModel Type\\rDefinition\\rKey Features / Applications\\rPrompt Engineering\\rTechniques used to design and optimize input prompts to elicit better responses from language models.\\rEnhances the effectiveness of ChatGPT by guiding its responses through carefully constructed prompts.\\rReinforcement Learning from Human Feedback (RLHF)\\rA training technique where models are improved based on feedback from human users, often involving ranking or scoring outputs.\\rUsed in ChatGPT to refine its responses and align them with user expectations.\\rZero-Shot, Few-Shot Learning\\rApproaches that allow the model to perform tasks with little to no additional training data (zero-shot) or with a small amount of examples (few-shot).\\rChatGPT can generalize across tasks without explicit training for each one.\\rDistillation Techniques\\rMethods used to create smaller, more efficient versions of large models while retaining performance.\\rMay be applied in future iterations to optimize ChatGPT for deployment on various platforms.\\rMultimodal Models (Future Development)\\rModels that can process and generate multiple types of data (e.g., text, images, audio).\\rFuture versions of ChatGPT may integrate multimodal capabilities.\\r\\r\\nCommon Hyperparameters in Both Machine Learning and Deep Learning - Programming Ocean Academy\\rHyperparameter\\rContext\\rLearning Rate\\rUsed in both contexts to control the step size in optimization algorithms. In deep learning, it often requires careful tuning due to complex architectures.\\rBatch Size\\rRelevant in both contexts; in deep learning, batch size can significantly impact training time and model performance.\\rEpochs\\rApplies to both; refers to the number of times the entire dataset is passed through the model during training.\\rRegularization Strength\\rUsed in both contexts to prevent overfitting through L1 (Lasso) or L2 (Ridge) regularization techniques.\\rOptimizer\\rRelevant in both; different optimization algorithms can be used in both machine learning models and neural networks.\\rMomentum\\rPrimarily used in deep learning to accelerate the optimization process, particularly in stochastic gradient descent (SGD).\\r\\r\\n\\r\\nHyperparameters for Specific Algorithms Part One - Programming Ocean Academy\\rAlgorithm\\rHyperparameter\\rDescription\\rLinear Regression\\r\\n \\rRegularization Type\\r\\nAlpha\\rType of regularization applied (L1 or L2).\\r\\nRegularization strength.\\rDecision Trees\\r\\n \\r\\n \\r\\n \\rMax Depth\\r\\nMin Samples Split\\r\\nMin Samples Leaf\\r\\nMax Features\\rMaximum depth of the tree.\\r\\nMinimum number of samples required to split an internal node.\\r\\nMinimum number of samples required to be at a leaf node.\\r\\nNumber of features to consider when looking for the best split.\\rRandom Forest\\r\\n \\r\\n \\rNumber of Estimators\\r\\nBootstrap\\r\\nMax Features\\rNumber of trees in the forest.\\r\\nWhether to use bootstrap samples when building trees.\\r\\nNumber of features to consider at each split.\\rSupport Vector Machines (SVM)\\r\\n \\r\\n \\rC\\r\\nKernel Type\\r\\nGamma\\rRegularization parameter.\\r\\nType of kernel used (linear, polynomial, RBF, etc.).\\r\\nKernel coefficient for RBF, poly, and sigmoid kernels.\\rK-Nearest Neighbors (KNN)\\r\\n \\rNumber of Neighbors (k)\\r\\nDistance Metric\\rNumber of neighbors to consider for classification or regression.\\r\\nMetric used to measure distance (Euclidean, Manhattan, etc.).\\rNeural Networks\\r\\n \\r\\n \\r\\n \\r\\n \\rNumber of Layers\\r\\nNumber of Neurons\\r\\nActivation Functions\\r\\nDropout Rate\\r\\nWeight Initialization\\rTotal number of hidden layers.\\r\\nNumber of neurons in each layer.\\r\\nFunctions used to introduce non-linearity (ReLU, Sigmoid, Tanh, etc.).\\r\\nFraction of neurons to drop during training to prevent overfitting.\\r\\nMethod for initializing weights (Xavier, He, etc.).\\r\\r\\nHyperparameters for Specific Algorithms Part Two - Programming Ocean Academy\\rAlgorithm\\rHyperparameter\\rDescription\\rGradient Boosting Machines (GBM)\\r\\n \\r\\n \\r\\n \\rNumber of Estimators\\r\\nLearning Rate\\r\\nMax Depth\\r\\nSubsample\\rNumber of boosting stages.\\r\\nStep size shrinkage used to prevent overfitting.\\r\\nMaximum depth of individual trees.\\r\\nFraction of samples used for fitting individual base learners.\\rXGBoost\\r\\n \\r\\n \\r\\n \\rLearning Rate\\r\\nMax Depth\\r\\nSubsample\\r\\nColsample_bytree\\rControls the contribution of each tree.\\r\\nMaximum depth of a tree.\\r\\nProportion of samples used for training each tree.\\r\\nProportion of features used for training each tree.\\rLightGBM\\r\\n \\r\\n \\r\\n \\rLearning Rate\\r\\nNum Leaves\\r\\nBagging Fraction\\r\\nFeature Fraction\\rSimilar to XGBoost.\\r\\nMaximum number of leaves in one tree.\\r\\nProportion of data to use for training.\\r\\nProportion of features to use for training.\\rClustering\\r\\n \\r\\n \\rNumber of Clusters (k)\\r\\nDistance Metric\\r\\nInitialization Method\\rIn k-means clustering.\\r\\nType of distance measure used.\\r\\nMethod to initialize cluster centroids (e.g., k-means++).\\rNatural Language Processing\\r\\n \\r\\n \\r\\n \\rEmbedding Dimension\\r\\nSequence Length\\r\\nDropout Rate\\r\\nAttention Heads\\rDimensionality of word embeddings.\\r\\nLength of input sequences in RNNs.\\r\\nFor regularization in NLP models.\\r\\nIn transformer models, number of attention heads.\\rOther Hyperparameters\\r\\n \\rEarly Stopping\\r\\nGrid Search or Random Search\\rCondition to stop training when validation performance stops improving.\\r\\nMethods for hyperparameter optimization.\\r\\r\\n\\r\\nA comparison between binary classification and multi-class classification in terms of hyperparameters for AI model architecture-Programming Ocean Academy\\rAspect\\rBinary Classification\\rMulti-Class Classification\\rDefinition\\rInvolves classifying data into two distinct categories (e.g., spam vs. not spam).\\rInvolves classifying data into three or more categories (e.g., types of flowers).\\rOutput Layer\\rTypically consists of a single output neuron with a sigmoid activation function.\\rConsists of multiple output neurons (one for each class) with a softmax activation function.\\rLoss Function\\rCommonly uses Binary Cross-Entropy Loss to measure the model\\'s performance.\\rTypically uses Categorical Cross-Entropy Loss for multi-class tasks or Sparse Categorical Cross-Entropy when labels are one-hot encoded.\\rActivation Function\\rUses Sigmoid function in the output layer to produce a probability between 0 and 1.\\rUses Softmax function in the output layer to produce probabilities for each class that sum to 1.\\rEvaluation Metrics\\rCommonly uses accuracy, precision, recall, F1-score, and ROC-AUC for evaluation.\\rUses accuracy, precision, recall, F1-score (macro/micro averages), and confusion matrix for evaluation.\\rData Preparation\\rTypically requires binary labels (0 or 1) in the dataset for each instance.\\rRequires categorical labels (e.g., 0, 1, 2) for each instance in the dataset, often converted to one-hot encoding.\\rTraining Approach\\rOften utilizes simpler models and may converge faster due to fewer output categories.\\rCan be more complex and may require more training data to effectively model multiple classes.\\rRegularization Techniques\\rCommon techniques include L1/L2 regularization and dropout to prevent overfitting.\\rSimilar regularization techniques are applied, but may also include class balancing methods (e.g., class weights) to address imbalances among classes.\\rHyperparameter Tuning\\rTypically involves tuning parameters such as learning rate, batch size, number of epochs, and dropout rates.\\rIn addition to the parameters in binary classification, may include tuning the number of neurons per output layer, learning rate scheduling, and strategies for dealing with class imbalance (e.g., oversampling, undersampling).\\rCommon Algorithms\\rLogistic Regression, Support Vector Machines (SVM), Decision Trees, and Neural Networks.\\rSoftmax Regression, Decision Trees, Random Forests, Gradient Boosting Machines, and Neural Networks.\\r\\r\\n\\r\\nThe mathematical derivatives techniques used in machine learning and deep learning - Programming Ocean Academy\\rDerivative Technique\\rDefinition/Description\\rUse in Machine Learning / Deep Learning\\rGradient Descent\\rAn optimization algorithm that uses the gradient of the loss function to update parameters.\\rCommonly used for minimizing loss functions in various models, including linear regression and neural networks.\\rBackpropagation\\rA method for calculating the gradient of the loss function with respect to each weight in the neural network.\\rEssential for training neural networks by efficiently computing gradients for weight updates.\\rPartial Derivatives\\rDerivatives of multivariable functions with respect to one variable while holding others constant.\\rUsed in optimization problems to understand how changing one parameter affects the output.\\rJacobian Matrix\\rA matrix of all first-order partial derivatives of a vector-valued function.\\rHelps in understanding the sensitivity of outputs to changes in inputs, especially in deep learning models.\\rHessian Matrix\\rA square matrix of second-order partial derivatives of a scalar-valued function.\\rUsed to analyze the curvature of the loss function, aiding in optimization algorithms like Newton\\'s method.\\rChain Rule\\rA formula for computing the derivative of the composition of functions.\\rFundamental in backpropagation to compute gradients of nested functions in neural networks.\\rStochastic Gradient Descent (SGD)\\rA variant of gradient descent that uses a random subset of data to compute the gradient.\\rEfficiently updates model parameters in large datasets, commonly used in deep learning.\\rAdaptive Learning Rate Methods\\rTechniques like Adam, RMSprop that adjust the learning rate based on gradient statistics.\\rEnhances convergence speed and stability in training neural networks.\\rTaylor Series Expansion\\rAn approximation of a function as an infinite sum of terms calculated from the function\\'s derivatives at a single point.\\rHelps in optimizing functions by approximating them locally, useful in methods like Newton\\'s method.\\rSecond Derivative Test\\rA method to determine the concavity of functions and identify local minima or maxima.\\rHelps in understanding the optimization landscape and in implementing more sophisticated optimization algorithms.\\r\\r\\n\\r\\n\\r\\nThe different applications of computer vision across various industries- Programming Ocean Academy\\rApplication\\rDescription\\rContent Organization\\rIdentifies people or objects in photos to organize them, commonly used in photo storage and social media applications.\\rText Extraction\\rUses optical character recognition (OCR) to extract text from images for content discoverability and document processing in robotic process automation.\\rAugmented Reality\\rDetects and tracks physical objects in real-time to place virtual objects realistically in a physical environment.\\rAgriculture\\rAnalyzes images from satellites, drones, or planes to monitor harvests, detect weeds, and identify crop nutrient deficiencies.\\rAutonomous Vehicles\\rUses real-time object identification and tracking to gather information around the car, enabling self-driving cars to navigate and make decisions.\\rHealthcare\\rAnalyzes medical images (e.g., X-rays, MRIs) to assist doctors in identifying problems and making faster, more accurate diagnoses.\\rSports\\rTracks objects, players, and plays to assist with play analysis and strategy development in sports events.\\rManufacturing\\rMonitors machinery for maintenance, product quality, and packaging inspection on production lines using computer vision.\\rSpatial Analysis\\rIdentifies and tracks the movement of people or objects (e.g., cars) within a space for spatial analytics, useful in security or traffic management.\\rFace Recognition\\rIdentifies individuals based on their facial features, commonly used for security, authentication, and personalization applications.\\r\\r\\n\\r\\nAspect\\rGrayscale Images\\rColored Images\\rColor Channels\\r1 channel (intensity values)\\r3 channels (Red, Green, Blue - RGB)\\rPixel Values\\rEach pixel has a single value representing intensity (0-255).\\rEach pixel has three values representing RGB color intensities (0-255).\\rFile Size\\rSmaller file size since only one channel is stored.\\rLarger file size due to the need to store three channels.\\rComplexity\\rSimpler representation, easier for algorithms to process.\\rMore complex data due to the three channels, requiring more computation.\\rApplications\\rSuitable for tasks where color information is not critical (e.g., edge detection, document scanning).\\rEssential for tasks where color is important (e.g., object recognition, art analysis).\\rMemory Usage\\rLower memory usage due to single channel data.\\rHigher memory usage due to multiple channels (3x more than grayscale).\\rVisual Information\\rLimited to shades of gray, no color information.\\rRich in visual information, including a wide range of colors.\\rProcessing Time\\rFaster processing since there\\'s only one channel to analyze.\\rSlower processing due to multiple channels and increased complexity.\\rRepresentation\\rEach pixel represents only brightness or intensity.\\rEach pixel represents color through a combination of RGB intensities.\\rTraining Neural Networks\\rOften used in simpler models or when color is irrelevant.\\rUsed in more complex models where color plays a significant role in classification.\\r\\r\\n\\r\\nColor Channel Type\\rDescription\\rChannels\\rCommon Use Cases\\rAdvantages\\rDisadvantages\\rRGB (Red, Green, Blue)\\rRepresents images using three channels: Red, Green, and Blue. Colors are formed by combining different intensities of these channels.\\r3 (Red, Green, Blue)\\rMost common in digital displays, cameras, and web images.\\rWidely supported, intuitive representation for digital images.\\rLimited in representing real-world colors precisely.\\rRGBA (Red, Green, Blue, Alpha)\\rAdds an alpha channel to RGB to represent transparency. Each pixel has an opacity value.\\r4 (Red, Green, Blue, Alpha)\\rUsed in images where transparency is important (e.g., icons).\\rProvides support for transparency effects.\\rLarger file size due to the additional alpha channel.\\rCMYK (Cyan, Magenta, Yellow, Black)\\rA subtractive color model used in printing. Colors are formed by subtracting colors from white (paper background).\\r4 (Cyan, Magenta, Yellow, Black)\\rPrimarily used in color printing (magazines, posters, etc.).\\rAccurately represents printed colors.\\rNot used for displays, limited support in digital systems.\\rGrayscale\\rRepresents images with shades of gray, from black to white. It only measures light intensity.\\r1 (Gray)\\rUsed in black-and-white photography, document scanning.\\rSimple and efficient for tasks where color is irrelevant.\\rNo color information, limited to shades of gray.\\rYCbCr\\rA color space used in video compression. Y represents luminance, while Cb and Cr represent chrominance (color information).\\r3 (Y, Cb, Cr)\\rCommonly used in video encoding and broadcasting.\\rSeparates brightness and color data, efficient for video compression.\\rNot as intuitive as RGB for direct color manipulation.\\rLAB (CIELAB)\\rA color space that separates lightness (L) from chromaticity (A and B channels).\\r3 (L, A, B)\\rUsed in image editing, color correction, and perception studies.\\rMore perceptually uniform, better for color adjustments.\\rComputationally more complex, less intuitive than RGB.\\rHSB/HSV (Hue, Saturation, Brightness/Value)\\rA cylindrical representation of color, focusing on hue, saturation, and brightness.\\r3 (Hue, Saturation, Brightness/Value)\\rUsed in graphic design, color adjustments, and image editing.\\rMakes it easier to adjust hue and saturation independently of brightness.\\rNot ideal for image storage and transmission.\\rHSL (Hue, Saturation, Lightness)\\rSimilar to HSV but represents color with hue, saturation, and lightness.\\r3 (Hue, Saturation, Lightness)\\rUsed in image editing software, color pickers.\\rIntuitive for adjusting lightness separately from hue and saturation.\\rCan produce non-intuitive results for certain colors.\\r\\r\\n\\r\\n\\r\\n\\r\\nA table summarizing the different CNN architectures - Programming Ocean Academy\\rCNN Architecture\\rDescription\\rKey Features\\rCommon Use Cases\\rLeNet\\rOne of the earliest CNN architectures, developed by Yann LeCun, primarily for digit recognition.\\rSimple architecture with 5 layers, used for handwritten digit classification (MNIST dataset).\\rHandwritten digit recognition, image classification.\\rAlexNet\\rA breakthrough CNN architecture that won the 2012 ImageNet competition, brought CNNs into mainstream attention.\\rDeeper and wider than LeNet, uses ReLU activation, dropout, and data augmentation to reduce overfitting.\\rLarge-scale image classification (ImageNet).\\rVGG\\rKnown for its simplicity and depth, with popular configurations like VGG16 and VGG19.\\rUses small (3x3) convolution filters and a deep architecture (16-19 layers).\\rImage classification, object detection.\\rResNet\\rIntroduces residual connections (skip connections) to solve the vanishing gradient problem in deep networks.\\rEnables training of very deep networks (50+ layers), residual blocks make backpropagation more efficient.\\rImage classification, object detection, segmentation.\\rYOLO (You Only Look Once)\\rA state-of-the-art real-time object detection model that processes the entire image in one pass.\\rFast object detection, treats detection as a regression problem instead of a classification problem.\\rReal-time object detection, autonomous vehicles, video surveillance.\\r\\r\\n\\r\\nThe OpenAI Microscope for different vision models - Programming Ocean Academy\\rModel\\rDescription\\rNodes\\rAlexNet\\rLandmark model that won the 2012 ImageNet competition, widely cited in computer vision.\\r26\\rAlexNet (Places)\\rSame architecture as AlexNet, but trained on the Places365 dataset.\\r26\\rInception v1 (GoogLeNet)\\rSet the state of the art in ImageNet classification in 2014.\\r84\\rInception v1 (Places)\\rSame architecture as Inception v1, but trained on the Places365 dataset.\\r84\\rVGG 19\\rSimpler network introduced in 2014, uses only 3x3 convolutions and no branches.\\r27\\rInception v3\\rReleased in 2015, an improved and more efficient version of the Inception architecture.\\r127\\rInception v4\\rFourth iteration of the Inception architecture, released in 2016 with a focus on uniformity.\\r197\\rResNet v2 50\\rResNet variant with 50 layers, uses skip connections to help gradients propagate in deep networks.\\r58\\rCLIP ResNet 50 v0\\rOlder checkpoint of a ResNet 50 model trained with contrastive loss.\\r67\\rCLIP ResNet 50\\rStandard CLIP ResNet-50 architecture.\\r73\\rCLIP ResNet 101\\rCLIP model based on ResNet-101 architecture.\\r141\\rCLIP ResNet 50 4x\\rScaled-up version of CLIP ResNet-50, 4 times larger.\\r113\\rCLIP ResNet 50 16x\\rScaled-up version of CLIP ResNet-50, 16 times larger.\\r169\\r\\r\\nThe CNN architectures for Different Models-Part One- Programming Ocean Academy\\rCNN Architecture\\rDescription\\rKey Features\\rCommon Use Cases\\rAlexNet\\rLandmark model that won the 2012 ImageNet competition.\\rDeep architecture with 5 convolutional layers and ReLU activation.\\rImage classification, object detection.\\rAlexNet (Places)\\rAlexNet architecture trained on the Places365 dataset for scene recognition.\\rSame structure as AlexNet but optimized for different datasets.\\rScene classification, spatial context analysis.\\rInception v1 (GoogLeNet)\\rSet the state of the art in ImageNet classification in 2014 with a novel architecture.\\rUtilizes inception modules to efficiently handle multiple filter sizes.\\rImage classification, object detection, feature extraction.\\rInception v1 (Places)\\rInception v1 model trained on the Places365 dataset for scene analysis.\\rSame architecture as Inception v1, tailored for scene recognition.\\rScene classification, spatial analysis.\\rVGG 19\\rA simpler network introduced in 2014, known for its uniform architecture.\\rConsists of 16 convolutional layers with 3x3 filters and max pooling.\\rImage classification, transfer learning.\\rInception v3\\rAn improved version of Inception architecture, released in 2015 for enhanced performance.\\rIncreased depth and efficiency with more inception modules.\\rImage classification, fine-grained object detection.\\rInception v4\\rFourth iteration of the Inception architecture, focusing on uniformity and performance.\\rEven deeper with advanced inception modules for better feature extraction.\\rImage classification, advanced object detection.\\r\\r\\n\\r\\nThe CNN architectures for Different Models-Part Two- Programming Ocean Academy\\rCNN Architecture\\rDescription\\rKey Features\\rCommon Use Cases\\rResNet v2 50\\rResNet variant that uses skip connections to enable deeper networks without vanishing gradients.\\r50 layers with residual connections for gradient flow.\\rImage classification, object detection, and segmentation.\\rCLIP ResNet 50 v0\\rEarly checkpoint of ResNet 50 trained with contrastive loss for improved performance in understanding images and text.\\rIncorporates contrastive learning to enhance multimodal understanding.\\rMultimodal tasks, image-text associations.\\rCLIP ResNet 50\\rStandard CLIP model based on ResNet-50 architecture for image and text understanding.\\rSimilar to the ResNet 50 architecture with contrastive loss training.\\rImage and text classification, multimodal tasks.\\rCLIP ResNet 101\\rA deeper version of CLIP model based on ResNet-101 architecture for better performance.\\rMore layers enhance feature extraction capabilities.\\rAdvanced image-text associations, multimodal applications.\\rCLIP ResNet 50 4x\\rScaled-up version of CLIP ResNet-50, designed for improved performance in larger datasets.\\rFour times the original model size, leading to more capacity for learning.\\rLarge-scale image classification, complex tasks.\\rCLIP ResNet 50 16x\\rFurther scaled-up version of CLIP ResNet-50, maximizing model capacity and learning potential.\\rSixteen times the original model size, ideal for extensive datasets.\\rVery large-scale image and text tasks, advanced applications.\\r\\r\\n\\r\\nHow to calculate the number of parameters in the convolutional layer- Programming Ocean Academy\\rComponent\\rCalculation\\rValue\\rExplanation\\rFilter size (kernel)\\r3 x 3\\r9\\rEach filter is a 3x3 matrix, so 9 values per channel\\rInput channels\\r9 (weights per channel) x 3 channels\\r27\\rThe input has 3 color channels (RGB)\\rBias term\\r1 per filter\\r1\\rEach filter has one bias value\\rParameters per filter\\r27 (weights) + 1 (bias)\\r28\\rTotal number of parameters for one filter\\rNumber of filters\\r28 parameters/filter x 32 filters\\r896\\r32 filters, each with 28 parameters\\r\\r\\n\\r\\nLayer Summary - Programming  Ocean Academy\\rLayer\\rPurpose\\rConv2D (32 filters)\\rExtracts low-level features from the input image\\rMaxPooling2D\\rReduces spatial dimensions, retaining important features\\rConv2D (64 filters)\\rExtracts more complex features\\rMaxPooling2D\\rFurther reduces the size of feature maps\\rConv2D (64 filters)\\rExtracts deeper, more abstract features\\rFlatten\\rFlattens the output to a 1D vector for fully connected layers\\rDense (64 neurons, ReLU)\\rCombines features and learns the classification patterns\\rDense (10 neurons)\\rOutput layer that predicts the class (10 possible classes)\\r\\r\\nComparison between different types of Cross-Entropy Loss used in neural networks - Programming Ocean Academy\\rType\\rUse Case\\rInput Format\\rFormula (Loss Calculation)\\rKey Features\\rBinary Cross-Entropy\\r\\n \\r\\n \\rBinary classification (two classes, e.g., cat vs dog)\\r\\n \\r\\n \\rInput: one probability value per sample (output between 0 and 1), corresponding to class 0 or 1\\r\\n \\r\\n \\r- [y*log?(p)+(1-y)*log?(1-p)]\\r- Used for binary classification.\\r\\n- Target labels are 0 or 1.\\r\\n- Output activation function: Sigmoid.\\rCategorical Cross-Entropy\\r\\n \\rMulti-class classification (when classes are mutually exclusive)\\r\\n \\rInput: one-hot encoded vectors for target labels (e.g., [1, 0, 0] for class 0)\\r\\n \\r - ?_(i=1)^NÂ¦?y_I*log??(p_i ? ?)  \\r\\nwhere N is the number of classes\\r- Used for multi-class classification where each instance belongs to one class.\\r\\n- Softmax output layer.\\rSparse Categorical Cross-Entropy\\r\\n \\rMulti-class classification with sparse integer labels\\r\\n \\rInput: integer label (e.g., 0 for class 0) without one-hot encoding\\r\\n \\rSame as categorical cross-entropy, but integer labels are converted internally to one-hot vectors\\r\\n \\r- Works like categorical cross-entropy but without requiring one-hot encoding.\\r\\n- Convenient for large number of classes.\\r\\r\\n\\r\\n\\r\\nparameter of the tf.keras.layers.Conv2D function Part One- Programming Ocean Academy\\rParameter\\rType\\rDefault Value\\rDescription\\rfilters\\rint\\rNone\\rThe number of filters (or kernels) to use in the convolution operation. Each filter learns to detect different features in the input data.\\rkernel_size\\rtuple or int\\r(3, 3)\\rSpecifies the height and width of the 2D convolution window. Can be a single integer (for square kernels) or a tuple of two integers (for rectangular kernels).\\rstrides\\rtuple or int\\r(1, 1)\\rThe strides of the convolution along the height and width. This determines how much the filter moves across the input image. Default is 1 in both dimensions.\\rpadding\\rstr\\r\\'valid\\'\\rSpecifies the padding method: \\'valid\\' (no padding, leading to reduced output dimensions) or \\'same\\' (padding is added to keep output dimensions the same).\\rdata_format\\rstr\\rNone\\rThe format of the input data: \\'channels_last\\' (default, with shape (batch, height, width, channels)) or \\'channels_first\\' (with shape (batch, channels, height, width)).\\rdilation_rate\\rtuple\\r(1, 1)\\rSpecifies the dilation rate for dilated convolution. Dilation expands the kernel size effectively, allowing for a larger receptive field without increasing parameters.\\rgroups\\rint\\r1\\rControls the grouping of the input and output channels. Setting this value greater than 1 creates a grouped convolution, which can be useful for specific architectures like MobileNets.\\ractivation\\rcallable\\rNone\\rA function to apply as an activation function after the convolution. Common options include tf.keras.activations.relu, tf.keras.activations.sigmoid, etc.\\r- \\r\\nparameter of the tf.keras.layers.Conv2D function Part Two - Programming Ocean Academy\\rParameter\\rType\\rDefault Value\\rDescription\\ruse_bias\\rbool\\rTRUE\\rIndicates whether to include a bias term in the convolution operation. If set to False, the bias will not be added.\\rkernel_initializer\\rstr or callable\\r\\'glorot_uniform\\'\\rThe initializer for the kernel weights matrix. Common options include \\'he_normal\\', \\'glorot_uniform\\', etc., which determine how the initial weights are set.\\rbias_initializer\\rstr or callable\\r\\'zeros\\'\\rThe initializer for the bias vector. Default is to initialize biases to zero.\\rkernel_regularizer\\rcallable\\rNone\\rA function used to apply regularization to the kernel weights. Useful for preventing overfitting by adding penalties for large weights.\\rbias_regularizer\\rcallable\\rNone\\rA function used to apply regularization to the bias vector. Similar to kernel regularization, but for biases.\\ractivity_regularizer\\rcallable\\rNone\\rA function used to apply regularization to the output of the layer. This can be used to control the output value\\'s size and prevent overfitting.\\rkernel_constraint\\rcallable\\rNone\\rA function to constrain the kernel weights during optimization. This can be used to enforce weight constraints (e.g., non-negativity).\\rbias_constraint\\rcallable\\rNone\\rA function to constrain the bias during optimization. Similar to kernel constraints, this enforces constraints on the bias weights.\\rkwargs\\rdict\\rNone\\rAdditional keyword arguments passed to the layer. This can include parameters for other functionalities that might be specific to the user\\'s needs.\\r\\r\\n\\r\\nDefinition of Computer Vision and different uses of Computer Vision - Programming Ocean Academy\\rAspect\\rDefinition\\rUses of Computer Vision\\rDefinition\\rComputer Vision (CV) is a field of artificial intelligence (AI) that enables computers to interpret and process visual data (images, videos) similarly to human vision.\\r-\\rImage Classification\\rCV is used to classify images into categories based on their content.\\rExample: Sorting images into categories like cats, dogs, cars, etc. (e.g., Cats vs. Dogs project).\\rObject Detection\\rCV can detect objects within an image or video, often using bounding boxes.\\rExample: Detecting pedestrians, vehicles in autonomous driving, facial recognition, and security cameras.\\rImage Segmentation\\rCV divides an image into segments, identifying objects or regions of interest.\\rExample: Medical imaging to highlight tumors, or background removal in photography.\\rImage Recognition\\rIdentifying specific objects or people in an image.\\rExample: Face recognition systems (security, tagging in social media).\\rVideo Analysis\\rCV processes video data to recognize activities, movements, or changes over time.\\rExample: Surveillance, traffic analysis, or video summarization.\\rOptical Character Recognition (OCR)\\rExtracting and recognizing text from images or documents.\\rExample: Digitizing printed or handwritten text, such as scanning books or recognizing license plates.\\rPose Estimation\\rDetecting human body posture or movement from images or video.\\rExample: Fitness apps to analyze posture, motion capture for movies/games, AR applications.\\r3D Scene Reconstruction\\rReconstructing a 3D model or scene from multiple 2D images.\\rExample: Augmented reality, 3D mapping, or virtual reality environments.\\rAutonomous Vehicles\\rCV helps vehicles understand their surroundings in real time by processing visual data.\\rExample: Self-driving cars detecting traffic signs, pedestrians, lane boundaries, and other vehicles.\\rMedical Imaging\\rEnhancing and analyzing medical images for diagnostic purposes.\\rExample: MRI and CT scan analysis to detect diseases like cancer or fractures.\\rRetail and E-commerce\\rAnalyzing customer behavior, product images, and augmented reality fitting rooms.\\rExample: Virtual try-ons, analyzing product preferences, and in-store activity monitoring (Amazon Go).\\rAgriculture\\rCV helps monitor plant growth, detect pests, and assess crop health.\\rExample: Using drones to monitor large-scale farms, detecting diseases in crops from images.\\rManufacturing\\rAutomated inspection of products for quality control using CV systems.\\rExample: Detecting defects or errors in production lines, robotic assembly.\\rAugmented Reality (AR)\\rIntegrating digital objects into real-world environments using CV.\\rExample: AR filters on social media (e.g., Snapchat), real-time navigation apps, or interactive gaming.\\rRobotics\\rCV allows robots to perceive their environment and perform tasks autonomously.\\rExample: Robots performing warehouse sorting, cleaning, or object manipulation based on visual data.\\rContent Moderation\\rAutomatically moderating visual content on platforms.\\rExample: Detecting inappropriate images or videos for content moderation on social media platforms.\\r\\r\\n\\r\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the text dataset"
      ],
      "metadata": {
        "id": "nJ4ClWdSwNUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlYYhecXKVlC",
        "outputId": "dd7e0c22-cf03-4cce-cc4e-29be9db2b350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "# 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "\n",
        "url = 'https://programmingoceanacademy.s3.ap-southeast-1.amazonaws.com/Practical_labs/Data+Science+and+Data+Analysis+Tables+%26+comparisons.txt'\n",
        "response = requests.get(url)\n",
        "print(response.status_code)\n",
        "data = response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0WLjhhN2Qyo",
        "outputId": "909763d1-032b-4034-e59d-cc5b015a5749"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241660"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAEp2yEdwXI6",
        "outputId": "718c25ba-ba93-4ddf-85c1-7f940ccf07db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZthr7D3TJ69"
      },
      "source": [
        "## Convert the text data into tokens (Sub units)\n",
        "\n",
        "Tokenisation Types:\n",
        "1. Character\n",
        "2. Word\n",
        "3. SubWord Tokenisation\n",
        "4. Sentence-Level Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56UqiyC_LIOP",
        "outputId": "96310663-3b42-4f83-e2d5-e054a0708933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (53797 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[201, 198, 201, 198, 201, 198, 50249, 9886, 287, 6060, 5800, 290, 9552, 18892, 16544, 532, 30297, 10692, 8581, 12, 1238, 1731, 201, 198, 15167, 2229, 10692, 8581, 532, 6060, 520, 3643, 290, 28403, 201, 1722, 806, 201, 38824, 4803, 201, 43035, 201, 47, 541, 4470, 201, 38105, 201, 6601, 3981, 201, 30026, 3455, 201, 1273, 2850, 20793, 1366, 329, 3781, 201, 1273, 2850, 1111, 20793, 290, 555, 7249, 1522, 1366, 201, 8291, 69, 364, 1366, 422, 530, 1080, 284, 1194, 201, 1273, 2850, 20793, 1366, 18306, 201, 7004, 2617, 286, 257, 1366, 20933, 27571, 329, 2176, 1597, 1627, 393, 5011, 201, 6601, 5994, 201, 23828, 3093, 20793, 201, 10265, 20793, 290, 555, 7249, 1522, 201, 49321, 20793, 201, 44909, 1522, 1366, 201, 44909, 1522, 1366, 201, 29990, 201, 6090, 5412, 1588, 15343, 286, 1366, 201, 5960, 3916, 329, 4858, 1366, 6143, 201, 5960, 3916, 329, 1366, 3356, 201, 3351, 2040, 1864, 284, 5339, 201, 18712, 263, 24637, 286, 257, 1366, 20933, 201, 18709, 278, 201, 6747, 6211, 1366, 13389, 201, 6090, 3650, 1366, 1231, 3161, 32815, 6770, 201, 13470, 82, 1366, 5202, 1231, 29057, 2695, 201, 6747, 6211, 3716, 42517, 1112, 201, 37, 420, 2664, 319, 2176, 1597, 2476, 201, 15457, 23939, 201, 5569, 12, 38096, 329, 23696, 201, 5569, 290, 3551, 12, 38096, 201, 49321, 3551, 12, 38096, 11, 351, 9743, 1744, 201, 35608, 286, 1100, 290, 3551, 4560, 201, 51, 603, 1850, 329, 2176, 2985, 393, 13346, 201, 24220, 1387, 201, 37058, 1877, 24812, 329, 45069, 201, 20535, 24812, 329, 22534, 1366, 201, 20535, 24812, 329, 1366, 4351, 201, 37058, 1877, 24812, 4560, 201, 27871, 320, 1143, 329, 5443, 1895, 1626, 257, 2176, 7386, 201, 13383, 8219, 201, 39618, 27458, 9262, 201, 6747, 2421, 7044, 23989, 201, 13383, 8219, 8338, 319, 11523, 13357, 201, 39618, 3218, 9262, 201, 39618, 9262, 284, 10548, 351, 21568, 1597, 2476, 201, 16281, 201, 32, 6308, 1664, 338, 1366, 20933, 7000, 8611, 4406, 11, 6491, 1321, 11, 290, 13184, 1366, 13, 632, 26609, 290, 1618, 4340, 20793, 1366, 422, 2972, 4237, 588, 966, 12, 1659, 12, 21378, 3341, 11, 2691, 4200, 9554, 11, 290, 13184, 4542, 3788, 13, 16213, 6448, 779, 428, 1366, 20933, 284, 16602, 4200, 11257, 11, 6491, 4069, 11, 290, 13184, 4542, 10064, 13, 201, 32, 11409, 4009, 338, 1366, 13546, 7000, 257, 5909, 7177, 286, 1366, 3858, 11, 1390, 7914, 1535, 4406, 357, 7249, 1522, 828, 3315, 4263, 357, 403, 7249, 1522, 828, 5827, 12, 27568, 1366, 422, 5806, 2977, 357, 325, 11632, 12, 7249, 1522, 828, 290, 2267, 16125, 357, 403, 7249, 1522, 737, 26685, 290, 1366, 5519, 460, 1895, 428, 1366, 13546, 284, 1620, 6190, 23696, 11, 884, 355, 33344, 21128, 329, 4369, 13669, 393, 3265, 1535, 4542, 13, 201, 2025, 304, 12, 27061, 1664, 23986, 257, 1366, 11523, 284, 4351, 1103, 12, 2435, 4200, 1366, 422, 663, 2691, 3650, 284, 257, 29024, 1366, 20933, 13, 383, 11523, 26609, 48878, 1538, 1366, 11, 17706, 3306, 38226, 357, 68, 13, 70, 1539, 32784, 11, 36513, 828, 290, 15989, 262, 1366, 656, 262, 20933, 329, 3781, 13, 770, 11523, 19047, 326, 262, 1366, 318, 17282, 17609, 290, 1695, 329, 19376, 2551, 12, 8601, 13, 201, 32, 3176, 9901, 3544, 257, 50126, 6831, 4542, 1080, 357, 49, 11012, 5653, 8, 284, 3650, 6491, 11754, 1321, 11, 1390, 1848, 25223, 11, 8611, 2106, 11, 290, 8063, 3307, 13, 383, 6831, 6971, 3716, 42517, 1112, 329, 4568, 884, 355, 7394, 13326, 11, 2526, 8922, 11, 290, 6491, 2776, 4542, 13, 632, 19047, 1366, 11540, 11, 15794, 11, 290, 2324, 329, 4688, 11754, 4560, 13, 201, 22005, 257, 1588, 9138, 1664, 11, 262, 4200, 5011, 16047, 257, 1366, 11277, 2176, 284, 663, 4560, 13, 770, 4200, 1366, 11277, 4909, 1321, 319, 4200, 8945, 11, 6491, 30084, 11, 290, 1720, 2854, 20731, 5981, 284, 262, 4200, 1074, 338, 15221, 13, 2750, 10759, 319, 4200, 12, 5363, 1366, 290, 4955, 27571, 23696, 4899, 11, 262, 1366, 11277, 5419, 4200, 11663, 2610, 2854, 11, 11092, 4200, 11, 290, 27183, 7124, 10064, 329, 511, 1597, 4326, 13, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 532, 26947, 6374, 40, 39267, 16494, 2394, 34, 23518, 201, 18559, 5142, 201, 39, 4533, 404, 201, 39, 425, 201, 4561, 668, 201, 27313, 201, 12804, 6060, 25161, 201, 6601, 28593, 50028, 290, 43301, 15417, 201, 3118, 1431, 6060, 28403, 25161, 201, 6601, 28403, 9104, 201, 33, 963, 28403, 201, 33, 963, 28403, 357, 17861, 12, 2339, 20743, 8, 201, 33, 963, 290, 6416, 12, 7575, 28403, 201, 6601, 20514, 201, 39, 4533, 404, 4307, 6169, 9220, 4482, 357, 39, 8068, 50, 8, 201, 39, 8068, 50, 11, 3819, 9220, 11998, 201, 818, 12, 30871, 20514, 357, 4965, 346, 1153, 4307, 6169, 16092, 292, 1039, 532, 371, 16458, 82, 8, 201, 20746, 15417, 201, 39, 9711, 357, 39, 425, 43301, 15417, 8, 201, 39, 425, 9711, 357, 17861, 12, 2339, 8, 201, 4561, 668, 16363, 11, 11361, 11, 38334, 11, 7349, 11, 290, 517, 201, 36, 589, 286, 5765, 201, 5377, 11141, 11, 4433, 7349, 8300, 4678, 201, 17861, 12, 2339, 20743, 11, 4577, 329, 16363, 2985, 201, 15946, 1460, 23113, 287, 3294, 8950, 329, 10152, 286, 779, 201, 6601, 49127, 201, 37214, 1366, 13389, 9889, 201, 2767, 43, 9889, 329, 1366, 13389, 201, 11627, 2021, 1366, 13389, 9889, 201, 6601, 28403, 8729, 201, 11122, 789, 2233, 284, 15458, 7587, 201, 37, 1603, 621, 367, 4533, 404, 11, 23392, 329, 20743, 201, 818, 12, 31673, 7587, 329, 1029, 2866, 201, 11041, 35536, 201, 12804, 1366, 6143, 290, 15458, 7587, 201, 6601, 16202, 50028, 11, 512, 12, 71, 420, 42517, 1112, 201, 33, 963, 290, 1103, 12, 2435, 1366, 7587, 11, 4572, 4673, 11, 4823, 7587, 11, 290, 517, 201, 36, 12541, 201, 7841, 286, 262, 367, 4533, 404, 13187, 201, 7841, 286, 262, 367, 4533, 404, 13187, 201, 7841, 286, 262, 367, 4533, 404, 13187, 11, 475, 460, 307, 973, 14799, 201, 26198, 8549, 201, 39, 4533, 404, 338, 575, 1503, 45, 20857, 13511, 201, 34500, 9700, 351, 367, 4533, 404, 338, 8271, 4542, 201, 39582, 12, 259, 13946, 4706, 11, 27669, 4235, 1695, 201, 37, 1721, 309, 37668, 201, 15946, 1460, 8046, 15621, 832, 30330, 201, 15946, 1460, 8046, 15621, 832, 30330, 201, 15946, 1460, 8046, 15621, 832, 31144, 1321, 290, 1366, 664, 296, 1996, 341, 201, 15633, 12, 7575, 28403, 201, 3673, 11080, 329, 1103, 12, 2435, 7587, 201, 37214, 1103, 12, 2435, 9889, 201, 15979, 2096, 1103, 12, 2435, 7587, 832, 11305, 290, 15458, 12881, 201, 32273, 13932, 278, 201, 5377, 11141, 24549, 2672, 329, 16586, 2854, 201, 36, 292, 959, 2854, 24549, 201, 15946, 1460, 2972, 23989, 7605, 201, 201, 198, 6601, 28403, 351, 1115, 1593, 24843, 6060, 5800, 20003, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 25189, 4891, 367, 4533, 404, 201, 25189, 4891, 33235, 201, 25189, 4891, 17732, 201, 29064, 201, 32, 4947, 286, 4899, 329, 9387, 6143, 290, 7587, 286, 1263, 1366, 13, 201, 32, 1366, 20933, 3170, 319, 1353, 286, 367, 4533, 404, 329, 42517, 1112, 290, 11149, 1588, 40522, 13, 201, 32, 9387, 1366, 23696, 9355, 329, 3716, 1366, 7587, 287, 1103, 12, 2435, 13, 201, 13383, 36109, 201, 12, 367, 4533, 404, 4307, 6169, 9220, 4482, 357, 39, 8068, 50, 8, 201, 12, 44743, 319, 5572, 10652, 393, 584, 1366, 6143, 3341, 588, 24843, 367, 14881, 13, 201, 12, 554, 12, 31673, 7587, 3113, 351, 1104, 329, 3294, 8300, 8950, 13, 201, 31425, 4482, 201, 12, 5572, 10652, 25, 13341, 896, 1588, 3696, 656, 7021, 290, 1233, 7657, 1973, 3294, 13760, 13, 201, 12, 41835, 20150, 290, 12405, 2482, 13, 201, 12, 1680, 1895, 1366, 422, 2972, 4237, 11, 1390, 5572, 10652, 290, 33235, 13, 201, 6601, 49500, 201, 12, 7157, 829, 20793, 11, 10663, 12, 7249, 1522, 11, 290, 555, 7249, 1522, 1366, 13, 201, 12, 7157, 829, 1366, 8574, 287, 5572, 10652, 393, 367, 14881, 11, 23392, 329, 1588, 12, 9888, 20743, 13, 201, 12, 7157, 829, 14333, 23696, 11, 11305, 1366, 11, 4572, 4673, 11, 290, 12152, 43, 8861, 13, 201, 18709, 278, 201, 12, 347, 963, 7587, 11, 351, 10730, 29964, 319, 9387, 1366, 7021, 13, 201, 12, 4149, 12, 3106, 20743, 11, 11080, 329, 12152, 43, 290, 1366, 16202, 50028, 475, 407, 329, 1029, 12, 12287, 8611, 7587, 13, 201, 12, 6416, 12, 2435, 7587, 11, 287, 12, 31673, 2653, 602, 11, 42123, 284, 11898, 691, 618, 3306, 13, 201, 3351, 282, 1799, 201, 12, 1446, 2040, 9493, 11458, 416, 4375, 517, 13760, 284, 262, 13946, 13, 201, 12, 1446, 2040, 351, 367, 4533, 404, 338, 9387, 6143, 1080, 11, 475, 20743, 423, 1029, 24812, 13, 201, 12, 1446, 2040, 18306, 351, 663, 287, 12, 31673, 7587, 290, 460, 307, 973, 351, 27669, 393, 367, 4533, 404, 32966, 1586, 13, 201, 37, 1721, 309, 37668, 201, 12, 18407, 16856, 1366, 7021, 319, 3294, 13760, 284, 2948, 1366, 2994, 13, 201, 12, 37947, 298, 319, 367, 4533, 404, 338, 8046, 15621, 11, 475, 20743, 460, 307, 3105, 13, 201, 12, 40050, 12, 83, 13625, 415, 351, 287, 12, 31673, 1366, 7628, 13, 201, 32273, 201, 12, 6446, 12, 16803, 290, 9314, 11, 475, 460, 307, 3105, 2233, 284, 11898, 12, 3106, 6143, 290, 15458, 7587, 13, 201, 12, 3334, 24812, 2233, 284, 15458, 12, 17107, 7587, 290, 1588, 12, 9888, 1366, 20743, 13, 201, 12, 3334, 12, 12287, 2653, 602, 351, 287, 12, 31673, 7587, 11, 8868, 24812, 13, 201, 15633, 12, 2435, 4476, 5738, 201, 12, 1892, 3562, 329, 1103, 12, 2435, 7587, 13, 201, 12, 1892, 11080, 329, 1103, 12, 2435, 7587, 2233, 284, 1029, 24812, 13, 201, 12, 39198, 329, 1103, 12, 2435, 1366, 7587, 290, 11305, 23696, 13, 201, 11041, 35536, 201, 12, 520, 3255, 1588, 40522, 11, 1366, 16202, 50028, 11, 9041, 10084, 1366, 17519, 13, 201, 12, 12152, 43, 11, 6447, 11, 290, 1366, 3781, 810, 1029, 24812, 318, 10909, 13, 201, 12, 21365, 23696, 11, 11305, 1366, 7587, 11, 4572, 4673, 11, 290, 12152, 43, 8861, 13, 201, 15167, 2229, 42860, 201, 12, 11460, 3093, 7349, 12, 3106, 13, 201, 12, 16363, 12, 2339, 12405, 3303, 13, 201, 12, 45267, 7349, 11, 38334, 11, 11361, 11, 371, 11, 290, 16363, 13, 201, 34500, 1358, 201, 12, 1680, 307, 11521, 351, 2972, 4899, 290, 3341, 329, 1263, 1366, 7587, 13, 201, 12, 28477, 319, 367, 4533, 404, 11, 48105, 351, 5572, 10652, 290, 367, 14881, 13, 201, 12, 1680, 1057, 319, 1353, 286, 367, 4533, 404, 11, 290, 48105, 351, 5572, 10652, 11, 33235, 11, 290, 584, 1366, 4237, 13, 201, 201, 198, 201, 198, 201, 6601, 29269, 10854, 274, 532, 30297, 10692, 8581, 532, 6060, 5800, 18892, 16544, 201, 8600, 201, 11828, 201, 22362, 17148, 278, 6060, 29269, 28510, 201, 7248, 1598, 15221, 6402, 1575, 12, 48649, 3292, 12, 8210, 11, 9922, 11, 290, 37496, 286, 2482, 13, 201, 17563, 278, 6060, 201, 33234, 1958, 3081, 1366, 4237, 11, 6402, 2099, 11, 11500, 11, 290, 1575, 12, 10760, 6517, 13, 201, 6719, 36948, 6060, 201, 32657, 290, 35139, 8246, 1366, 11, 10829, 8563, 11, 9041, 4814, 1366, 11, 290, 13359, 11540, 13, 201, 8291, 15464, 6060, 201, 5841, 1958, 1366, 284, 4646, 13357, 393, 4197, 3781, 11, 1262, 7605, 588, 46500, 393, 11315, 13, 201, 1273, 3255, 6060, 201, 22658, 14434, 1366, 30835, 11, 5086, 36263, 1895, 329, 3781, 290, 13359, 6782, 13, 201, 44, 3191, 6060, 201, 44836, 2972, 3781, 5050, 290, 16113, 11, 1390, 32704, 11, 284, 23658, 7104, 11257, 13, 201, 36, 2100, 11927, 29269, 15691, 201, 14402, 2746, 2854, 11, 6431, 10171, 13829, 7538, 11, 290, 11629, 9404, 35139, 3781, 13, 201, 201, 198, 15167, 2229, 10692, 8581, 532, 6060, 5800, 18892, 16544, 12, 18630, 33, 12, 48609, 201, 1722, 806, 201, 24898, 2194, 11998, 201, 37, 22863, 46254, 201, 23416, 9498, 201, 38052, 287, 2972, 9554, 329, 10763, 11, 884, 355, 12074, 290, 3203, 201, 23828, 3093, 5625, 287, 6308, 11754, 290, 9604, 329, 31521, 24627, 4568, 201, 10267, 425, 201, 24898, 1571, 5981, 2695, 393, 3186, 284, 2985, 1912, 319, 511, 15387, 290, 4069, 201, 33234, 4035, 6196, 24627, 8945, 287, 1103, 12, 2435, 201, 25574, 6368, 201, 18274, 346, 4340, 16113, 588, 25408, 25431, 11, 2695, 12, 3106, 25431, 11, 290, 17593, 5766, 1634, 201, 29733, 82, 4572, 4673, 4981, 884, 355, 32172, 13326, 11, 28679, 4673, 11, 290, 555, 16668, 16149, 4673, 201, 6601, 26406, 201, 6892, 444, 319, 2836, 12213, 11, 10109, 11, 290, 6754, 4069, 284, 7716, 10763, 201, 37702, 12271, 8611, 1366, 11, 2836, 4069, 7572, 11, 290, 6754, 7394, 2663, 284, 4512, 4981, 201, 15633, 12, 2435, 201, 41248, 602, 460, 307, 7560, 287, 1103, 12, 2435, 1912, 319, 2836, 4028, 290, 15387, 201, 37, 22863, 13326, 14051, 287, 1103, 12, 2435, 284, 2952, 6056, 13678, 8945, 329, 2252, 3645, 201, 24749, 17677, 201, 35476, 1817, 2836, 1998, 290, 5732, 12352, 416, 4955, 28949, 10763, 201, 12621, 862, 2948, 3176, 9089, 416, 13720, 290, 47165, 24627, 4568, 287, 262, 11754, 6567, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 532, 6060, 5800, 18892, 16544, 12, 18630, 33, 532, 1238, 1731, 201, 1722, 806, 201, 37573, 18252, 201, 8645, 876, 9552, 201, 29744, 18252, 201, 36621, 201, 18274, 346, 4340, 16113, 284, 2193, 7572, 422, 1366, 290, 787, 16277, 393, 5370, 201, 37, 420, 2664, 319, 15453, 649, 1366, 1912, 319, 4499, 7572, 11, 17007, 7958, 1692, 16389, 201, 7004, 2617, 286, 4572, 4673, 11, 26490, 3294, 11685, 286, 17019, 7686, 329, 3716, 8861, 201, 34888, 201, 39156, 9278, 11, 1398, 6637, 11, 290, 32966, 1586, 201, 12443, 341, 286, 649, 1366, 11, 884, 355, 4263, 11, 2647, 11, 290, 2420, 201, 5377, 11141, 3912, 9465, 11, 4046, 9465, 11, 290, 2939, 7587, 201, 25574, 6368, 201, 42986, 28679, 4673, 11, 555, 16668, 16149, 4673, 11, 290, 37414, 4673, 201, 18274, 346, 4340, 4981, 588, 2980, 876, 1215, 690, 36098, 27862, 357, 45028, 82, 8, 290, 15965, 864, 5231, 6571, 19815, 364, 357, 11731, 23041, 8, 201, 6892, 444, 319, 17019, 3127, 45619, 588, 3063, 2122, 282, 17019, 7686, 357, 18474, 82, 8, 290, 42465, 17019, 7686, 357, 49, 6144, 82, 8, 201, 44357, 6060, 201, 39618, 15494, 1366, 329, 28679, 4673, 11, 290, 743, 779, 9642, 9608, 276, 1366, 329, 555, 16668, 16149, 4673, 201, 2898, 1328, 319, 1588, 15343, 286, 1366, 284, 2193, 10238, 7572, 290, 24570, 201, 2898, 1328, 319, 7667, 40522, 284, 27183, 2746, 10007, 290, 2193, 3716, 3033, 201, 26410, 201, 39156, 9278, 393, 1398, 6637, 1912, 319, 5128, 1366, 201, 8645, 689, 649, 1366, 8405, 1912, 319, 4499, 7572, 201, 26410, 82, 16277, 11, 1398, 6637, 11, 393, 7560, 2695, 1912, 319, 5128, 201, 41995, 201, 42559, 306, 973, 287, 2972, 18209, 11, 1390, 9604, 11, 11409, 11, 290, 7124, 201, 4677, 18511, 287, 1242, 5270, 11, 2695, 6282, 11, 290, 1366, 16339, 14374, 201, 38052, 287, 4046, 9465, 11, 2939, 7587, 11, 3288, 3303, 7587, 11, 290, 18284, 5059, 201, 5377, 1996, 864, 4333, 201, 39618, 10768, 284, 1029, 31350, 4133, 11, 6906, 319, 2746, 13357, 201, 11522, 1746, 2383, 31350, 1176, 11, 1690, 25137, 32516, 329, 3047, 201, 23037, 82, 7667, 31350, 4133, 2233, 284, 3716, 3127, 45619, 290, 1588, 40522, 201, 201, 198, 15167, 2229, 10692, 8581, 532, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 8979, 18980, 201, 11828, 201, 27275, 3969, 201, 13856, 320, 863, 8255, 9220, 5178, 1381, 201, 8206, 3696, 810, 1123, 1627, 468, 3815, 11266, 416, 257, 46728, 2676, 13, 1520, 64, 12, 25512, 515, 3815, 357, 7902, 23266, 8, 290, 7400, 12, 25512, 515, 3815, 357, 4694, 23266, 8, 389, 2219, 6096, 13, 201, 12, 4216, 320, 2676, 31555, 3815, 532, 8070, 46728, 270, 364, 25, 39650, 11, 7400, 11, 7633, 11, 11723, 2318, 11, 2272, 532, 371, 1666, 2380, 4406, 532, 3274, 5752, 1690, 4909, 5721, 24697, 201, 15905, 24134, 4946, 23735, 31843, 21760, 357, 55, 6561, 55, 8, 201, 55, 5805, 12, 3106, 30117, 5794, 4166, 416, 5413, 13, 1680, 3994, 3294, 2499, 258, 1039, 351, 1366, 8389, 656, 15274, 290, 15180, 13, 201, 12, 23735, 12, 3106, 532, 20401, 2499, 258, 1039, 287, 257, 670, 2070, 532, 371, 1666, 290, 15180, 16481, 1366, 532, 39794, 3994, 1366, 532, 23904, 9857, 284, 584, 5479, 532, 29454, 329, 2324, 532, 26003, 3613, 17412, 2438, 201, 11627, 27339, 2940, 929, 15417, 357, 55, 5805, 8, 201, 9704, 929, 3303, 351, 900, 3173, 329, 21004, 1366, 13, 1778, 4674, 329, 7216, 1321, 625, 262, 5230, 13, 201, 12, 2940, 929, 3303, 532, 4149, 540, 416, 5384, 290, 8217, 532, 39198, 329, 5230, 1366, 11478, 532, 1400, 2747, 18156, 15940, 588, 11532, 532, 19193, 290, 8300, 3303, 4795, 532, 45157, 6945, 1366, 7373, 1022, 3341, 201, 13924, 540, 16854, 18980, 357, 20456, 8, 201, 19246, 276, 416, 21771, 284, 1944, 4963, 4795, 286, 3788, 11, 6890, 11, 290, 5361, 3341, 13, 201, 12, 6013, 276, 416, 21771, 532, 16854, 5794, 532, 13362, 286, 3788, 14, 10424, 1574, 532, 48221, 942, 6414, 3359, 1973, 4410, 532, 8070, 306, 973, 287, 2742, 290, 3176, 4963, 532, 1680, 307, 973, 329, 1296, 1366, 5726, 201, 29584, 7391, 9515, 1892, 341, 357, 40386, 8, 201, 8206, 12, 3106, 1280, 3210, 329, 39573, 20793, 1366, 625, 262, 3992, 13, 201, 12, 8255, 12, 3106, 532, 4946, 3210, 532, 32112, 1522, 1366, 11478, 532, 15417, 12, 34750, 532, 4149, 540, 287, 597, 8300, 3303, 532, 16789, 284, 779, 532, 3082, 16873, 351, 2972, 22616, 532, 1778, 4674, 329, 7373, 1366, 286, 597, 2546, 290, 2099, 11, 1390, 6597, 290, 2008, 532, 8070, 306, 973, 287, 23113, 290, 3992, 2594, 329, 1366, 5163, 201, 201, 198, 15167, 2229, 10692, 8581, 532, 6601, 5800, 18892, 16544, 532, 18630, 33, 48609, 201, 40341, 45205, 286, 6060, 23782, 201, 36621, 201, 9218, 11045, 201, 35475, 15372, 201, 32, 1366, 11444, 7228, 8136, 284, 2761, 416, 22712, 1366, 357, 14261, 393, 1402, 8, 1262, 5035, 4899, 290, 48556, 6373, 6840, 13, 201, 35475, 15372, 357, 6601, 5800, 30396, 8, 201, 6601, 3783, 318, 644, 1366, 5519, 466, 13, 201, 25031, 7651, 7920, 201, 13798, 1586, 318, 644, 12037, 466, 26, 12470, 11, 1366, 3783, 318, 644, 1366, 5519, 466, 13, 201, 6187, 13, 18653, 17113, 4244, 201, 7469, 1127, 257, 1366, 11444, 355, 530, 508, 460, 1429, 1588, 40522, 18306, 11, 30086, 82, 13905, 4981, 11, 290, 4433, 1913, 18069, 4469, 13, 201, 6187, 13, 18653, 17113, 4244, 357, 6601, 5800, 30396, 8, 201, 6601, 3783, 318, 18876, 422, 7869, 11, 475, 4433, 257, 4469, 287, 37139, 11, 41443, 11, 290, 12867, 14, 14269, 3969, 13, 201, 6187, 13, 3208, 346, 201, 32, 1366, 11444, 318, 257, 13516, 286, 4678, 326, 460, 12116, 17218, 422, 1366, 290, 1560, 13206, 3923, 351, 1366, 13, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 27275, 2569, 201, 44909, 1522, 6060, 201, 13900, 72, 12, 44909, 1522, 6060, 201, 3118, 7249, 1522, 6060, 201, 36621, 201, 6601, 351, 257, 880, 12, 23211, 4645, 393, 32815, 13, 201, 6601, 351, 617, 26564, 6608, 11, 16523, 257, 5969, 32815, 13, 201, 6601, 1231, 257, 22024, 856, 4645, 393, 32815, 13, 201, 31425, 18980, 201, 1273, 1850, 287, 880, 12, 23211, 3897, 5356, 357, 19608, 18826, 737, 201, 34, 34574, 307, 8574, 287, 15274, 290, 15180, 588, 20083, 13, 201, 49321, 8574, 287, 3696, 393, 1400, 17861, 20083, 13, 201, 27730, 201, 17861, 20083, 11, 4104, 42011, 11, 2691, 5107, 13, 201, 55, 5805, 11, 19449, 11, 7237, 11, 13934, 3121, 2977, 13, 201, 13908, 5468, 11, 1919, 2056, 21318, 11, 4263, 11, 4963, 13, 201, 15457, 2247, 201, 36, 292, 813, 9857, 290, 15475, 351, 3210, 4899, 13, 201, 39618, 32096, 290, 10794, 329, 3781, 13, 201, 37288, 4433, 16976, 4899, 329, 3781, 13, 201, 32750, 201, 6090, 307, 15475, 1262, 3210, 1366, 3781, 5050, 13, 201, 39618, 27658, 32096, 290, 7587, 13, 201, 6747, 6211, 10107, 3781, 393, 16976, 4899, 13, 201, 6601, 9104, 201, 2782, 19079, 284, 257, 7368, 1366, 2746, 13, 201, 19242, 617, 26564, 6608, 475, 16523, 257, 20831, 32815, 13, 201, 2949, 34450, 4645, 393, 32815, 13, 201, 39, 959, 9282, 201, 7155, 82, 257, 7646, 18911, 13, 201, 4264, 1299, 15940, 290, 4847, 329, 36115, 1366, 13, 201, 2949, 11519, 18911, 26, 1366, 743, 307, 595, 30280, 13, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 6601, 8090, 201, 11828, 201, 27730, 201, 6892, 864, 16092, 18826, 201, 38052, 329, 23069, 20793, 1366, 351, 880, 12, 23211, 3897, 5356, 13, 201, 17861, 9652, 11, 18650, 11, 33476, 11, 19764, 20137, 17, 201, 7414, 265, 13283, 201, 22658, 1366, 287, 8631, 2420, 5794, 351, 4406, 11266, 416, 46728, 270, 364, 13, 201, 7902, 53, 3696, 201, 55, 5805, 16092, 292, 1039, 201, 4264, 391, 1366, 7498, 510, 1262, 15940, 11, 11080, 329, 38958, 1366, 10552, 13, 201, 14439, 16255, 11, 3331, 6299, 201, 2969, 3792, 290, 5313, 6168, 201, 15946, 485, 20314, 329, 22534, 290, 50122, 1366, 422, 2972, 4237, 13, 201, 14254, 7824, 11, 3203, 7824, 11, 4283, 1910, 23113, 201, 13908, 1446, 2416, 278, 201, 11627, 974, 82, 5981, 1366, 422, 555, 7249, 1522, 3992, 4237, 1912, 319, 5447, 10007, 13, 201, 38413, 4135, 50, 10486, 11, 1446, 2416, 88, 11, 16492, 292, 11, 15300, 47477, 201, 6601, 13860, 82, 201, 46384, 2301, 689, 6937, 15190, 286, 1366, 422, 4237, 588, 38488, 4410, 11, 5479, 11, 290, 1919, 2056, 13, 201, 25189, 4891, 46906, 11, 24843, 17732, 43124, 11, 24843, 8865, 201, 49, 5432, 18272, 82, 201, 19209, 942, 6153, 1366, 422, 2691, 14216, 290, 1705, 5043, 329, 11305, 284, 2836, 4410, 13, 201, 9980, 9293, 11, 2691, 14216, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 9218, 11045, 201, 22093, 201, 6892, 864, 16092, 18826, 201, 12, 26719, 2247, 286, 50126, 20083, 329, 20793, 1366, 13, 532, 44495, 2291, 3867, 1366, 1022, 1180, 20083, 290, 7219, 351, 2196, 278, 2428, 13, 201, 15200, 2122, 286, 791, 7249, 1522, 6060, 201, 12, 15648, 286, 555, 7249, 1522, 1366, 357, 6404, 82, 11, 4963, 11, 23735, 11, 19449, 737, 532, 4718, 864, 20083, 1986, 6459, 351, 4334, 3551, 12, 38096, 5479, 588, 38488, 290, 1919, 2056, 1366, 13, 201, 19852, 1905, 286, 6060, 26406, 201, 12, 6060, 12037, 761, 284, 670, 351, 3210, 17519, 357, 7902, 53, 11, 19449, 11, 23735, 8, 290, 20622, 17519, 13, 532, 6060, 4237, 2291, 50126, 20083, 11, 1400, 17861, 20083, 11, 1263, 1366, 38072, 11, 1366, 379, 1334, 11, 11305, 1366, 11, 290, 1366, 287, 6268, 13, 201, 41812, 34120, 351, 6060, 5178, 1381, 201, 12, 5972, 1366, 10969, 6459, 2233, 284, 663, 555, 7249, 1522, 3450, 11, 1690, 10616, 2183, 4899, 329, 3781, 13, 532, 23735, 11, 981, 2968, 287, 262, 1613, 11, 460, 307, 8271, 12, 38096, 2233, 284, 663, 15940, 13, 532, 19449, 8618, 11533, 329, 663, 21654, 290, 318, 6768, 973, 287, 30617, 913, 23113, 13, 532, 968, 263, 17519, 588, 24843, 5184, 305, 389, 13977, 11533, 329, 511, 9332, 13, 201, 12885, 1359, 6060, 44101, 44495, 201, 12, 17934, 286, 23202, 1366, 422, 360, 65, 17, 284, 16363, 9652, 13, 532, 44495, 2291, 5400, 287, 1330, 14, 39344, 7767, 290, 9041, 2041, 3435, 287, 262, 1366, 13, 532, 20615, 8893, 743, 2421, 1180, 46728, 270, 364, 2233, 284, 15874, 2041, 3435, 13, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 33221, 201, 11828, 201, 9171, 14706, 28578, 201, 9171, 14706, 318, 1366, 326, 8477, 584, 1366, 11, 8780, 287, 20083, 11, 1366, 16202, 50028, 11, 290, 1597, 4430, 3341, 13, 7683, 1388, 3858, 25, 6276, 11, 1429, 11, 290, 1597, 20150, 13, 201, 45638, 3395, 14706, 201, 7469, 1127, 1366, 8573, 11, 8574, 287, 1080, 18388, 82, 13, 29581, 8893, 11, 1366, 18388, 82, 11, 290, 1321, 319, 20083, 11, 8893, 11, 290, 15180, 13, 201, 18709, 3395, 14706, 201, 24564, 22090, 7767, 287, 1597, 3341, 13, 2892, 6742, 1080, 2854, 11, 1366, 3356, 11, 290, 2836, 1895, 13, 201, 24749, 3395, 14706, 201, 12621, 862, 2985, 7073, 290, 1833, 1366, 13, 29581, 12673, 3307, 11, 1366, 16969, 11, 290, 1366, 2723, 8787, 13, 201, 9171, 14706, 8549, 201, 19904, 9010, 5922, 4788, 329, 22534, 290, 32029, 1366, 13, 376, 420, 2664, 319, 4441, 257, 9314, 1366, 18388, 329, 6942, 1366, 4009, 13, 15860, 1817, 1366, 9412, 11, 9585, 1799, 11, 290, 18848, 13, 201, 20939, 590, 286, 3395, 14706, 8549, 201, 35476, 1817, 1366, 18848, 416, 4955, 4732, 290, 31144, 13, 48221, 942, 1366, 3081, 11, 15794, 11, 290, 2324, 13, 13585, 6392, 689, 4050, 1366, 4542, 290, 8748, 1973, 262, 4009, 13, 201, 16979, 934, 3395, 14706, 8549, 20003, 201, 12, 19764, 14151, 38882, 6188, 9652, 532, 7257, 5256, 5404, 6060, 9104, 263, 532, 18650, 45927, 35869, 532, 35516, 6060, 38410, 9652, 532, 7193, 437, 6060, 37759, 532, 978, 341, 6060, 44515, 532, 48323, 6188, 12194, 446, 532, 5413, 22134, 6060, 44515, 532, 19764, 14959, 20414, 44515, 532, 18650, 14973, 3395, 14706, 8549, 357, 46, 3620, 44, 8, 532, 30019, 425, 3395, 14706, 9142, 532, 791, 22238, 6060, 44515, 532, 1366, 13, 6894, 532, 45255, 1512, 64, 14973, 6060, 44515, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 40596, 201, 36621, 201, 6935, 64, 12, 25512, 515, 3815, 357, 7902, 23266, 8, 201, 13856, 320, 863, 2420, 3696, 810, 262, 46728, 2676, 318, 257, 39650, 13, 16718, 284, 3650, 20793, 1366, 13, 201, 13856, 320, 863, 2420, 2393, 17519, 201, 8206, 3696, 389, 973, 284, 3650, 1366, 810, 1123, 1627, 393, 5752, 468, 3815, 11266, 416, 257, 46728, 2676, 13, 317, 46728, 2676, 318, 257, 8379, 286, 530, 393, 517, 3435, 31577, 262, 18645, 1022, 3815, 13, 8070, 46728, 270, 364, 2291, 39650, 11, 7400, 11, 7633, 11, 11723, 2318, 11, 290, 2272, 13, 201, 2949, 17861, 20083, 201, 27354, 18826, 389, 3562, 284, 3650, 290, 6687, 555, 7249, 1522, 1366, 290, 2148, 3781, 4899, 329, 17247, 428, 2099, 286, 1366, 13, 201, 14439, 45389, 28403, 357, 3535, 7250, 8, 11998, 201, 11964, 82, 326, 2962, 319, 9041, 1597, 8945, 290, 23069, 20793, 1366, 13, 201, 6892, 864, 20083, 201, 27354, 18826, 389, 3562, 284, 3650, 20793, 1366, 351, 880, 12, 23211, 3897, 5356, 290, 1104, 3210, 1366, 3781, 5050, 290, 4899, 13, 201, 50, 641, 669, 201, 13603, 1063, 884, 355, 8060, 23158, 278, 11998, 357, 38, 3705, 8, 290, 8829, 31902, 38657, 357, 32754, 2389, 8, 15940, 7716, 20793, 1366, 13, 201, 44458, 42011, 201, 25423, 5479, 588, 24134, 290, 3012, 31843, 42011, 389, 973, 329, 16924, 290, 22712, 20793, 1366, 13, 201, 17861, 16092, 18826, 201, 27354, 18826, 326, 779, 32112, 1522, 43301, 15417, 357, 17861, 8, 329, 16215, 11, 29349, 11, 290, 42517, 1112, 1366, 287, 20793, 17519, 13, 201, 33349, 12, 25512, 515, 3815, 357, 4694, 23266, 8, 201, 13856, 320, 863, 2420, 3696, 810, 262, 46728, 2676, 318, 257, 7400, 13, 16718, 355, 281, 5559, 284, 44189, 618, 18875, 725, 292, 389, 1944, 287, 2420, 1366, 13, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 40596, 201, 36621, 201, 6601, 1432, 13264, 201, 32, 2276, 3381, 973, 284, 3522, 284, 1366, 326, 468, 587, 7723, 11, 8389, 11, 290, 11557, 523, 326, 340, 460, 307, 973, 329, 1597, 4560, 393, 35959, 329, 6447, 290, 1366, 3781, 13, 632, 460, 307, 257, 1402, 393, 1588, 6831, 6884, 351, 530, 393, 517, 20083, 326, 2824, 11, 6687, 11, 290, 3650, 1366, 5621, 13, 554, 428, 2008, 11, 356, 481, 2148, 281, 16700, 286, 262, 1180, 3858, 286, 38072, 534, 1366, 1244, 26412, 287, 11, 884, 355, 20083, 11, 1366, 43675, 11, 290, 1263, 1366, 7000, 11, 290, 10716, 606, 287, 3744, 3703, 287, 2252, 5861, 13, 201, 38105, 201, 32, 4947, 286, 1366, 11, 393, 1321, 11, 3562, 329, 262, 5128, 11, 6143, 11, 2989, 290, 45069, 11, 290, 17613, 286, 1366, 13, 317, 24047, 8549, 4482, 11, 393, 20137, 5653, 11, 318, 257, 900, 286, 4056, 326, 8075, 290, 16047, 262, 6831, 13, 632, 3578, 345, 284, 3650, 11, 13096, 11, 290, 7925, 1321, 422, 262, 6831, 1262, 257, 2163, 1444, 42517, 1112, 13, 201, 6892, 864, 16092, 18826, 201, 7583, 6412, 284, 355, 371, 11012, 5653, 274, 11, 50126, 20083, 1382, 319, 262, 26564, 7811, 286, 6228, 3696, 11, 351, 1366, 8389, 656, 257, 7400, 934, 5794, 351, 15274, 290, 15180, 1708, 257, 880, 12, 23211, 4645, 290, 32815, 13, 12101, 6228, 3696, 11, 371, 11012, 5653, 274, 389, 23392, 329, 1366, 4560, 290, 42517, 1112, 7411, 867, 8893, 290, 881, 4025, 1366, 15343, 13, 32112, 1522, 43301, 15417, 11, 393, 16363, 11, 318, 262, 3210, 42517, 1112, 3303, 329, 50126, 20083, 13, 201, 15419, 12, 6892, 864, 16092, 18826, 201, 7583, 1900, 355, 1400, 17861, 11, 393, 366, 3673, 5514, 16363, 1600, 1729, 12, 2411, 864, 20083, 9349, 287, 2882, 284, 262, 6115, 11, 9573, 11, 290, 2866, 379, 543, 1366, 318, 852, 7560, 1909, 11, 8384, 12824, 416, 14901, 287, 6279, 14492, 11, 262, 4455, 286, 11597, 11, 290, 1919, 2056, 25068, 13, 28477, 329, 2866, 11, 13688, 11, 290, 5046, 11, 1729, 12, 2411, 864, 20083, 925, 340, 1744, 284, 3650, 1366, 287, 257, 32815, 12, 1203, 393, 1479, 12, 687, 6977, 13, 1400, 17861, 318, 6768, 973, 329, 7587, 1263, 1366, 13, 201, 6601, 45927, 201, 32, 4318, 16099, 326, 4017, 3212, 1321, 2406, 422, 37433, 4237, 290, 14652, 689, 340, 832, 262, 7925, 11, 6121, 11, 290, 3440, 1429, 357, 2767, 43, 1429, 8, 656, 530, 9815, 6831, 329, 23696, 290, 1597, 4430, 13, 1629, 257, 845, 1029, 12, 5715, 11, 262, 12152, 43, 1429, 5419, 345, 284, 7925, 1366, 422, 1180, 1366, 4237, 11, 6121, 262, 1366, 656, 257, 3424, 290, 24284, 1181, 11, 290, 3440, 262, 1366, 656, 262, 13953, 338, 1366, 16099, 13, 201, 6601, 3981, 82, 201, 6601, 285, 5889, 389, 6352, 1039, 286, 1366, 43675, 326, 2962, 319, 2176, 1597, 3951, 393, 13346, 13, 201, 6601, 24153, 201, 6601, 24768, 389, 38072, 326, 3650, 5909, 6867, 286, 8246, 1366, 287, 663, 6868, 5794, 1566, 340, 318, 2622, 13, 201, 12804, 6060, 41835, 201, 20344, 6169, 31350, 290, 6143, 6884, 284, 3650, 11, 5046, 11, 290, 1429, 845, 1588, 1366, 5621, 13, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 40596, 201, 36621, 201, 6892, 864, 24047, 201, 32, 4947, 286, 1366, 8389, 656, 257, 3084, 4645, 11, 810, 262, 8893, 460, 307, 6692, 1912, 319, 2219, 1366, 13, 33220, 3473, 286, 15274, 290, 15180, 11, 351, 15274, 10200, 4406, 290, 15180, 10200, 12608, 13, 33220, 460, 307, 3519, 1912, 319, 4888, 1366, 11, 5086, 329, 262, 45069, 286, 649, 8893, 351, 257, 2060, 12405, 13, 4718, 864, 20083, 779, 32112, 1522, 43301, 15417, 357, 17861, 8, 329, 42517, 1112, 1366, 13, 1119, 17775, 1366, 49052, 11, 2897, 1366, 11540, 832, 1366, 2099, 17778, 11, 290, 2148, 2324, 3033, 329, 6856, 1895, 284, 1366, 13, 201, 27730, 286, 4718, 864, 16092, 18826, 201, 12, 19764, 20137, 17, 532, 5413, 16363, 9652, 532, 33476, 532, 18650, 24047, 532, 2947, 47701, 532, 6186, 4718, 864, 24047, 4809, 357, 49, 5258, 8, 532, 3012, 10130, 16363, 532, 19764, 20137, 17, 319, 10130, 532, 18650, 10130, 532, 16363, 22134, 201, 2782, 4520, 1095, 286, 4718, 864, 16092, 18826, 201, 12, 26719, 2247, 25, 19179, 460, 307, 925, 284, 262, 6831, 4645, 981, 340, 338, 2491, 13, 532, 39963, 49052, 25, 1855, 320, 4340, 1366, 50124, 13, 532, 412, 589, 286, 11559, 290, 9336, 7628, 25, 3242, 364, 2562, 10784, 14, 11748, 3689, 290, 12948, 10162, 278, 287, 6279, 12, 3106, 8136, 13, 532, 7125, 2389, 12, 47587, 25, 48221, 942, 1366, 9922, 290, 15794, 13, 532, 3894, 12, 47045, 290, 15345, 3037, 13, 201, 11041, 35536, 329, 4718, 864, 16092, 18826, 201, 12, 7467, 45389, 28403, 357, 3535, 7250, 2599, 3894, 12, 2385, 863, 329, 8611, 12, 17107, 8861, 351, 10792, 20743, 290, 5992, 13, 532, 6060, 28593, 20089, 25, 30011, 1143, 329, 7467, 16213, 22869, 28403, 357, 3535, 2969, 8, 329, 6754, 1366, 3781, 13, 532, 38488, 23555, 25, 4401, 6551, 6831, 4610, 329, 13157, 290, 7587, 1366, 422, 5743, 4410, 13, 201, 19352, 20597, 286, 4718, 864, 16092, 18826, 201, 12, 1892, 11080, 329, 10663, 12, 7249, 1522, 290, 555, 7249, 1522, 1366, 23696, 13, 532, 36991, 1022, 371, 11012, 5653, 82, 4433, 10411, 3897, 5356, 290, 1366, 3858, 13, 532, 44943, 319, 262, 4129, 286, 1366, 7032, 13, 201, 17875, 1739, 797, 2768, 590, 286, 371, 11012, 5653, 201, 8332, 262, 6954, 286, 1366, 8514, 11, 884, 355, 1263, 1366, 290, 38488, 11, 50126, 20083, 2555, 284, 307, 262, 45718, 3037, 329, 1762, 351, 20793, 1366, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 6601, 5800, 15941, 45486, 201, 6601, 31433, 8549, 201, 10669, 31433, 8549, 201, 23002, 1009, 2039, 12103, 201, 41206, 2039, 12103, 201, 6601, 8549, 201, 12, 9745, 11, 21160, 11, 290, 19818, 1366, 30835, 11, 18306, 11, 290, 1575, 12, 10760, 2280, 422, 2972, 4237, 588, 3009, 11, 29583, 74, 433, 11, 6343, 11, 290, 14173, 669, 13, 201, 12, 7221, 1096, 290, 6687, 1593, 1366, 7723, 422, 1180, 4237, 287, 257, 4318, 4067, 13, 201, 12, 47081, 1080, 4133, 284, 12260, 290, 11767, 262, 2438, 13, 201, 12, 47081, 257, 44573, 290, 4899, 284, 1205, 11, 3494, 11, 12260, 11, 1332, 11, 290, 6061, 2723, 2438, 13, 201, 6601, 38410, 290, 49127, 201, 12, 29677, 11, 26981, 11, 290, 8778, 357, 2767, 43, 8, 1366, 422, 3294, 38072, 656, 257, 4318, 6060, 45927, 13, 201, 12, 10628, 1630, 290, 12438, 329, 11149, 2458, 284, 3788, 4493, 6, 2438, 13, 201, 12, 46267, 284, 17632, 262, 2723, 2438, 13, 201, 12, 4522, 23041, 588, 19764, 14959, 11733, 329, 5922, 11, 4856, 11, 290, 29682, 2723, 2438, 13, 201, 6601, 15612, 1634, 201, 12, 29681, 605, 10552, 286, 1366, 290, 1321, 1262, 15907, 11, 21528, 11, 8739, 11, 3503, 13, 201, 12, 7221, 2890, 290, 11149, 1366, 351, 2196, 278, 290, 12438, 1104, 13, 201, 12, 20003, 329, 33393, 290, 23710, 2438, 13, 201, 12, 23983, 290, 18640, 4899, 2810, 416, 4522, 23041, 284, 33836, 1103, 12, 6894, 4069, 13, 201, 17633, 11819, 201, 12, 16835, 1366, 290, 16602, 7572, 1262, 4572, 4673, 16113, 13, 201, 12, 42225, 1570, 329, 11149, 281, 13184, 286, 6798, 13, 201, 12, 4482, 4133, 329, 23710, 290, 45505, 2438, 13, 201, 12, 10130, 12, 3106, 9706, 12493, 588, 19764, 14959, 11733, 329, 662, 36948, 11, 3047, 11, 290, 29682, 4981, 13, 201, 17633, 34706, 434, 201, 12, 15995, 4873, 4166, 4981, 656, 3227, 12493, 2884, 23113, 13, 201, 12, 8734, 11, 30081, 11, 290, 6687, 2438, 3696, 11640, 13, 201, 12, 20003, 329, 33393, 290, 23710, 2438, 13, 201, 12, 35432, 4899, 588, 19764, 14959, 11733, 290, 19764, 26543, 418, 16189, 3526, 13302, 47238, 329, 5922, 2769, 4673, 290, 4572, 4673, 4981, 13, 201, 17633, 37484, 290, 25809, 201, 12, 45012, 3081, 8794, 284, 4155, 2746, 9922, 11, 22692, 11, 290, 12373, 1108, 13, 201, 12, 399, 14, 32, 201, 12, 46267, 329, 33393, 290, 23710, 2438, 13, 201, 12, 399, 14, 32, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 6601, 8549, 20003, 201, 6601, 38410, 290, 49127, 20003, 201, 6601, 15612, 1634, 20003, 201, 17633, 20003, 201, 10669, 31433, 8549, 20003, 201, 6601, 31433, 8549, 20003, 201, 3666, 17861, 11, 2947, 47701, 11, 42591, 11012, 11, 24843, 48225, 11012, 11, 24843, 46750, 11, 367, 4533, 404, 9220, 4482, 11, 327, 27446, 11, 48567, 2989, 201, 25189, 4891, 3701, 37535, 11, 509, 3266, 37535, 11, 24843, 46906, 11, 24843, 399, 22238, 11, 24843, 17732, 17861, 11, 19081, 22083, 201, 47, 39291, 16240, 11, 31788, 11, 509, 571, 2271, 11, 24843, 5200, 364, 316, 201, 25189, 4891, 46690, 9399, 11, 1001, 25900, 11, 12554, 527, 3262, 274, 11, 2297, 5183, 4946, 33377, 11, 337, 3123, 499, 11, 309, 22854, 37535, 2139, 11, 309, 22854, 37535, 300, 578, 11, 309, 22854, 37535, 16605, 26755, 201, 38, 270, 11, 21722, 11, 15151, 17822, 11, 4722, 27041, 316, 201, 25189, 4891, 22494, 11, 440, 6322, 72, 412, 1362, 544, 11, 39859, 78, 201, 201, 198, 201, 198, 201, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 41206, 2039, 12103, 201, 11828, 201, 41, 929, 88, 353, 201, 12, 21365, 11361, 8300, 2891, 13, 220, 532, 45267, 625, 257, 3470, 1180, 8300, 8950, 832, 50207, 13, 220, 532, 791, 6945, 10314, 11, 2438, 11, 5072, 11, 7582, 9729, 11, 290, 5874, 4582, 287, 257, 2060, 3188, 13, 220, 532, 449, 929, 88, 353, 3498, 318, 262, 1306, 2196, 286, 449, 929, 88, 353, 5740, 12106, 11, 6011, 517, 3660, 290, 26507, 10959, 13, 201, 25189, 4891, 9033, 48425, 201, 12, 45827, 416, 449, 929, 88, 353, 5740, 12106, 11, 3769, 257, 2092, 1998, 13, 220, 532, 35432, 29353, 12971, 1231, 10616, 7097, 12782, 13, 220, 532, 40402, 7552, 286, 9889, 1262, 3224, 12782, 13, 201, 49, 41501, 201, 12, 7712, 2858, 329, 7869, 290, 1366, 3783, 11, 11541, 329, 371, 290, 3917, 371, 12782, 13, 220, 532, 3242, 364, 11361, 2478, 1626, 262, 371, 2858, 13, 220, 532, 791, 6945, 8300, 11, 9706, 11, 28769, 11, 6569, 1366, 1895, 11, 13936, 11, 290, 32704, 656, 530, 2891, 13, 201, 4561, 88, 1082, 201, 12, 39312, 873, 262, 11244, 286, 371, 41501, 329, 262, 11361, 995, 13, 220, 532, 15995, 9700, 2438, 11, 10314, 11, 290, 5874, 4582, 656, 257, 2060, 21978, 13, 201, 2601, 5819, 37497, 2039, 12103, 201, 11828, 201, 25189, 4891, 17732, 201, 12, 347, 963, 1366, 7587, 3113, 351, 14174, 16578, 1799, 13, 220, 532, 16718, 1973, 2972, 11798, 11, 6011, 10730, 7587, 9889, 13, 201, 25189, 4891, 1610, 676, 201, 12, 13860, 12, 36948, 3113, 10759, 319, 1103, 12, 2435, 1366, 15190, 13, 220, 532, 45267, 1111, 15458, 290, 4269, 7587, 11497, 328, 907, 13, 201, 19591, 201, 12, 376, 420, 2664, 319, 1588, 12, 9888, 2769, 4673, 2746, 3047, 13, 220, 532, 26603, 2478, 287, 1366, 3783, 9706, 12493, 13, 201, 37, 2132, 35432, 290, 15612, 20003, 201, 11828, 201, 29132, 12789, 201, 12, 15612, 2836, 7071, 351, 6715, 12, 392, 12, 14781, 9889, 13, 220, 532, 28477, 12, 259, 32704, 290, 7552, 9889, 351, 371, 11, 11361, 11, 290, 34472, 284, 24843, 17732, 13, 201, 40141, 201, 12, 12892, 12846, 621, 22466, 12789, 475, 4577, 284, 779, 13, 220, 532, 3242, 364, 5874, 4899, 329, 1366, 11812, 11, 13389, 11, 32704, 11, 290, 2746, 2615, 13, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 25391, 21743, 201, 48401, 20003, 201, 6601, 8549, 201, 12, 18650, 24047, 220, 532, 5413, 16363, 9652, 220, 532, 19764, 360, 65, 17, 201, 6601, 38410, 290, 49127, 201, 12, 45255, 1512, 64, 4333, 23656, 532, 19764, 14151, 38882, 6060, 29391, 532, 48323, 3186, 532, 18650, 3186, 532, 35516, 3186, 532, 7193, 437, 3186, 532, 5413, 3186, 532, 14959, 11733, 27850, 357, 42813, 6060, 6524, 15451, 8, 201, 6601, 15612, 1634, 201, 12, 8655, 559, 532, 5413, 4333, 20068, 532, 19764, 26543, 418, 30437, 532, 14959, 11733, 27850, 201, 17633, 20003, 329, 11819, 201, 12, 311, 3705, 50, 9104, 263, 532, 35516, 14973, 29295, 532, 14959, 11733, 27850, 357, 42813, 311, 3705, 50, 9104, 263, 8, 201, 17633, 20003, 329, 34706, 434, 201, 12, 311, 3705, 50, 37322, 341, 290, 34706, 434, 6168, 201, 17633, 20003, 329, 37484, 290, 25809, 201, 2949, 5981, 5068, 4899, 1695, 26, 1280, 2723, 9871, 201, 10669, 31433, 8549, 201, 11505, 12, 10459, 4899, 588, 15151, 290, 21722, 389, 3210, 201, 6601, 31433, 8549, 201, 12, 45255, 1512, 64, 14973, 6060, 3948, 590, 532, 19764, 6188, 3948, 590, 44515, 201, 41206, 9344, 201, 12, 14959, 11733, 357, 17721, 12, 3106, 290, 11364, 2196, 8, 532, 367, 17, 46, 12434, 1203, 9552, 201, 37, 2132, 35432, 15612, 20003, 201, 12, 14959, 11733, 357, 45525, 351, 14959, 4946, 21589, 8, 532, 367, 17, 46, 12434, 1203, 9552, 201, 201, 198, 201, 198, 201, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 25391, 21743, 201, 48401, 10130, 20003, 201, 37, 2132, 35432, 15612, 19193, 201, 12, 14959, 11733, 290, 14959, 4946, 21589, 220, 532, 5413, 22134, 10850, 18252, 220, 532, 367, 17, 46, 12434, 1203, 9552, 201, 6601, 8549, 201, 12, 6186, 5313, 6168, 41542, 11012, 220, 532, 10130, 415, 220, 532, 19764, 360, 65, 17, 355, 257, 2139, 201, 6601, 38410, 290, 49127, 201, 12, 45255, 1512, 64, 10130, 6060, 38410, 532, 19764, 6060, 6524, 15451, 357, 3911, 286, 14959, 11733, 8, 201, 6601, 15612, 1634, 201, 12, 16092, 480, 263, 220, 532, 19764, 26543, 418, 7320, 9345, 18389, 220, 532, 19764, 6060, 6524, 15451, 357, 3911, 286, 14959, 11733, 8, 201, 17633, 11819, 201, 12, 14959, 10850, 18252, 220, 532, 3012, 9552, 19193, 13614, 201, 17633, 34706, 434, 201, 12, 311, 3705, 50, 37322, 341, 290, 34706, 434, 6168, 220, 532, 6186, 28733, 48890, 9104, 18289, 220, 532, 14959, 10850, 18252, 201, 17633, 37484, 201, 12, 14959, 4946, 29990, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 18559, 5142, 329, 360, 13221, 278, 15417, 284, 14365, 201, 16979, 934, 42860, 201, 19626, 534, 2476, 290, 262, 2761, 345, 389, 2111, 284, 8494, 201, 37906, 11, 371, 11, 16363, 201, 8021, 408, 508, 345, 389, 18120, 777, 2761, 329, 201, 3351, 6081, 11, 7349, 11, 327, 4880, 11, 22300, 201, 201, 29584, 7391, 11, 19599, 11, 1514, 11, 10888, 11, 15612, 14392, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 14490, 286, 11361, 201, 42166, 896, 286, 8554, 11361, 201, 35, 1608, 290, 554, 4717, 27848, 2096, 286, 262, 11361, 8108, 201, 20468, 26343, 6118, 11056, 201, 12, 11459, 290, 31744, 15582, 201, 12, 6127, 286, 28579, 13359, 3747, 290, 14900, 329, 477, 1866, 201, 44140, 2741, 201, 12, 35225, 3298, 2055, 290, 5129, 286, 10314, 201, 12, 8108, 15446, 588, 9485, 43, 50192, 4441, 3338, 290, 19889, 9029, 201, 6601, 43793, 874, 201, 12, 18535, 18486, 329, 1366, 3783, 11, 9552, 11, 4572, 4673, 11, 3992, 2478, 11, 3503, 13, 201, 12, 31879, 11094, 4056, 588, 9485, 43, 50192, 5742, 739, 33469, 2628, 1716, 4075, 6809, 290, 2766, 287, 262, 11361, 2055, 201, 201, 12, 3998, 13187, 286, 12782, 290, 29251, 329, 2972, 8861, 201, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 14490, 286, 262, 371, 15417, 201, 42166, 896, 286, 8554, 371, 201, 22289, 35530, 329, 8113, 278, 351, 3819, 371, 18987, 201, 12, 5133, 2569, 1547, 201, 11146, 3788, 11, 5086, 329, 2839, 11, 5068, 11, 290, 1171, 779, 201, 12, 779, 49, 201, 12, 6550, 23380, 1547, 201, 12, 36848, 416, 257, 3094, 3298, 2055, 286, 661, 508, 765, 284, 8494, 1263, 2761, 201, 12, 4162, 49, 201, 12, 6060, 18295, 201, 12, 15690, 12, 17107, 15582, 1838, 340, 4577, 284, 15772, 422, 10688, 284, 2438, 329, 46184, 351, 645, 393, 10926, 8300, 4469, 201, 12, 7031, 49, 12545, 201, 201, 12, 13601, 16099, 286, 13905, 3725, 201, 12, 371, 12, 9435, 444, 201, 201, 12, 38410, 351, 584, 8950, 588, 327, 4880, 11, 7349, 11, 11361, 11, 3503, 13, 201, 12, 371, 1628, 3052, 329, 371, 19993, 290, 2995, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 12, 3158, 48609, 201, 9218, 11045, 201, 17861, 318, 8720, 16293, 355, 366, 408, 28381, 1288, 1, 393, 366, 3107, 417, 1911, 201, 1026, 6296, 329, 366, 44909, 1522, 43301, 15417, 1, 290, 318, 1729, 12, 1676, 771, 1523, 13, 201, 17861, 318, 2829, 290, 3665, 11, 1642, 340, 8811, 973, 416, 1366, 5519, 13, 201, 19246, 276, 416, 19764, 287, 16489, 11, 1642, 340, 4697, 621, 11361, 290, 371, 13, 201, 17861, 318, 3562, 329, 11149, 1366, 287, 50126, 20083, 13, 201, 6892, 864, 20083, 3473, 286, 734, 12, 19577, 8893, 351, 5969, 15180, 290, 7885, 15274, 13, 201, 17861, 3303, 4847, 2291, 27166, 2664, 11, 10604, 507, 11, 14322, 16856, 11, 2264, 10640, 11, 290, 44936, 13, 201, 41730, 16363, 318, 8119, 329, 2972, 1366, 3783, 9176, 11, 884, 355, 1597, 14, 7890, 13440, 290, 1366, 12037, 13, 201, 17861, 13536, 1277, 1895, 284, 1366, 11, 26347, 510, 30798, 30632, 13, 201, 1026, 318, 281, 3537, 11584, 3210, 11, 5086, 329, 2493, 1799, 1973, 1180, 20083, 13, 201, 40009, 16363, 20083, 389, 1695, 11, 1390, 33476, 11, 2947, 47701, 11, 18650, 11, 3503, 13, 201, 17861, 15582, 743, 7565, 6906, 319, 262, 50126, 6831, 4542, 1080, 852, 973, 13, 201, 34888, 319, 4673, 16363, 329, 257, 2176, 6831, 290, 6107, 656, 262, 2055, 329, 1104, 13, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 12, 3158, 48609, 201, 32065, 201, 11828, 201, 16979, 934, 20003, 14, 42026, 201, 29584, 201, 12218, 12, 29983, 2134, 12, 17107, 8300, 3303, 13, 23399, 306, 8197, 287, 262, 13953, 2272, 13, 201, 54, 38001, 11, 7349, 12, 5805, 11, 24843, 10373, 8019, 11, 10766, 40684, 19, 11, 367, 4533, 404, 201, 3351, 6081, 201, 12218, 12, 29983, 3303, 6493, 10345, 8300, 290, 1913, 9037, 19720, 13, 4225, 3575, 540, 351, 7349, 13, 201, 25189, 4891, 17732, 357, 8201, 28616, 11, 10373, 8019, 11, 29681, 55, 11, 17732, 43124, 8, 201, 34, 4880, 201, 12218, 12, 29983, 8300, 3303, 11, 7552, 286, 327, 13, 47081, 5443, 7587, 11, 1080, 8300, 9889, 13, 201, 51, 22854, 37535, 11, 42591, 11012, 11, 327, 21223, 201, 29584, 7391, 201, 12218, 12, 29983, 3303, 16610, 3675, 22616, 351, 19081, 13, 8457, 13, 1892, 3519, 284, 7349, 13, 201, 51, 22854, 37535, 13, 8457, 11, 371, 12, 8457, 201, 16980, 544, 201, 5960, 3916, 329, 1029, 12, 26585, 29052, 3781, 290, 31350, 3783, 13, 21522, 728, 4056, 355, 3049, 355, 327, 393, 6401, 2596, 13, 201, 16980, 544, 11012, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 11505, 8090, 201, 11146, 10442, 201, 1925, 6734, 276, 416, 262, 4946, 12, 7416, 18362, 357, 2640, 40, 8, 201, 7469, 1389, 416, 262, 3232, 10442, 5693, 357, 10652, 37, 8, 201, 5167, 1597, 12, 18143, 201, 5167, 5670, 319, 257, 900, 286, 3815, 201, 27730, 25, 11361, 11, 24843, 17732, 11, 309, 22854, 37535, 201, 27730, 25, 371, 11, 22961, 3082, 5329, 12251, 357, 38, 4093, 8, 201, 11146, 284, 779, 201, 11146, 284, 779, 201, 37288, 3544, 262, 3611, 5094, 13789, 357, 16630, 52, 8, 201, 37288, 3544, 16625, 17441, 416, 262, 376, 20802, 201, 15979, 2096, 12438, 201, 15979, 2096, 12438, 201, 6090, 307, 973, 26478, 1346, 351, 1479, 3788, 287, 867, 2663, 201, 12, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 27313, 201, 43, 11127, 201, 23010, 811, 38589, 46267, 357, 37906, 8, 201, 12, 16492, 292, 12, 31835, 20519, 201, 36259, 1634, 46267, 357, 37906, 8, 201, 12, 6550, 29487, 8019, 12, 1001, 397, 1211, 201, 11922, 12, 4971, 10373, 14, 19260, 46267, 357, 37906, 8, 201, 12, 10286, 15813, 12, 35720, 12, 17337, 292, 201, 29744, 18252, 46267, 357, 37906, 8, 201, 12, 309, 22854, 37535, 12, 9485, 13165, 354, 201, 43, 11127, 287, 3819, 42860, 201, 12, 24843, 17732, 357, 37906, 11, 371, 11, 38334, 11, 16363, 13219, 9621, 357, 3351, 6081, 13219, 4403, 19260, 357, 3351, 6081, 13219, 308, 70, 29487, 17, 357, 49, 8, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 23377, 201, 36621, 201, 42166, 896, 201, 17227, 36965, 201, 47206, 292, 201, 37906, 5888, 4955, 1366, 8573, 290, 5499, 329, 6942, 1366, 17512, 290, 3781, 13, 201, 12, 3242, 364, 6060, 19778, 2134, 329, 2562, 17512, 286, 20793, 1366, 7874, 47081, 4899, 329, 1366, 12724, 11, 27179, 9269, 11, 290, 35981, 7874, 45267, 9041, 286, 4814, 1366, 290, 640, 12, 25076, 1366, 7874, 38410, 351, 584, 11361, 12782, 884, 355, 31835, 20519, 290, 6550, 29487, 8019, 13, 201, 6601, 12724, 11, 17512, 11, 3781, 11, 290, 11824, 13, 201, 33111, 20519, 201, 37906, 5888, 329, 29052, 14492, 11, 4955, 1104, 329, 1588, 11, 5021, 12, 19577, 26515, 290, 2603, 45977, 13, 201, 12, 412, 5632, 9041, 286, 1588, 26515, 290, 2603, 45977, 7874, 5683, 2021, 18069, 5499, 329, 7177, 17512, 290, 4560, 7874, 38410, 351, 584, 11361, 12782, 7874, 45267, 14174, 37139, 11, 34296, 5277, 6121, 11, 290, 4738, 1271, 5270, 13, 201, 23010, 811, 14492, 11, 29052, 3781, 11, 14174, 37139, 11, 13905, 3781, 13, 201, 19044, 29487, 8019, 201, 37906, 5888, 329, 4441, 9037, 11, 14333, 11, 290, 15108, 5874, 4582, 287, 11361, 13, 201, 12, 23399, 2837, 286, 7110, 3858, 1390, 1627, 21528, 11, 2318, 15907, 11, 41058, 21528, 11, 1554, 26836, 11, 3503, 7874, 38254, 38322, 7110, 5585, 7874, 7929, 329, 3294, 1366, 17519, 290, 5072, 3858, 7874, 1001, 321, 1203, 11812, 351, 16492, 292, 290, 31835, 20519, 13, 201, 6601, 32704, 11, 39180, 2870, 1366, 3781, 11, 10470, 286, 2267, 6373, 13, 201, 4653, 397, 1211, 201, 17126, 19929, 1366, 32704, 5888, 1912, 319, 6550, 29487, 8019, 11, 4955, 257, 1029, 12, 5715, 7071, 329, 4441, 10966, 290, 30304, 21528, 13, 201, 12, 45157, 6945, 3716, 5874, 4582, 351, 35327, 15582, 7874, 45267, 13905, 29353, 351, 3170, 12, 259, 13460, 290, 3124, 6340, 23014, 7874, 38410, 351, 16492, 292, 6060, 35439, 7874, 15891, 7110, 3858, 290, 3033, 3688, 284, 6550, 29487, 8019, 13, 201, 17126, 19929, 1366, 32704, 11, 39180, 2870, 1366, 3781, 11, 10470, 12, 1493, 21528, 13, 201, 50, 979, 15813, 12, 35720, 201, 37906, 5888, 4955, 2829, 290, 6942, 4899, 329, 1366, 9691, 290, 1366, 3781, 13, 201, 12, 17427, 290, 6414, 7824, 329, 2615, 290, 22232, 4572, 4673, 4981, 7874, 40917, 10314, 290, 2836, 12, 13120, 7071, 7874, 23399, 2837, 286, 16113, 329, 17923, 11, 20683, 11, 32966, 1586, 11, 290, 15793, 1483, 7741, 7874, 38410, 351, 584, 11361, 12782, 884, 355, 16492, 292, 290, 31835, 20519, 13, 201, 37573, 4673, 16113, 2478, 290, 12660, 11, 33344, 21128, 11, 1366, 9691, 13, 201, 42, 263, 292, 201, 11922, 12, 5715, 17019, 7686, 7824, 11, 3194, 287, 11361, 290, 6007, 286, 2491, 319, 1353, 286, 309, 22854, 37535, 11, 383, 5733, 11, 393, 5413, 38655, 16984, 15813, 13, 201, 12, 45157, 6945, 262, 5103, 11, 3047, 11, 290, 12660, 286, 2769, 4673, 4981, 7874, 11787, 12, 13120, 7071, 329, 2615, 3716, 17019, 7686, 7874, 45267, 1111, 9135, 290, 11362, 20309, 7874, 1001, 321, 1203, 11812, 351, 309, 22854, 37535, 290, 584, 2769, 4673, 29251, 7874, 5683, 2021, 2055, 1104, 290, 10314, 13, 201, 29744, 4673, 2746, 2478, 11, 41369, 278, 11, 29315, 11, 2267, 11, 3227, 14833, 13, 201, 51, 22854, 37535, 201, 11505, 12, 10459, 4572, 4673, 9355, 4166, 416, 3012, 14842, 11, 6007, 286, 2491, 319, 32340, 11, 32516, 11, 290, 309, 5105, 82, 13, 201, 12, 34529, 540, 290, 12846, 10959, 11080, 329, 257, 3094, 2837, 286, 5479, 7874, 3334, 12, 26585, 29964, 351, 1104, 329, 9387, 14492, 7874, 5683, 2021, 5888, 13187, 329, 4572, 4673, 290, 2769, 4673, 8861, 7874, 38410, 351, 584, 29251, 884, 355, 17337, 292, 290, 9485, 15884, 354, 13, 201, 29744, 4673, 2746, 2478, 11, 2267, 11, 3227, 14833, 11, 43865, 4572, 4673, 8861, 13, 201, 20519, 15884, 354, 201, 11505, 12, 10459, 2769, 4673, 9355, 4166, 416, 3203, 338, 9552, 4992, 2248, 11, 36360, 13688, 290, 10152, 286, 779, 13, 201, 12, 26977, 29964, 4823, 5086, 329, 8925, 17019, 3127, 45619, 7874, 38647, 959, 28769, 290, 41369, 278, 351, 11361, 291, 15582, 7874, 7929, 329, 11362, 20309, 290, 9387, 3047, 7874, 13601, 290, 4075, 2055, 14329, 284, 262, 9355, 338, 2478, 290, 13187, 7874, 38410, 351, 584, 11361, 12782, 290, 29251, 13, 201, 29744, 4673, 2746, 2478, 11, 2267, 11, 29315, 11, 41369, 278, 11, 3227, 14833, 13, 201, 25189, 4891, 17732, 201, 12218, 12, 29983, 13946, 14492, 9355, 3562, 329, 3049, 290, 6942, 1588, 12, 9888, 1366, 7587, 13, 201, 12, 554, 12, 31673, 14492, 329, 5801, 1366, 7587, 7874, 40050, 12, 83, 37668, 290, 17843, 287, 9387, 14492, 12493, 7874, 7929, 329, 2972, 8300, 8950, 1390, 11361, 11, 38334, 11, 7349, 11, 290, 16363, 7874, 38410, 351, 584, 1263, 1366, 4899, 290, 29251, 7874, 34529, 1799, 290, 2854, 23989, 329, 1588, 12, 9888, 1366, 23696, 13, 201, 21968, 12, 9888, 1366, 7587, 11, 1366, 23696, 11, 4572, 4673, 319, 1263, 1366, 11, 9387, 14492, 13, 201, 53, 1533, 292, 357, 3351, 6081, 8, 201, 3351, 6081, 5888, 329, 4441, 13905, 1366, 5874, 4582, 11, 4955, 281, 7824, 329, 15453, 15907, 290, 21528, 422, 17732, 6060, 35439, 13, 201, 12, 1001, 321, 1203, 11812, 351, 17732, 6060, 35439, 329, 1366, 32704, 319, 9387, 40522, 7874, 7929, 329, 2972, 8262, 3858, 290, 12186, 11, 1390, 2318, 15907, 11, 1554, 26836, 11, 290, 41058, 21528, 7874, 5683, 27339, 290, 38322, 8262, 8398, 3689, 7874, 20737, 284, 670, 351, 1111, 1957, 290, 9387, 1366, 4237, 13, 201, 17126, 19929, 1366, 32704, 11, 39180, 2870, 1366, 3781, 11, 1263, 1366, 32704, 13, 201, 12804, 19260, 357, 3351, 6081, 8, 201, 20344, 6169, 2769, 4673, 5888, 329, 24843, 17732, 11, 15882, 2769, 4673, 319, 9387, 40522, 13, 201, 12, 2039, 2977, 2769, 4673, 2746, 3047, 290, 32278, 3264, 319, 17732, 23163, 7874, 45267, 257, 3094, 2837, 286, 2769, 4673, 4981, 290, 16113, 7874, 38410, 351, 17732, 10373, 8019, 290, 6060, 19778, 23113, 7874, 34529, 1799, 329, 3047, 4981, 319, 1588, 12, 9888, 40522, 7874, 46021, 351, 4683, 17732, 13187, 4899, 290, 12782, 13, 201, 20344, 6169, 2769, 4673, 2746, 3047, 11, 1588, 12, 9888, 2769, 4673, 8861, 11, 1263, 1366, 23696, 351, 2769, 4673, 13, 201, 1130, 29487, 17, 357, 49, 8, 201, 49, 5301, 329, 4441, 19992, 290, 3716, 1366, 5874, 4582, 11, 7867, 416, 262, 20159, 3876, 286, 19840, 13, 201, 12, 47081, 257, 12846, 290, 3665, 15582, 329, 4441, 13767, 21528, 290, 15907, 7874, 5683, 2021, 31344, 3689, 329, 7110, 5585, 290, 12461, 7874, 1001, 321, 1203, 11812, 351, 371, 338, 1366, 17512, 290, 3781, 9889, 7874, 7929, 329, 3716, 1366, 8573, 290, 13905, 38226, 13, 201, 201, 201, 198, 201, 198, 201, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 40596, 201, 36621, 201, 17614, 201, 34934, 6946, 1022, 734, 5207, 286, 3788, 416, 4955, 281, 7071, 329, 17311, 290, 23862, 1231, 21294, 262, 30203, 3307, 13, 201, 47206, 292, 201, 16281, 286, 257, 5888, 351, 281, 7824, 326, 3578, 6946, 351, 3788, 6805, 329, 1366, 7587, 1231, 3725, 286, 262, 30203, 13, 201, 51, 22854, 37535, 201, 7282, 437, 3194, 287, 327, 4880, 351, 23113, 1695, 329, 11361, 11, 11933, 11, 327, 4880, 11, 7349, 11, 290, 1514, 11, 5086, 6946, 290, 32121, 286, 309, 22854, 37535, 338, 10345, 871, 287, 1180, 8300, 8950, 13, 201, 49, 6465, 7824, 201, 1273, 1746, 329, 10858, 864, 1812, 20558, 7824, 11, 15882, 6946, 625, 262, 5230, 284, 17624, 4133, 884, 355, 6143, 11, 1366, 11, 290, 16113, 13, 201, 11792, 201, 7120, 1430, 11, 543, 48556, 351, 262, 3992, 2139, 832, 262, 30617, 7824, 13, 201, 26198, 201, 464, 3992, 2139, 17535, 2884, 262, 30617, 7824, 11, 4955, 10345, 871, 393, 1366, 13, 201, 12915, 4122, 201, 464, 4067, 810, 262, 5456, 1895, 274, 262, 3992, 2139, 284, 3758, 7007, 290, 3328, 9109, 13, 201, 18453, 201, 31837, 416, 262, 5456, 284, 262, 8271, 2884, 14626, 5050, 11, 7268, 7729, 329, 262, 4905, 284, 307, 6157, 13, 201, 31077, 201, 13615, 276, 416, 262, 8271, 284, 262, 5456, 2884, 14626, 5050, 11, 7268, 1321, 393, 1366, 9167, 416, 262, 5456, 13, 201, 40717, 16000, 201, 38052, 284, 21937, 1366, 625, 262, 5230, 1022, 262, 5456, 290, 262, 8271, 13, 201, 40386, 201, 6601, 5794, 973, 287, 14626, 6218, 329, 7007, 290, 9109, 11, 7268, 7729, 393, 1321, 287, 257, 20793, 5642, 13, 201, 16281, 25, 8255, 284, 24709, 7824, 201, 3103, 24040, 4046, 284, 2420, 26, 5456, 12800, 281, 6597, 2393, 2884, 257, 24582, 2581, 11, 290, 262, 7824, 5860, 262, 2420, 26955, 287, 257, 2882, 2884, 257, 17151, 2581, 13, 201, 16281, 25, 15417, 3602, 41880, 7824, 201, 8291, 75, 689, 2420, 422, 530, 3303, 284, 1194, 26, 5456, 12800, 2420, 284, 307, 14251, 11, 290, 262, 7824, 5860, 262, 14251, 2420, 13, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 33221, 201, 11828, 201, 6601, 5345, 30396, 201, 32, 20793, 4947, 286, 1366, 10200, 1321, 287, 2972, 17519, 884, 355, 2420, 11, 3146, 11, 4263, 11, 6597, 11, 393, 2008, 3696, 13, 16904, 934, 1366, 5621, 389, 8389, 656, 15274, 290, 15180, 11, 1690, 8574, 287, 17519, 588, 44189, 357, 6935, 64, 8621, 283, 515, 27068, 737, 201, 31431, 286, 6060, 33147, 1056, 201, 2898, 324, 8736, 11, 1366, 5621, 547, 2839, 11, 7268, 20622, 393, 15279, 1321, 11, 290, 547, 407, 4888, 7271, 13, 2102, 11, 867, 12066, 783, 2148, 1366, 5621, 355, 366, 9654, 1366, 553, 1642, 606, 1695, 329, 1479, 284, 262, 1171, 13, 201, 21188, 286, 6060, 201, 6601, 5621, 389, 18229, 422, 2972, 12066, 1390, 5654, 6712, 11, 6905, 11, 5745, 11, 290, 2706, 13, 40343, 290, 5745, 8688, 7715, 1366, 5621, 319, 10233, 884, 355, 262, 3773, 11, 3592, 11, 11409, 11, 9358, 11, 290, 262, 2858, 13, 201, 20012, 6060, 13789, 12729, 357, 8610, 13534, 8, 201, 8610, 13534, 373, 2727, 416, 262, 7020, 5693, 284, 2209, 15665, 4786, 3519, 284, 262, 6082, 290, 779, 286, 1280, 1366, 13, 632, 3407, 734, 16625, 25, 6458, 13534, 12, 2484, 1723, 290, 6458, 13534, 12, 5990, 33532, 13, 2312, 16625, 8160, 2846, 329, 1262, 290, 30620, 1366, 5621, 11, 351, 6458, 13534, 12, 5990, 33532, 5086, 19008, 1231, 262, 9079, 284, 2648, 2458, 13, 201, 8610, 13534, 12, 2484, 1723, 13789, 201, 34934, 2985, 284, 779, 290, 13096, 1366, 5621, 11, 351, 262, 9079, 284, 2648, 597, 19008, 739, 262, 976, 5964, 2846, 355, 262, 2656, 1366, 13, 201, 8610, 13534, 12, 5990, 33532, 13789, 201, 34934, 2985, 284, 779, 290, 13096, 1366, 5621, 1231, 262, 9079, 284, 2648, 2458, 13, 18987, 389, 407, 31586, 284, 2648, 19008, 284, 262, 1366, 13, 201, 26950, 529, 286, 4946, 6060, 319, 6060, 5800, 201, 11505, 1366, 468, 2826, 257, 2383, 2597, 287, 262, 3349, 286, 1366, 3783, 11, 4572, 4673, 11, 290, 11666, 4430, 416, 4955, 24068, 351, 1895, 284, 257, 3094, 2837, 286, 1366, 5621, 13, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 33221, 201, 11828, 201, 6601, 31433, 304, 55, 3803, 357, 5631, 55, 8, 201, 9865, 44, 338, 1280, 1366, 16099, 11, 4955, 257, 36768, 4947, 286, 1029, 12, 13237, 1280, 1366, 5621, 422, 19764, 4992, 290, 13467, 2368, 12, 10608, 4237, 13, 201, 30026, 3455, 286, 17051, 55, 201, 2514, 2148, 6505, 351, 1895, 284, 3748, 11, 1029, 12, 13237, 1366, 5621, 329, 779, 287, 13953, 5479, 13, 201, 6601, 21394, 14898, 319, 17051, 55, 201, 5631, 55, 4394, 2972, 1366, 5621, 11, 1390, 4263, 11, 2008, 11, 2420, 11, 290, 6597, 1366, 13, 201, 20012, 6060, 13789, 12729, 357, 8610, 13534, 8, 201, 6601, 5621, 319, 17051, 55, 389, 925, 1695, 739, 262, 6458, 13534, 284, 17016, 1366, 7373, 290, 12438, 11, 13359, 1598, 5964, 290, 8748, 2846, 13, 201, 6425, 2070, 36361, 82, 201, 5631, 55, 3769, 11808, 43935, 326, 5698, 6505, 832, 1366, 12724, 11, 662, 12, 36948, 11, 39180, 2870, 3781, 11, 290, 517, 13, 2773, 1366, 5621, 2291, 6190, 43935, 329, 8861, 884, 355, 4441, 15907, 11, 3047, 4572, 4673, 4981, 11, 290, 9489, 13905, 290, 640, 12, 25076, 3781, 13, 201, 15457, 278, 17051, 55, 201, 19246, 364, 460, 1895, 17051, 55, 319, 262, 19764, 23836, 3052, 739, 366, 11505, 8090, 379, 19764, 1, 290, 788, 17246, 366, 6601, 31433, 304, 55, 3803, 1, 422, 262, 4268, 2902, 6859, 13, 201, 18438, 3255, 6060, 21394, 319, 17051, 55, 201, 19246, 364, 460, 7301, 2972, 1280, 1366, 5621, 319, 17051, 55, 11, 884, 355, 262, 366, 15285, 3838, 15615, 6060, 532, 35216, 12690, 1, 1366, 900, 11, 543, 4909, 6193, 1366, 422, 1757, 376, 13, 10401, 12690, 287, 968, 1971, 13, 201, 15457, 278, 5740, 12106, 287, 14959, 11733, 201, 6425, 12106, 3917, 351, 1366, 5621, 319, 17051, 55, 460, 307, 17535, 290, 10945, 287, 14959, 11733, 11, 5086, 6505, 284, 1620, 1366, 12724, 11, 662, 12, 36948, 11, 39180, 2870, 3781, 11, 290, 517, 13, 201, 6601, 13283, 14898, 319, 17051, 55, 201, 5631, 55, 3769, 1895, 284, 530, 393, 517, 1366, 3696, 3917, 351, 1123, 1366, 900, 13, 34152, 460, 1570, 290, 1895, 777, 1366, 3696, 284, 779, 287, 511, 4493, 13, 201, 34500, 1358, 351, 19764, 10130, 290, 14959, 11733, 201, 19246, 364, 460, 2604, 656, 511, 19764, 10130, 1848, 11, 2251, 257, 1628, 11, 290, 3440, 17051, 55, 43935, 656, 262, 1628, 287, 14959, 11733, 13, 5740, 12106, 460, 307, 10945, 287, 14959, 11733, 329, 1366, 3781, 290, 13936, 201, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 33221, 201, 11828, 201, 37573, 18252, 357, 5805, 8, 201, 5842, 274, 16113, 357, 27530, 8, 284, 5911, 7572, 287, 1366, 13, 201, 17633, 13614, 201, 18709, 416, 543, 262, 2746, 22974, 1366, 7572, 13, 201, 31431, 286, 10850, 18252, 201, 12, 3115, 16149, 18252, 25, 26848, 5128, 1366, 290, 3376, 23862, 329, 3047, 13, 29581, 20683, 290, 17923, 4981, 13, 532, 791, 16668, 16149, 18252, 25, 16213, 12271, 9642, 9608, 276, 1366, 284, 5911, 7572, 290, 4645, 13, 29581, 32966, 1586, 4981, 13, 532, 22299, 13442, 18252, 25, 30667, 832, 4473, 290, 4049, 284, 20487, 11530, 13, 201, 12442, 16149, 18252, 24897, 201, 12, 3310, 2234, 32329, 25, 49461, 35575, 3815, 13, 532, 40984, 32329, 25, 49461, 9376, 393, 6097, 13, 201, 29744, 18252, 201, 13409, 1143, 2099, 286, 10373, 326, 795, 15968, 1692, 3632, 15025, 13, 16718, 329, 22712, 3288, 3303, 11, 4263, 11, 6597, 11, 2008, 11, 290, 517, 13, 201, 44357, 10766, 18252, 32329, 201, 39618, 1588, 15494, 40522, 290, 318, 2653, 15208, 18590, 13, 15183, 19653, 25, 309, 22854, 37535, 11, 9485, 15884, 354, 11, 17337, 292, 13, 3771, 12, 35311, 4981, 1695, 287, 2746, 1976, 16426, 13, 201, 25954, 257, 9104, 17934, 201, 12, 9745, 290, 8335, 1366, 13, 532, 36052, 8246, 3047, 1366, 13, 532, 10934, 393, 2922, 257, 2746, 13, 532, 16835, 262, 2746, 319, 5597, 1366, 13, 532, 16213, 2736, 3047, 2482, 290, 11629, 378, 13, 532, 34706, 262, 8776, 2746, 13, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 33221, 201, 11828, 201, 17633, 31433, 304, 55, 3803, 357, 22921, 8, 201, 12, 3232, 1280, 12, 10459, 16099, 329, 2769, 4673, 4981, 13, 532, 47081, 3492, 12, 1462, 12, 1904, 290, 38322, 2769, 4673, 4580, 30416, 13, 532, 3242, 364, 4981, 6789, 290, 6061, 540, 287, 1957, 290, 6279, 12493, 13, 532, 32329, 1695, 739, 583, 33532, 1280, 12, 10459, 16625, 13, 201, 42166, 896, 286, 8554, 3771, 12, 35311, 32329, 201, 12, 2297, 26873, 640, 284, 1988, 13, 532, 311, 3080, 4133, 11, 4827, 11, 290, 640, 2672, 329, 3047, 422, 12692, 13, 201, 7293, 3906, 286, 9104, 12, 31293, 4527, 15271, 201, 12, 3771, 12, 35311, 2769, 4673, 2746, 13, 532, 23412, 662, 12, 36948, 2438, 13, 532, 25235, 1281, 12, 36948, 2438, 13, 532, 8997, 1143, 1171, 7824, 13, 201, 20344, 3890, 286, 25882, 4527, 30416, 201, 12, 28477, 290, 9387, 355, 1280, 12, 10459, 25716, 4263, 319, 21722, 13, 532, 1680, 307, 27658, 329, 2614, 290, 5068, 779, 13, 201, 49322, 434, 17406, 341, 19193, 82, 201, 12, 12554, 527, 3262, 274, 25, 17406, 689, 14833, 11, 20796, 11, 290, 4542, 286, 25716, 4263, 13, 532, 2297, 10983, 4946, 33377, 25, 14973, 12, 9526, 12554, 527, 3262, 274, 3859, 1695, 319, 3294, 6279, 9554, 13, 201, 18438, 3255, 14322, 18156, 32329, 201, 12, 16440, 25962, 12, 1069, 3803, 13, 2398, 284, 1570, 290, 779, 2747, 18156, 4981, 13, 532, 17934, 25, 9515, 31029, 2746, 13, 201, 12814, 6127, 25553, 329, 9104, 4225, 2673, 201, 12, 6127, 25553, 25, 7467, 2891, 329, 12857, 2166, 12, 437, 8950, 13, 532, 40402, 10375, 351, 25882, 309, 22854, 37535, 13, 8457, 2746, 329, 2134, 13326, 13, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 33221, 201, 16130, 201, 36621, 286, 449, 929, 88, 353, 5740, 12106, 201, 12, 19486, 1900, 355, 366, 72, 37906, 553, 4166, 329, 11361, 8300, 13, 201, 201, 12, 7152, 2434, 449, 929, 88, 353, 284, 1104, 3224, 8950, 25, 22300, 11, 11361, 11, 290, 371, 11, 1871, 1854, 13, 201, 201, 12, 34270, 12, 3106, 3586, 5086, 6282, 290, 7373, 286, 4963, 351, 2438, 11, 27490, 11, 5874, 4582, 11, 290, 8689, 2420, 13, 201, 23595, 286, 449, 929, 88, 353, 5740, 12106, 201, 12, 14336, 1127, 35644, 2420, 11, 2438, 7021, 11, 290, 2438, 5072, 287, 257, 2060, 2393, 13, 201, 201, 12, 2980, 689, 5072, 11, 1390, 21528, 290, 8893, 11, 1626, 262, 20922, 2393, 13, 201, 201, 12, 36472, 540, 284, 12960, 393, 11532, 17519, 329, 7373, 13, 201, 21906, 284, 449, 929, 88, 353, 17822, 201, 12, 34270, 12, 3106, 3586, 329, 22534, 3294, 449, 929, 88, 353, 5740, 2070, 3696, 290, 584, 2438, 14, 7890, 3696, 13, 201, 201, 12, 5683, 2412, 10345, 871, 286, 449, 929, 88, 353, 5740, 12106, 351, 3033, 588, 3294, 43935, 11, 2420, 15719, 11, 30237, 11, 3503, 13, 201, 201, 12, 3082, 16873, 351, 2972, 2393, 17519, 884, 355, 44189, 11, 19449, 11, 12960, 11, 30310, 11, 290, 1854, 13, 201, 201, 12, 4946, 12, 10459, 290, 6971, 11812, 351, 6279, 12, 3106, 2594, 588, 19764, 290, 3012, 1623, 397, 13, 201, 30838, 290, 29566, 201, 12, 1680, 307, 6589, 2884, 3141, 1627, 1262, 7347, 2721, 13, 201, 201, 12, 1680, 307, 15680, 15726, 832, 1052, 330, 13533, 19193, 422, 1052, 330, 13533, 13, 785, 13, 201, 201, 12, 1052, 330, 13533, 6082, 3407, 449, 929, 88, 353, 290, 449, 929, 88, 353, 17822, 13, 201, 201, 12, 14504, 276, 2196, 1695, 287, 20389, 7311, 23500, 11, 18591, 262, 761, 329, 1957, 26162, 13, 201, 201, 198, 201, 198, 201, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 33221, 201, 32, 2733, 201, 28768, 11, 35835, 278, 11, 42226, 889, 201, 12, 6914, 366, 10987, 1, 4936, 393, 779, 15576, 1343, 6062, 29401, 284, 12260, 4778, 13, 201, 34, 19187, 201, 12, 3060, 649, 2685, 351, 262, 5556, 6194, 287, 262, 50149, 13, 201, 201, 38727, 2685, 416, 12264, 366, 18378, 1, 319, 262, 1388, 6859, 2318, 11, 788, 366, 38727, 39794, 553, 393, 1262, 29401, 360, 2124, 17, 13, 201, 28516, 351, 20401, 5740, 12106, 201, 12, 4946, 649, 20922, 351, 5556, 4936, 393, 366, 8979, 1, 1875, 366, 11505, 257, 649, 24008, 1, 393, 366, 11505, 257, 649, 20922, 526, 201, 201, 12, 10028, 43935, 11, 1295, 1735, 416, 1735, 11, 393, 319, 4553, 22524, 13, 201, 34695, 278, 15691, 201, 12, 5765, 2940, 2902, 4778, 329, 8714, 290, 2420, 16969, 13, 201, 201, 12, 38240, 4778, 290, 23862, 656, 19392, 329, 10470, 13, 201, 201, 12, 3454, 1460, 11244, 16316, 2438, 11, 5874, 4582, 11, 2420, 11, 290, 23862, 355, 636, 286, 257, 1628, 13, 201, 39079, 889, 5588, 5740, 12106, 201, 12, 6914, 2245, 7196, 319, 262, 40217, 284, 4423, 866, 43935, 13, 201, 201, 12, 15527, 378, 477, 10991, 379, 1752, 393, 4423, 866, 17033, 13, 201, 201, 12, 366, 2949, 9720, 1, 379, 1353, 826, 9217, 28621, 6246, 13, 13872, 22524, 13, 201, 201, 198, 201, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 33221, 201, 11828, 201, 36621, 286, 32169, 201, 12, 317, 31350, 3113, 23710, 2438, 287, 257, 5740, 2070, 2393, 13, 201, 201, 40009, 449, 929, 88, 353, 509, 44930, 1695, 329, 1180, 8950, 13, 201, 28516, 351, 509, 44930, 201, 12, 1649, 257, 5740, 2070, 9808, 11, 262, 3519, 9720, 18617, 6338, 13, 201, 201, 12, 509, 44930, 1620, 2653, 602, 290, 4439, 2482, 618, 5740, 2070, 318, 10945, 13, 201, 201, 12, 2773, 8950, 662, 12, 37050, 287, 20389, 7311, 2248, 2858, 25, 11361, 11, 24843, 11, 22300, 11, 371, 11, 15608, 13, 201, 201, 12, 9683, 3303, 329, 6060, 5800, 1628, 2402, 20922, 4219, 13, 201, 201, 12, 11361, 9720, 13536, 9706, 286, 11361, 4778, 11, 9194, 5072, 13, 201, 201, 12, 3819, 50207, 1695, 25, 24843, 11, 22300, 11, 371, 11, 15608, 13, 201, 201, 12, 9683, 9720, 319, 4219, 2443, 393, 422, 4268, 2902, 6859, 287, 1353, 826, 5228, 286, 5740, 2070, 13, 201, 201, 12, 1869, 935, 2721, 8950, 832, 3141, 1627, 7071, 357, 5097, 40, 8, 319, 1957, 4572, 611, 407, 662, 12, 37050, 13, 201, 201, 198, 201, 198, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 1722, 806, 201, 11828, 201, 41, 929, 88, 353, 29778, 201, 12, 1846, 1154, 902, 257, 734, 12, 14681, 2746, 351, 257, 9720, 290, 257, 5456, 13, 201, 201, 12, 20985, 25, 26491, 357, 68, 13, 70, 1539, 6444, 287, 449, 929, 88, 353, 5740, 2070, 8, 329, 7216, 2438, 284, 262, 9720, 13, 201, 201, 12, 32169, 25, 8393, 1769, 2438, 290, 5860, 2482, 284, 262, 5456, 329, 3359, 13, 201, 41, 929, 88, 353, 5740, 12106, 201, 12, 10858, 2438, 11, 20150, 11, 10154, 11, 290, 23862, 13, 201, 201, 197, 12, 8858, 276, 422, 6444, 284, 5740, 2070, 4382, 355, 19449, 2393, 351, 764, 541, 2047, 65, 7552, 13, 201, 6425, 2070, 9652, 201, 12, 20549, 856, 329, 8914, 290, 11046, 43935, 13, 201, 42, 7948, 37497, 201, 12, 8393, 1769, 2438, 4778, 287, 262, 5740, 2070, 618, 2836, 4539, 606, 13, 201, 3103, 9641, 10854, 201, 12, 36965, 9716, 261, 1851, 2891, 284, 10385, 3696, 284, 584, 17519, 13, 201, 201, 3103, 9641, 9018, 662, 41341, 30620, 20922, 11, 1033, 4337, 23202, 284, 649, 5794, 11, 290, 1281, 41341, 2457, 2890, 5072, 13, 201, 16281, 201, 12, 35602, 889, 20922, 2393, 284, 11532, 9018, 662, 41341, 11, 1033, 4337, 11, 290, 1281, 41341, 11, 7186, 287, 281, 11532, 2393, 326, 460, 307, 9066, 2884, 663, 10289, 13, 201, 201, 198, 201, 15167, 2229, 10692, 8581, 12, 6060, 5800, 18892, 16544, 532, 3158, 48609, 201, 1722, 806, 201, 11828, 201, 5377, 1996, 864, 5740, 12106, 201, 12, 29176, 2438, 11, 31350, 5072, 11, 44742, 2420, 11, 290, 40162, 4133, 656, 257, 2060, 3188, 13, 201, 41, 929, 88, 353, 5740, 2070, 201, 12, 22623, 2099, 286, 31350, 20922, 6493, 9264, 286, 8300, 8950, 13, 201, 41, 929, 88, 353, 17822, 290, 22269, 6127, 201, 12, 22623, 12493, 329, 4441, 290, 30620, 449, 929, 88, 353, 5740, 12106, 319, 257, 1957, 3335, 13, 201, 41, 929, 88, 353, 17822, 201, 12, 5313, 12, 3106, 3586, 5086, 6282, 286, 2438, 11, 14333, 5874, 4582, 11, 2420, 11, 290, 27490, 13, 29581, 7667, 662, 12, 37050, 11361, 12782, 588, 31835, 20519, 11, 16492, 292, 11, 290, 6550, 29487, 8019, 13, 201, 2025, 330, 13533, 201, 12, 3232, 290, 1280, 12, 10459, 32137, 329, 11361, 290, 371, 11, 9593, 625, 20007, 12782, 13, 45267, 1366, 3783, 290, 4572, 4673, 8861, 13, 3242, 364, 1479, 2055, 1104, 290, 1052, 330, 13533, 13244, 23823, 25757, 13, 201, 2025, 330, 13533, 13244, 23823, 201, 12, 29681, 605, 2836, 7071, 36045, 9988, 286, 649, 10392, 1231, 43749, 13, 40402, 13925, 449, 929, 88, 353, 17822, 290, 15975, 3224, 12493, 13, 201, 41, 929, 88, 353, 5740, 2070, 357, 2025, 330, 13533, 18, 8, 201, 12, 9455, 284, 923, 449, 929, 88, 353, 5740, 2070, 287, 1052, 330, 13533, 2858, 13, 201, 41, 929, 88, 353, 17822, 16189, 3526, 201, 12, 48547, 2884, 1957, 4774, 287, 6444, 13, 1869, 1095, 449, 929, 88, 353, 5740, 12106, 13, 201, 6425, 2070, 21582, 201, 12, 13610, 649, 449, 929, 88, 353, 5740, 2070, 416, 17246, 11361, 513, 422, 262, 4268, 2902, 6859, 13, 7152, 480, 20922, 422, 705, 46332, 6, 284, 10348, 1438, 13, 201, 28780, 24897, 201, 12, 6127, 4778, 3994, 2438, 284, 307, 10945, 287, 9720, 290, 3359, 5072, 13, 2940, 2902, 4778, 3994, 5527, 2420, 290, 3359, 39559, 5072, 13, 201, 10002, 278, 5740, 12106, 201, 12, 8798, 4321, 3689, 2884, 9220, 6859, 13, 9683, 10348, 4321, 5794, 13, 201, 36259, 11733, 6127, 357, 20304, 6127, 8, 201, 12, 3232, 11, 1280, 12, 10459, 2438, 5464, 6493, 28769, 290, 4876, 12, 20270, 4560, 13, 3082, 16873, 351, 7020, 11, 3964, 11, 290, 40017, 13, 3242, 364, 15582, 21292, 11, 8295, 12, 521, 298, 341, 11, 290, 517, 13, 201, 11627, 5736, 329, 22269, 6127, 201, 12, 15545, 11361, 18366, 284, 12260, 11361, 2438, 287, 22269, 6127, 13, 8798, 18366, 2884, 49751, 6859, 393, 19212, 1343, 15576, 1343, 1395, 8251, 13, 11140, 329, 366, 37906, 1, 290, 2721, 3519, 18366, 13, 201, 12814, 22269, 6127, 351, 449, 929, 88, 353, 201, 12, 4946, 22269, 6127, 422, 1052, 330, 13533, 13244, 23823, 393, 4321, 422, 2438, 13, 41464, 19149, 952, 13, 785, 13, 17056, 495, 18366, 284, 12260, 11361, 2438, 13, 19430, 290, 12260, 2438, 287, 449, 929, 88, 353, 5740, 2070, 5794, 13, 201, 50, 2703, 287, 22269, 6127, 201, 12, 12793, 3696, 416, 35210, 284, 9220, 6859, 290, 17246, 12793, 13, 201, 201, 198, 201, 198, 1722, 806, 201, 6601, 14691, 201, 6601, 30437, 201, 36621, 201, 11627, 974, 278, 3616, 422, 1366, 329, 2551, 12, 8601, 13, 201, 36307, 11, 3781, 11, 290, 10794, 286, 1366, 284, 1104, 2551, 12, 8601, 290, 3061, 13293, 13, 201, 43642, 201, 45, 6018, 11, 5670, 319, 37895, 17218, 422, 1366, 13, 201, 30507, 11, 38932, 1366, 4947, 11, 3781, 11, 10794, 11, 290, 2551, 1104, 13, 201, 34888, 201, 43467, 1613, 2995, 290, 11257, 13, 201, 43467, 1613, 2995, 11, 25539, 2003, 10906, 11, 290, 39973, 4028, 13, 201, 46202, 201, 24915, 12464, 290, 40693, 1366, 3781, 7605, 13, 201, 6601, 11228, 11, 11824, 11, 13936, 11, 4542, 11, 6143, 11, 12660, 11, 290, 7373, 286, 17218, 13, 201, 30026, 3455, 201, 47522, 34871, 2551, 12, 8601, 1912, 319, 6754, 1366, 13, 201, 818, 15464, 2551, 12, 8601, 1973, 2972, 26564, 5499, 290, 25539, 2003, 10906, 13, 201, 31431, 201, 24915, 12464, 3781, 11, 40693, 3781, 13, 201, 24564, 1968, 425, 11, 23584, 11, 33344, 11, 906, 6519, 425, 23696, 13, 201, 25574, 6368, 201, 49926, 313, 8497, 3781, 11, 20683, 3781, 11, 2695, 3781, 13, 201, 36259, 1634, 11, 13905, 3781, 11, 4572, 4673, 11, 23989, 13, 201, 27730, 201, 37702, 9510, 4200, 1366, 284, 5911, 11257, 13, 201, 47, 17407, 278, 6491, 4069, 284, 27183, 7124, 10064, 13, 201, 49045, 201, 38, 391, 17218, 422, 1366, 284, 4175, 5370, 13, 201, 47531, 2551, 12, 8601, 11, 9494, 9332, 11, 290, 4620, 26564, 15221, 13, 201, 26950, 529, 201, 3546, 13857, 17218, 329, 2176, 2683, 13, 201, 14617, 12, 4354, 10039, 5410, 290, 12948, 9025, 13, 201, 33637, 201, 44458, 21760, 3788, 11, 13905, 3788, 11, 1366, 32704, 4899, 13, 201, 24749, 4430, 9554, 11, 6190, 23696, 4899, 11, 4572, 4673, 16113, 13, 201, 201, 198, 201, 198, 1722, 806, 201, 6601, 14691, 201, 6601, 30437, 201, 5308, 7574, 201, 11627, 974, 278, 17218, 422, 1366, 329, 2551, 12, 8601, 13, 201, 41206, 286, 1366, 31108, 422, 8246, 1366, 38382, 284, 7585, 286, 1366, 3186, 13, 201, 43642, 201, 3103, 1087, 4111, 319, 2095, 2890, 1366, 290, 9194, 16277, 13, 201, 30507, 11, 9505, 1366, 4947, 11, 4009, 11, 12724, 11, 6447, 11, 32704, 11, 290, 7585, 13, 201, 34888, 201, 43467, 1366, 7572, 290, 11257, 13, 201, 5124, 3039, 1366, 31108, 290, 45780, 1366, 7767, 13, 201, 49, 4316, 201, 6601, 13440, 13, 201, 6601, 12037, 329, 6884, 290, 26355, 11, 1366, 13440, 329, 3781, 6805, 13, 201, 15739, 2171, 201, 17126, 19929, 3781, 11, 21128, 11, 16363, 11, 32704, 13, 201, 6601, 6143, 11, 21128, 11, 6946, 11, 10314, 13, 201, 28813, 1945, 201, 6601, 3781, 318, 257, 7515, 286, 1366, 23696, 11, 475, 407, 7927, 25470, 13, 201, 3237, 1366, 3781, 318, 257, 7515, 286, 1366, 23696, 11, 475, 407, 477, 23696, 7767, 6211, 3781, 13, 201, 31431, 201, 24564, 1968, 425, 11, 23584, 11, 33344, 11, 906, 6519, 425, 3781, 13, 201, 24749, 4430, 11, 3176, 21128, 11, 7124, 2267, 13, 201, 25574, 6368, 201, 17126, 19929, 3781, 11, 21128, 11, 16363, 11, 32704, 13, 201, 6601, 11824, 11, 13905, 3781, 11, 21128, 11, 1366, 32704, 13, 201, 42166, 896, 201, 49026, 2551, 12, 8601, 1912, 319, 6754, 1366, 13, 201, 35453, 7767, 11, 2003, 16277, 11, 290, 4542, 4028, 13, 201, 11828, 201, 3103, 1087, 4111, 3842, 1626, 262, 1366, 23696, 11523, 13, 201, 7293, 36321, 1429, 7411, 2972, 9176, 290, 4678, 287, 11149, 1366, 31108, 13, 201, 201, 198, 18839, 14492, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 11828, 201, 36621, 201, 33129, 286, 319, 12, 28550, 14492, 4133, 357, 3262, 5225, 11, 9597, 11, 6143, 11, 5479, 8, 625, 262, 4455, 11, 6032, 319, 257, 1414, 12, 292, 12, 5832, 12, 2188, 4308, 13, 201, 5842, 274, 201, 12, 5313, 12, 3106, 5479, 357, 68, 13, 70, 1539, 3012, 9974, 11, 38930, 8, 201, 198, 532, 7320, 3788, 357, 68, 13, 70, 1539, 17329, 3174, 8, 201, 198, 12, 6060, 6143, 290, 11559, 201, 198, 12, 14504, 278, 9293, 290, 5479, 201, 42166, 896, 201, 12, 6446, 10653, 357, 3919, 761, 329, 5789, 6884, 8, 201, 198, 12, 34529, 1799, 357, 11201, 392, 4133, 355, 2622, 8, 201, 198, 12, 8798, 2247, 357, 15526, 422, 597, 3335, 351, 5230, 4637, 8, 201, 30611, 782, 9998, 201, 12, 3334, 13688, 290, 16578, 1799, 201, 198, 12, 2297, 26873, 6890, 3484, 201, 198, 12, 37322, 341, 832, 1103, 12, 2435, 1895, 201, 198, 12, 23603, 5992, 290, 9262, 416, 9549, 201, 44898, 47556, 201, 12, 4765, 7476, 357, 7890, 31523, 11, 15656, 8, 201, 198, 12, 37947, 1387, 319, 5230, 19843, 201, 198, 12, 32480, 329, 45832, 201, 198, 12, 15302, 1630, 625, 6884, 4542, 201, 201, 198, 32, 7208, 3084, 1022, 314, 7252, 50, 11, 350, 7252, 50, 11, 290, 311, 7252, 50, 532, 30297, 10692, 8581, 1114, 6060, 5800, 290, 35941, 9345, 201, 1722, 806, 201, 40, 7252, 50, 357, 18943, 6410, 355, 257, 4809, 8, 201, 47, 7252, 50, 357, 37148, 355, 257, 4809, 8, 201, 50, 7252, 50, 357, 25423, 355, 257, 4809, 8, 201, 36621, 201, 15946, 1460, 7166, 1143, 14492, 4133, 588, 9597, 11, 6143, 11, 290, 19140, 201, 15946, 1460, 9554, 329, 6505, 284, 1382, 11, 1332, 11, 290, 6061, 5479, 201, 13856, 1191, 3788, 5479, 625, 262, 5230, 11, 12007, 416, 257, 10131, 201, 12982, 42988, 201, 14490, 6687, 262, 6884, 357, 2640, 11, 6143, 11, 19140, 11, 3503, 2014, 201, 14490, 6687, 5479, 290, 1366, 11, 475, 262, 3859, 318, 9456, 416, 262, 10131, 201, 14490, 691, 6687, 3788, 6460, 11, 351, 262, 10131, 11149, 6884, 290, 3859, 201, 11041, 8913, 201, 13014, 329, 7283, 44563, 393, 2706, 18139, 1844, 1630, 625, 511, 6884, 201, 7390, 2287, 329, 6505, 18139, 257, 3492, 12, 1462, 12, 1904, 3859, 329, 2615, 6725, 201, 13014, 329, 886, 12, 18417, 10616, 1895, 284, 6279, 12, 3106, 3788, 5479, 201, 27730, 201, 24888, 5313, 6168, 357, 12298, 50, 828, 5413, 22134, 11, 3012, 3082, 1133, 7117, 201, 11708, 2034, 7117, 11, 5413, 22134, 350, 7252, 50, 11, 2332, 11601, 201, 11708, 10933, 10223, 357, 38, 4529, 11, 14432, 82, 828, 17329, 3174, 11, 38930, 201, 3351, 282, 1799, 201, 11922, 16578, 1799, 11, 2985, 460, 2620, 14, 12501, 260, 589, 4133, 355, 2622, 201, 3351, 282, 540, 2478, 2858, 201, 25423, 16578, 1799, 1912, 319, 14569, 393, 15665, 201, 15988, 201, 36124, 3634, 1241, 286, 1630, 625, 4133, 290, 6884, 201, 5841, 21620, 1630, 625, 5479, 475, 1342, 625, 262, 10238, 6884, 201, 3123, 459, 1630, 11, 2985, 2962, 319, 1262, 262, 3788, 1231, 18916, 546, 6884, 201, 13729, 9104, 201, 19197, 329, 7166, 1143, 6890, 357, 2655, 690, 11, 6143, 8, 201, 19197, 329, 3859, 290, 4899, 357, 19608, 18826, 11, 3504, 1574, 8, 201, 7004, 33584, 12, 3106, 13045, 329, 3788, 1895, 201, 2782, 4520, 1095, 201, 12, 6462, 1630, 625, 6884, 201, 12, 45157, 6945, 2478, 201, 12, 16789, 284, 779, 201, 201, 12, 26719, 856, 201, 12, 2297, 26873, 640, 284, 6061, 201, 12, 8798, 856, 422, 6609, 201, 201, 12, 8562, 13821, 201, 12, 1400, 761, 284, 6687, 10238, 6884, 201, 12, 1400, 9262, 393, 9988, 2672, 201, 7279, 13461, 1095, 201, 12, 19157, 284, 6687, 201, 12, 15302, 31344, 286, 3859, 201, 12, 12892, 1630, 625, 3788, 290, 663, 8398, 201, 201, 12, 26848, 16976, 3725, 329, 8398, 201, 12, 12892, 1630, 625, 6884, 201, 12, 37947, 298, 319, 262, 10131, 329, 5992, 201, 201, 198, 464, 1936, 6393, 9695, 286, 6279, 14492, 12, 30297, 10692, 8581, 201, 27275, 2569, 201, 11828, 201, 2202, 12, 28550, 12189, 12, 15271, 201, 14490, 460, 1895, 14492, 4133, 357, 68, 13, 70, 1539, 7587, 1176, 11, 6143, 11, 3127, 8, 355, 2622, 11, 1231, 10616, 1692, 9572, 422, 262, 2139, 10131, 13, 201, 30507, 7311, 8798, 201, 18839, 2594, 460, 307, 17535, 625, 262, 3127, 422, 257, 3094, 2837, 286, 4410, 884, 355, 18151, 11, 17255, 11, 26635, 11, 290, 6915, 35011, 13, 201, 26198, 19850, 278, 201, 5377, 48074, 4133, 389, 44762, 284, 4691, 3294, 2985, 1262, 257, 5021, 12, 1452, 415, 2746, 11, 351, 8925, 8271, 20157, 1912, 319, 3512, 13, 18987, 389, 17261, 286, 7064, 13, 201, 35230, 312, 48567, 414, 201, 33236, 460, 307, 2952, 27464, 510, 393, 866, 284, 1826, 2836, 8665, 11, 4955, 12846, 290, 43865, 8271, 8287, 278, 13, 201, 5308, 34006, 4809, 201, 26198, 8748, 318, 20738, 290, 8630, 11, 5086, 2985, 284, 307, 5047, 1912, 319, 4036, 7327, 11, 13359, 1575, 9332, 290, 13902, 13, 201, 201, 198, 201, 198, 32, 7208, 286, 262, 1115, 6279, 14833, 4981, 12, 30297, 10692, 8581, 201, 49322, 434, 9104, 201, 11828, 201, 2782, 4520, 1095, 201, 7279, 13461, 1095, 201, 15202, 10130, 201, 18839, 2594, 389, 2810, 625, 262, 1171, 5230, 290, 4888, 1871, 3294, 5745, 13, 201, 13729, 12, 16803, 11, 43865, 11, 645, 761, 329, 287, 12, 4803, 6890, 11, 2562, 284, 1895, 13, 201, 22058, 1630, 625, 2324, 11, 2785, 329, 1366, 6782, 4786, 11, 4888, 4133, 351, 584, 5745, 13, 201, 29067, 10130, 201, 18839, 6884, 318, 11541, 973, 416, 257, 2060, 4009, 11, 2035, 5257, 20947, 393, 416, 257, 2368, 12, 10608, 11, 12007, 319, 12, 31605, 2696, 393, 572, 12, 15654, 13, 201, 13681, 263, 1630, 625, 2324, 11, 27658, 329, 26564, 2476, 11, 13105, 6782, 13, 201, 48708, 3484, 2233, 284, 7256, 6884, 11, 4433, 517, 4542, 11, 1342, 43865, 621, 1171, 6279, 13, 201, 21217, 10236, 10130, 201, 20575, 1127, 1171, 290, 2839, 15114, 11, 5086, 1366, 290, 5479, 284, 307, 4888, 1022, 606, 13, 201, 37, 2588, 2247, 11, 23392, 8271, 32121, 11, 3578, 8564, 1366, 284, 3520, 287, 2839, 6279, 981, 42389, 262, 16578, 1799, 286, 262, 1171, 6279, 13, 201, 5377, 11141, 4542, 11, 2440, 1575, 621, 5899, 1171, 6279, 11, 2785, 11812, 2428, 1022, 1171, 290, 2839, 15114, 13, 201, 201, 198, 201, 198, 32, 7208, 286, 262, 1115, 6279, 2139, 4981, 12, 30297, 10692, 8581, 201, 16177, 9104, 201, 11828, 201, 9218, 17571, 201, 27730, 201, 2782, 4520, 1095, 201, 7279, 13461, 1095, 201, 18943, 6410, 355, 257, 4809, 357, 40, 7252, 50, 8, 201, 15946, 1460, 7166, 1143, 14492, 4133, 625, 262, 5230, 11, 1390, 9597, 11, 6143, 11, 290, 19140, 13, 201, 15988, 625, 6884, 11, 1414, 12, 292, 12, 5832, 12, 2188, 2746, 11, 43865, 13, 201, 24888, 5313, 6168, 357, 12298, 50, 828, 5413, 22134, 11, 3012, 10130, 201, 37, 2588, 2247, 284, 6687, 290, 1630, 6884, 11, 16578, 1799, 11, 1575, 12, 16803, 329, 1588, 26211, 82, 13, 201, 39618, 6276, 13572, 284, 6687, 11, 2440, 5798, 329, 2324, 11, 1080, 5992, 11, 290, 9262, 13, 201, 37148, 355, 257, 4809, 357, 47, 7252, 50, 8, 201, 15946, 1460, 257, 3859, 326, 3578, 6505, 284, 1382, 11, 6061, 11, 290, 6687, 5479, 1231, 7219, 351, 262, 10238, 6884, 13, 201, 6719, 12, 11250, 1522, 3859, 11, 3586, 2478, 4899, 11, 20796, 13, 201, 11708, 2034, 7117, 11, 2332, 11601, 11, 5413, 22134, 2034, 6168, 201, 7738, 26873, 640, 329, 598, 2478, 11, 2562, 14833, 11, 645, 6884, 4542, 2622, 11, 43865, 13, 201, 37214, 1630, 625, 6884, 11, 1342, 13688, 329, 1728, 25412, 11, 2785, 18371, 5793, 12, 259, 13, 201, 25423, 355, 257, 4809, 357, 50, 7252, 50, 8, 201, 13856, 1191, 3788, 5479, 625, 262, 5230, 319, 257, 14569, 4308, 11, 351, 262, 10131, 11149, 6884, 11, 3859, 11, 290, 3788, 13, 201, 17932, 276, 3788, 11, 9857, 2884, 6444, 11, 645, 9262, 2672, 13, 201, 11708, 10933, 10223, 11, 17329, 3174, 11, 38930, 201, 2949, 761, 329, 9988, 393, 5992, 11, 9857, 422, 597, 3335, 11, 20039, 14569, 3484, 11, 10131, 15314, 2279, 13, 201, 37214, 31344, 11, 1366, 2324, 4786, 11, 20203, 319, 5230, 19843, 11, 1342, 1630, 625, 3788, 5992, 14, 40890, 13, 201, 201, 198, 32, 7208, 3084, 329, 262, 642, 31280, 286, 4403, 6060, 12, 30297, 10692, 8581, 329, 6060, 5800, 290, 35941, 9345, 201, 12804, 6060, 201, 36621, 201, 16281, 201, 41812, 34120, 201, 27524, 1922, 871, 201, 46261, 11683, 201, 464, 2866, 379, 543, 1366, 318, 7560, 290, 13686, 13, 201, 33869, 9516, 82, 2250, 286, 2008, 790, 5664, 13, 201, 18709, 278, 290, 22712, 1366, 287, 1103, 12, 2435, 13, 201, 15633, 12, 2435, 17218, 329, 3049, 2551, 12, 8601, 13, 201, 31715, 201, 464, 1588, 5046, 393, 12040, 286, 1366, 7560, 13, 201, 17, 13, 20, 28533, 1131, 9881, 286, 1366, 389, 4635, 4445, 13, 201, 1273, 3255, 290, 11149, 4858, 40522, 13, 201, 22453, 284, 7925, 8119, 17218, 422, 1588, 40522, 13, 201, 19852, 1905, 201, 464, 9573, 286, 1366, 3858, 357, 7249, 1522, 11, 555, 7249, 1522, 737, 201, 8206, 11, 4263, 11, 5861, 11, 12694, 1366, 422, 38488, 4410, 13, 201, 34500, 8821, 290, 7587, 1180, 3858, 286, 1366, 13, 201, 5167, 9815, 3781, 416, 19771, 2972, 1366, 3858, 13, 201, 13414, 4355, 201, 464, 9922, 290, 17843, 286, 262, 1366, 13, 201, 1795, 4, 286, 1366, 318, 555, 7249, 1522, 11, 543, 743, 3092, 15794, 13, 201, 4834, 82, 870, 1366, 3081, 290, 18591, 3991, 1321, 13, 201, 28971, 2551, 12, 8601, 351, 7187, 1366, 13, 201, 11395, 201, 464, 2694, 284, 27099, 11570, 17218, 422, 1366, 13, 201, 24749, 17218, 11, 6491, 4069, 11, 2526, 4542, 13, 201, 11627, 974, 278, 1988, 422, 5909, 11, 3716, 40522, 13, 201, 40281, 7630, 11, 9332, 11, 290, 11044, 832, 1366, 12, 15808, 5370, 13, 201, 201, 198, 201, 198, 201, 48485, 290, 649, 835, 287, 28403, 286, 4403, 6060, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 48485, 6060, 28403, 201, 12804, 6060, 28403, 201, 12804, 6060, 21852, 201, 34500, 1358, 286, 49686, 201, 6601, 5800, 290, 30437, 19045, 201, 32779, 2667, 34308, 201, 6601, 49500, 201, 6601, 3181, 284, 262, 3644, 329, 7587, 13, 201, 6601, 318, 26790, 11, 9387, 11, 290, 13686, 1973, 3294, 9061, 13, 201, 39, 4533, 404, 9355, 329, 1263, 1366, 13, 201, 20575, 1127, 4569, 5050, 357, 68, 13, 70, 1539, 7869, 8, 351, 649, 7605, 13, 201, 35230, 312, 3349, 290, 3649, 23082, 13, 201, 15200, 2122, 286, 2769, 4673, 290, 17019, 7686, 13, 201, 18709, 278, 38066, 201, 15167, 4539, 319, 262, 1844, 1366, 900, 13, 201, 13912, 7738, 7234, 25, 9347, 1429, 1233, 7657, 1366, 11, 44048, 1429, 13262, 689, 2482, 13, 201, 19246, 276, 422, 3012, 338, 3164, 13, 201, 11041, 286, 4572, 4673, 284, 16602, 1588, 1366, 5621, 13, 201, 6601, 3783, 2214, 373, 1342, 1900, 1936, 812, 2084, 13, 201, 11712, 17294, 47220, 287, 2274, 812, 13, 201, 3351, 282, 1799, 201, 37214, 416, 2060, 3644, 5339, 13, 201, 14993, 451, 20796, 25, 11198, 262, 9597, 11, 4274, 262, 2854, 14, 7890, 9041, 13, 201, 39, 4533, 404, 16252, 351, 262, 1366, 13, 201, 3351, 282, 1799, 8793, 832, 11812, 286, 31350, 7605, 13, 201, 6601, 3783, 13977, 23692, 8902, 13, 201, 40281, 3586, 416, 1688, 7261, 2706, 13, 201, 44893, 17677, 201, 23615, 20083, 290, 8300, 7605, 13, 201, 50237, 560, 2928, 319, 1366, 7587, 290, 9041, 13, 201, 24206, 2928, 319, 1263, 1366, 4542, 13, 201, 20575, 1127, 4569, 7869, 351, 4572, 4673, 14901, 13, 201, 43964, 6817, 287, 1597, 290, 3037, 13, 201, 29744, 4673, 3037, 11581, 8902, 13, 201, 15878, 15815, 201, 48485, 7605, 287, 779, 329, 4647, 13, 201, 3791, 7605, 4166, 284, 5412, 1588, 40522, 357, 68, 13, 70, 1539, 367, 4533, 404, 737, 201, 11610, 3898, 422, 3012, 338, 11044, 13, 201, 31439, 31350, 9889, 9494, 4569, 5050, 13, 201, 11712, 17294, 2478, 287, 262, 938, 1178, 812, 13, 201, 35230, 312, 2478, 287, 17019, 3127, 9889, 13, 201, 41995, 201, 37214, 416, 262, 9889, 286, 4569, 3341, 13, 201, 4834, 2977, 3781, 286, 5909, 290, 3716, 1366, 5621, 13, 201, 38052, 416, 867, 2706, 329, 1263, 1366, 13, 201, 4677, 18511, 287, 10084, 3006, 422, 1597, 284, 2267, 13, 201, 32779, 2667, 355, 257, 4688, 2214, 287, 23696, 13, 201, 38052, 20823, 416, 7261, 20178, 13, 201, 201, 198, 201, 198, 38816, 201, 23159, 201, 38475, 35, 357, 1273, 5374, 3477, 17701, 1153, 2935, 1087, 8, 201, 13295, 6530, 201, 48003, 425, 29278, 10062, 18991, 201, 1273, 5374, 3477, 17701, 1153, 2935, 1087, 201, 41730, 14806, 201, 48003, 425, 357, 929, 19581, 262, 4673, 2494, 329, 1123, 11507, 8, 201, 13715, 393, 14500, 7530, 357, 5171, 22119, 625, 640, 8, 201, 10260, 14330, 201, 20575, 1127, 12858, 357, 11085, 2589, 8, 290, 29605, 4673, 3965, 357, 12227, 2589, 8, 201, 13470, 306, 5992, 262, 19590, 1262, 262, 31312, 290, 4673, 2494, 201, 29252, 298, 388, 201, 29710, 3628, 306, 33181, 12858, 832, 717, 290, 1218, 7188, 357, 38002, 26251, 8, 201, 39618, 4553, 10107, 12858, 3381, 357, 25968, 8, 201, 3103, 332, 12745, 8729, 201, 37058, 5443, 2233, 284, 29605, 4673, 3965, 201, 11122, 789, 11, 2592, 618, 4673, 2494, 318, 407, 880, 12, 28286, 276, 201, 30871, 29566, 201, 39618, 517, 4088, 357, 43409, 717, 290, 1218, 7188, 8, 201, 22058, 4088, 12, 38096, 357, 8807, 7000, 3915, 2334, 8, 201, 11041, 35536, 201, 23044, 880, 329, 31210, 290, 29877, 1366, 11, 1690, 973, 287, 2769, 4673, 201, 5606, 4674, 329, 18599, 393, 880, 12, 28286, 276, 8861, 11, 4569, 4572, 4673, 201, 38197, 17143, 7307, 201, 39618, 24549, 220, 201, 13383, 306, 262, 4673, 2494, 357, 392, 12858, 611, 973, 8, 201, 33, 4448, 35074, 201, 5297, 11, 10690, 12, 30283, 276, 7746, 286, 717, 290, 1218, 7188, 201, 2949, 10690, 17137, 201, 50, 40545, 284, 18252, 14806, 201, 22058, 8564, 284, 262, 4238, 4673, 2494, 357, 23301, 284, 6068, 3458, 8, 201, 11922, 306, 8564, 26, 4433, 8161, 24549, 201, 33583, 1799, 201, 44831, 329, 257, 3094, 2837, 286, 8861, 11, 2592, 2769, 17019, 7686, 201, 5606, 4674, 329, 4833, 4981, 290, 18599, 23989, 8861, 201, 30611, 782, 9998, 201, 48003, 425, 4673, 3965, 26, 2499, 880, 329, 1588, 40522, 290, 4981, 201, 26437, 290, 6942, 329, 24748, 87, 23989, 2761, 201, 44898, 47556, 201, 5167, 2653, 15208, 18590, 11, 743, 625, 11147, 319, 1728, 2761, 201, 6836, 505, 284, 3105, 40826, 393, 1972, 7819, 287, 1957, 949, 8083, 201, 31467, 605, 5765, 201, 29744, 4673, 8861, 11, 2592, 8100, 82, 11, 371, 6144, 82, 11, 399, 19930, 11, 3503, 13, 201, 26416, 31312, 12, 3106, 23989, 11, 1402, 4981, 201, 201, 198, 201, 198, 201, 15167, 2229, 10692, 8581, 329, 6060, 5800, 290, 9552, 201, 464, 5087, 14329, 284, 262, 1575, 286, 2491, 257, 13601, 15417, 9104, 357, 3069, 44, 8, 201, 13729, 27929, 201, 11828, 201, 17633, 12849, 201, 43, 32270, 4981, 351, 517, 10007, 2421, 517, 4088, 357, 24115, 14, 13024, 2390, 8, 290, 31350, 4133, 13, 201, 49865, 36845, 201, 33346, 82, 393, 309, 5105, 82, 329, 3047, 14, 259, 4288, 389, 5789, 26, 6884, 3407, 9597, 11, 15134, 11, 290, 1176, 13, 201, 28925, 42158, 201, 11922, 2568, 779, 1141, 3047, 290, 32278, 5732, 13919, 3484, 13, 201, 44357, 36845, 201, 14617, 3047, 1661, 11, 4858, 40522, 11, 290, 8718, 17143, 2357, 24549, 5298, 24061, 290, 2568, 3484, 13, 201, 818, 4288, 36845, 201, 20746, 13357, 11, 15458, 2546, 11, 290, 1877, 12, 15460, 1387, 5359, 2689, 8271, 8748, 1141, 1103, 12, 2435, 32278, 13, 201, 18839, 14504, 278, 3691, 13, 1550, 12, 24914, 786, 201, 18839, 3484, 2291, 24061, 2250, 11, 6143, 11, 290, 20796, 26, 319, 12, 31605, 786, 3484, 3002, 6884, 290, 9262, 13, 201, 25423, 1222, 10483, 26426, 36845, 201, 17699, 3484, 329, 23989, 4899, 11, 9387, 3047, 11, 290, 7824, 8748, 13, 201, 18843, 864, 1222, 34857, 36845, 201, 42986, 2746, 19698, 11, 1005, 24674, 11, 290, 36700, 3484, 329, 12037, 290, 1366, 5519, 13, 201, 24074, 1222, 40536, 201, 27195, 13168, 11, 5713, 6143, 11, 290, 11846, 351, 1366, 6782, 6647, 751, 284, 262, 13919, 16965, 13, 201, 7934, 16090, 1222, 10243, 10394, 201, 6601, 4351, 11, 2592, 1973, 7652, 393, 15114, 11, 290, 13359, 1877, 12, 15460, 1387, 2882, 1661, 2620, 3484, 13, 201, 201, 198, 464, 1688, 1440, 3858, 287, 262, 10850, 4673, 11862, 12, 15167, 2229, 10692, 8581, 201, 1722, 806, 201, 9487, 2649, 201, 8081, 2234, 201, 2601, 436, 1586, 201, 29271, 3004, 1483, 33396, 201, 30026, 3455, 201, 34, 47467, 1096, 1366, 656, 28810, 6097, 201, 47, 17407, 12948, 3815, 201, 13247, 2092, 1366, 2173, 1231, 14722, 201, 7738, 7234, 262, 1271, 286, 3033, 981, 23934, 262, 24198, 201, 9218, 978, 7727, 907, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 12, 311, 15922, 201, 12, 406, 28372, 201, 12, 509, 5308, 504, 201, 12, 4217, 32, 201, 201, 12, 509, 46445, 32289, 201, 12, 48567, 7934, 201, 12, 13058, 1373, 1012, 436, 1586, 201, 12, 1148, 296, 499, 201, 201, 12, 11013, 425, 4696, 274, 201, 12, 20614, 201, 12, 6951, 44, 201, 12, 406, 2538, 201, 201, 12, 26147, 35, 5016, 7483, 201, 12, 311, 13024, 201, 220, 201, 12, 13058, 1373, 13302, 6048, 278, 201, 36674, 12849, 12642, 602, 201, 198, 220, 201, 12, 10933, 351, 1111, 1402, 38155, 3064, 42, 8, 290, 1588, 40522, 201, 12, 1114, 4833, 40522, 38155, 3064, 42, 828, 11080, 5050, 588, 26147, 35, 3310, 44292, 201, 12, 509, 5308, 504, 318, 922, 329, 1402, 40522, 38155, 940, 42, 8, 201, 12, 10933, 351, 4833, 40522, 38155, 940, 42, 8, 351, 16113, 588, 4217, 32, 11, 1148, 296, 499, 201, 201, 220, 201, 12, 311, 13024, 329, 1402, 290, 7090, 40522, 201, 12, 12558, 33, 963, 509, 5308, 504, 329, 1588, 40522, 201, 220, 201, 33986, 276, 6060, 9394, 24615, 201, 39618, 15494, 1366, 329, 3047, 201, 39618, 15494, 1366, 357, 40890, 290, 2496, 3815, 8, 201, 13921, 407, 2421, 15494, 1366, 201, 49321, 555, 16668, 16149, 26, 857, 407, 761, 14722, 201, 31431, 286, 14322, 9278, 201, 47, 17407, 9376, 357, 68, 13, 70, 1539, 18084, 393, 407, 18084, 8, 201, 47, 17407, 12948, 17794, 357, 68, 13, 70, 1539, 2156, 2756, 8, 201, 16742, 1448, 654, 287, 1366, 357, 68, 13, 70, 1539, 6491, 17894, 8, 201, 41762, 1366, 284, 2793, 15225, 357, 68, 13, 70, 1539, 38350, 1029, 12, 19577, 1366, 8, 201, 17227, 44495, 201, 198, 220, 201, 12, 10031, 2752, 1022, 14174, 290, 1729, 12, 29127, 1398, 13350, 201, 12, 3827, 32232, 351, 3716, 4981, 201, 12, 360, 13221, 278, 262, 16586, 1271, 286, 23163, 201, 12, 22014, 286, 1321, 618, 8868, 15225, 201, 201, 12, 49500, 545, 27753, 1366, 201, 12, 27018, 6817, 201, 12, 34529, 1799, 351, 1588, 1366, 201, 12, 10031, 2752, 262, 3376, 1271, 286, 6805, 201, 9218, 26423, 41140, 201, 198, 220, 201, 198, 220, 201, 12, 12849, 286, 27039, 201, 12, 17267, 590, 286, 3033, 201, 12, 7913, 286, 23163, 201, 12, 7913, 286, 3033, 201, 201, 12, 8255, 1366, 6050, 1799, 201, 12, 7913, 286, 8405, 201, 12, 12849, 286, 27039, 201, 12, 15965, 590, 22459, 201, 201, 12, 7913, 286, 6097, 201, 12, 10127, 3033, 389, 23551, 201, 12, 7913, 286, 1900, 9376, 201, 12, 22476, 864, 9332, 201, 9218, 5765, 35536, 201, 198, 220, 201, 198, 220, 201, 12, 7412, 393, 2420, 17923, 201, 12, 49461, 278, 4283, 4536, 201, 12, 5991, 10618, 341, 201, 12, 6060, 32704, 201, 201, 12, 1338, 321, 13326, 201, 12, 4558, 19913, 4200, 201, 12, 5483, 3127, 3781, 201, 12, 3771, 36948, 329, 584, 16113, 201, 201, 12, 17344, 13669, 201, 12, 2097, 2756, 17724, 201, 12, 7412, 10618, 341, 201, 12, 30964, 7741, 201, 201, 198, 201, 198, 29271, 3004, 1483, 33396, 7605, 12, 30297, 10692, 8581, 201, 25574, 2350, 201, 11828, 201, 5842, 274, 201, 35726, 201, 9444, 201, 7390, 2287, 1446, 268, 13010, 201, 42904, 8521, 35100, 14691, 357, 5662, 32, 8, 201, 198, 220, 201, 198, 220, 201, 32, 14174, 8173, 326, 4493, 1366, 4291, 2793, 12, 19577, 9029, 981, 48350, 24198, 13, 201, 198, 220, 201, 198, 220, 201, 12, 6060, 19794, 201, 198, 12, 15612, 1634, 201, 198, 12, 30964, 7741, 201, 12, 17427, 284, 3494, 201, 198, 12, 4599, 329, 1588, 40522, 201, 198, 220, 201, 12, 2195, 8139, 14174, 414, 201, 198, 12, 1680, 4425, 6179, 1799, 201, 198, 220, 201, 12, 1649, 262, 1366, 468, 257, 1588, 1271, 286, 3033, 351, 49052, 393, 16096, 201, 198, 220, 201, 198, 220, 201, 14993, 451, 8444, 3036, 42483, 14691, 357, 43, 5631, 8, 201, 198, 220, 201, 16775, 82, 1366, 4291, 2793, 15225, 981, 48350, 262, 14139, 1022, 6097, 13, 201, 198, 220, 201, 12, 27018, 22236, 329, 17923, 2761, 201, 198, 220, 201, 12, 12205, 1158, 1398, 14139, 201, 198, 12, 22476, 15208, 6942, 201, 12, 5514, 2499, 329, 15494, 1366, 201, 198, 12, 2195, 8139, 12822, 31562, 6082, 201, 12, 1649, 1398, 14139, 318, 1593, 329, 17923, 8861, 201, 198, 220, 201, 42, 7948, 4217, 32, 201, 198, 220, 201, 15419, 12, 29127, 2196, 286, 4217, 32, 1262, 9720, 5050, 284, 8006, 1729, 12, 29127, 6958, 13, 201, 198, 220, 201, 12, 8504, 12, 29127, 3895, 7741, 201, 198, 12, 3771, 36948, 329, 20546, 10128, 201, 12, 6790, 942, 3716, 6958, 201, 198, 12, 26719, 856, 329, 1729, 12, 29127, 1366, 201, 12, 22476, 15208, 5789, 201, 198, 12, 26848, 8161, 3572, 286, 50207, 201, 12, 1649, 262, 27039, 468, 3716, 11, 1729, 12, 29127, 6958, 290, 7572, 201, 198, 220, 201, 83, 12, 20344, 6169, 520, 5374, 3477, 28708, 13302, 6048, 278, 357, 83, 12, 50, 12161, 8, 201, 198, 220, 201, 15419, 12, 29127, 8173, 326, 43759, 262, 1957, 4645, 286, 1366, 290, 318, 8384, 973, 329, 32704, 13, 201, 198, 220, 201, 12, 6060, 32704, 201, 198, 12, 5905, 3255, 1029, 12, 19577, 1366, 201, 12, 30932, 329, 5874, 2890, 3716, 11, 1029, 12, 19577, 1366, 201, 198, 220, 201, 12, 22476, 15208, 5789, 201, 198, 12, 1892, 11080, 329, 1588, 40522, 201, 12, 1114, 5874, 2890, 3716, 40522, 287, 362, 35, 393, 513, 35, 11, 884, 355, 287, 39180, 2870, 1366, 3781, 201, 198, 220, 201, 3792, 296, 499, 201, 198, 220, 201, 32, 48048, 4673, 8173, 326, 12932, 284, 12201, 262, 3298, 4645, 286, 262, 1366, 13, 201, 198, 220, 201, 12, 8504, 12, 29127, 15793, 1483, 7741, 201, 198, 12, 15612, 2890, 19871, 10119, 201, 12, 6790, 942, 3298, 4645, 201, 198, 12, 29455, 329, 48048, 1366, 201, 12, 34887, 32723, 351, 7838, 201, 198, 12, 22476, 15208, 5789, 201, 12, 1649, 262, 1366, 7363, 319, 257, 1877, 12, 19577, 48048, 1626, 257, 1029, 12, 19577, 2272, 201, 198, 220, 201, 33711, 453, 44800, 13302, 6048, 278, 357, 43, 2538, 8, 201, 198, 220, 201, 32, 48048, 4673, 11862, 326, 43759, 1957, 14287, 287, 2793, 12, 19577, 19887, 13, 201, 198, 220, 201, 12, 8504, 12, 29127, 15793, 1483, 7741, 201, 198, 12, 15612, 1634, 201, 12, 1763, 11184, 1957, 8573, 201, 198, 12, 29455, 329, 1029, 12, 19577, 1366, 201, 12, 14173, 1800, 284, 7838, 201, 198, 12, 8314, 407, 12201, 3298, 4645, 201, 12, 1649, 262, 3061, 318, 284, 12201, 1957, 6958, 287, 1029, 12, 19577, 1366, 201, 198, 220, 201, 41384, 14691, 357, 7708, 8, 201, 198, 220, 201, 32, 13905, 2446, 326, 4981, 6515, 9633, 355, 14174, 17790, 286, 2785, 41270, 5087, 13, 201, 198, 220, 201, 12, 27018, 22236, 201, 198, 12, 6060, 13936, 201, 12, 45157, 6945, 1366, 10794, 201, 198, 12, 4599, 329, 23551, 9633, 201, 12, 2195, 8139, 14174, 6958, 201, 198, 12, 48907, 1799, 460, 307, 3716, 201, 12, 1649, 262, 9432, 318, 284, 5911, 10238, 5087, 11170, 35811, 1022, 3033, 201, 198, 220, 201, 16541, 6571, 19815, 364, 201, 198, 220, 201, 198, 220, 201, 8199, 1523, 3127, 12, 3106, 8173, 326, 22974, 6942, 24612, 286, 1366, 13, 201, 198, 220, 201, 198, 220, 201, 12, 27018, 4673, 201, 198, 12, 7412, 19794, 201, 198, 12, 5601, 78, 1710, 201, 12, 1680, 2746, 3716, 1729, 12, 29127, 5499, 201, 198, 12, 38254, 38322, 201, 198, 220, 201, 12, 26848, 257, 1256, 286, 1366, 290, 24549, 201, 198, 12, 22476, 15208, 5789, 201, 198, 220, 201, 12, 1649, 1762, 351, 1588, 40522, 290, 1729, 12, 29127, 1366, 11, 2592, 287, 2769, 4673, 26307, 201, 198, 220, 201, 198, 220, 201, 40566, 35100, 14691, 357, 25241, 8, 201, 198, 220, 201, 32, 8173, 326, 31555, 257, 1963, 42524, 6737, 656, 38298, 11, 4795, 6805, 13, 201, 198, 220, 201, 12, 24507, 2723, 14139, 201, 198, 12, 26484, 7587, 201, 12, 9938, 82, 4795, 3033, 201, 198, 12, 4599, 329, 1729, 12, 35389, 31562, 1366, 201, 12, 2195, 8139, 13905, 10404, 201, 198, 12, 14173, 1800, 284, 7838, 201, 12, 1649, 262, 4876, 318, 284, 7925, 4795, 10425, 393, 4237, 11, 884, 355, 287, 6597, 7587, 201, 198, 220, 201, 15205, 312, 16198, 1446, 4272, 357, 44, 5258, 8, 201, 198, 220, 201, 32, 1729, 12, 29127, 8173, 326, 4493, 1366, 656, 2793, 15225, 416, 23934, 5166, 3083, 18868, 13, 201, 198, 220, 201, 12, 15612, 2890, 26789, 393, 6249, 49941, 414, 1022, 1366, 2173, 201, 198, 220, 201, 12, 1763, 11184, 18868, 201, 198, 12, 29455, 329, 6249, 49941, 414, 1366, 201, 12, 22476, 15208, 5789, 201, 198, 12, 15302, 16578, 1799, 201, 12, 1649, 23934, 262, 5253, 1022, 1366, 2173, 318, 1593, 11, 884, 355, 287, 49615, 16855, 201, 198, 220, 201, 29531, 1143, 4217, 32, 201, 198, 220, 201, 32, 5443, 40874, 286, 4217, 32, 329, 1588, 40522, 326, 3544, 4738, 19887, 13, 201, 198, 220, 201, 12, 12549, 1366, 19794, 201, 198, 12, 20768, 3895, 7741, 201, 12, 412, 5632, 329, 845, 1588, 40522, 201, 198, 12, 4990, 1299, 749, 24198, 201, 12, 5514, 281, 40874, 286, 2081, 4217, 32, 201, 198, 12, 22014, 286, 15440, 201, 12, 1649, 31350, 4133, 389, 3614, 290, 262, 27039, 318, 4457, 1588, 201, 198, 220, 201, 2898, 19524, 515, 5573, 934, 11052, 4280, 296, 9150, 357, 50, 8898, 8, 201, 198, 220, 201, 198, 220, 201, 32, 17593, 5766, 1634, 8173, 973, 329, 15793, 1483, 7741, 11, 3573, 287, 29877, 1366, 13, 201, 198, 220, 201, 198, 220, 201, 12, 1338, 17208, 1366, 201, 198, 12, 8255, 3781, 201, 198, 12, 24936, 5766, 1634, 201, 12, 7157, 829, 29877, 1366, 880, 201, 198, 12, 49511, 329, 2420, 1366, 201, 198, 220, 201, 12, 406, 4629, 6179, 1799, 201, 198, 12, 2195, 8139, 14174, 6958, 201, 198, 220, 201, 12, 1649, 7219, 351, 29877, 1366, 11, 884, 355, 2420, 357, 68, 13, 70, 1539, 24958, 12, 2389, 37, 2603, 45977, 8, 201, 198, 220, 201, 198, 220, 201, 201, 198, 201, 198, 2601, 436, 1586, 978, 7727, 907, 8655, 12, 30297, 10692, 8581, 201, 2348, 42289, 201, 11041, 8913, 201, 42, 5308, 504, 201, 12218, 12, 29983, 32966, 1586, 329, 40522, 810, 23163, 389, 43180, 287, 5485, 13, 201, 39234, 33, 963, 509, 5308, 504, 201, 1890, 1588, 12, 9888, 40522, 351, 2854, 17778, 11, 257, 5443, 2196, 286, 509, 5308, 504, 13, 201, 35389, 31562, 337, 9602, 357, 38, 12038, 8, 201, 2215, 23163, 423, 15874, 10620, 290, 48804, 605, 15268, 11, 290, 345, 765, 1861, 14991, 2569, 13946, 9931, 13, 201, 49738, 1373, 1012, 436, 1586, 201, 11041, 913, 329, 1729, 12, 1102, 303, 87, 23163, 11, 304, 13, 70, 1539, 4823, 12, 3106, 32966, 1586, 393, 618, 23163, 389, 407, 3538, 2880, 540, 13, 201, 5308, 272, 33377, 201, 16742, 82, 262, 3641, 286, 23163, 11, 922, 329, 1366, 351, 281, 6439, 1271, 286, 23163, 290, 40647, 14292, 23163, 13, 201, 46384, 75, 12057, 876, 1012, 436, 1586, 201, 39, 959, 998, 605, 32966, 1586, 11, 4050, 329, 1402, 40522, 810, 6958, 1022, 1366, 2173, 389, 1593, 13, 201, 35, 4462, 44565, 201, 13014, 329, 1366, 351, 7838, 11, 6007, 286, 4917, 23163, 286, 15874, 15268, 290, 10620, 1231, 31577, 262, 1271, 286, 23163, 13, 201, 50091, 354, 201, 36, 5632, 32966, 1586, 2446, 329, 1588, 40522, 11, 2499, 880, 351, 1366, 326, 318, 18703, 453, 2087, 13, 201, 35191, 6269, 8772, 363, 341, 201, 38062, 4142, 15947, 262, 1271, 286, 23163, 1912, 319, 3275, 12, 6603, 278, 1022, 1366, 2173, 13, 201, 49738, 1373, 13302, 6048, 278, 201, 29271, 3004, 1483, 7741, 2446, 326, 460, 307, 973, 878, 32966, 1586, 16113, 329, 1365, 2854, 319, 20793, 1366, 13, 201, 53, 40469, 12038, 357, 23907, 864, 4696, 35610, 12822, 31562, 337, 9602, 9104, 8, 201, 2964, 65, 14991, 2569, 32966, 1586, 2446, 810, 262, 1271, 286, 23163, 318, 407, 5969, 26, 11080, 329, 8627, 1366, 13, 201, 201, 198, 32, 9815, 7208, 1022, 6060, 5800, 11, 6060, 14691, 11, 290, 6060, 14044, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 6601, 5800, 201, 6601, 14691, 201, 6601, 14044, 201, 36621, 201, 464, 2214, 5670, 319, 37895, 11570, 17218, 290, 16277, 422, 1588, 40522, 1262, 6190, 13905, 11, 4572, 4673, 11, 290, 9552, 7605, 13, 201, 464, 1429, 286, 49889, 11, 12724, 11, 25449, 11, 290, 21128, 1366, 284, 7073, 4465, 1321, 290, 1104, 2551, 12, 8601, 13, 201, 464, 3357, 286, 2615, 290, 10941, 262, 6884, 357, 79, 541, 20655, 11, 20083, 11, 3503, 2014, 326, 13536, 262, 4947, 11, 6143, 11, 290, 7587, 286, 1588, 40522, 329, 3781, 290, 4572, 4673, 13, 201, 9218, 20549, 7992, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 12, 6013, 4572, 4673, 4981, 290, 16113, 13, 201, 198, 12, 35006, 33344, 23696, 290, 9552, 2746, 3047, 13, 201, 198, 12, 35006, 11992, 1486, 329, 317, 14, 33, 4856, 13, 201, 198, 12, 29677, 2223, 540, 17218, 329, 1597, 393, 2267, 13, 201, 198, 12, 4440, 5344, 6373, 284, 26138, 13, 201, 12, 43301, 11, 3424, 11, 290, 38350, 40522, 13, 201, 198, 12, 28579, 35644, 3781, 286, 1459, 290, 6754, 11257, 13, 201, 198, 12, 10934, 14470, 12821, 290, 3136, 13, 201, 198, 12, 11440, 1958, 7572, 290, 6958, 287, 1366, 13, 201, 198, 12, 10478, 2551, 12, 6620, 1833, 262, 10939, 286, 1366, 13, 201, 12, 8495, 290, 1382, 1366, 31108, 13, 201, 198, 12, 1869, 496, 1366, 6143, 3341, 357, 68, 13, 70, 1539, 1366, 43675, 11, 24768, 737, 201, 198, 12, 48987, 1366, 3081, 290, 11500, 13, 201, 198, 12, 48282, 12152, 43, 357, 11627, 974, 11, 26981, 11, 8778, 8, 7767, 13, 201, 198, 12, 7929, 1366, 13440, 290, 1366, 5519, 351, 3424, 290, 24284, 40522, 13, 201, 49045, 201, 8645, 378, 16277, 290, 17218, 1262, 1366, 12, 15808, 4981, 11, 18120, 3716, 1597, 2761, 351, 6190, 23696, 11, 290, 4441, 13097, 8136, 13, 201, 22087, 5692, 393, 4837, 1833, 1459, 1366, 11257, 11, 5911, 1994, 17218, 11, 290, 5698, 5370, 351, 1598, 11, 7187, 3136, 1912, 319, 1366, 13, 201, 4834, 19532, 326, 1366, 318, 1695, 11, 9857, 11, 290, 8389, 6105, 11, 15882, 7209, 3781, 290, 1366, 3783, 7767, 13, 44290, 257, 4735, 6884, 329, 9041, 1588, 12, 9888, 40522, 13, 201, 15739, 2171, 20906, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 12, 34931, 3781, 201, 198, 12, 10850, 4673, 1222, 9552, 201, 198, 12, 30297, 357, 37906, 11, 371, 11, 16363, 8, 201, 198, 12, 6060, 1319, 27499, 201, 198, 12, 13435, 19473, 201, 198, 12, 6060, 32704, 201, 198, 12, 20414, 286, 6279, 9554, 357, 12298, 50, 11, 22134, 11, 402, 8697, 8, 201, 198, 12, 9104, 12660, 1222, 23989, 7605, 201, 12, 13535, 30063, 4678, 201, 198, 12, 6060, 12724, 1222, 13389, 201, 198, 12, 6060, 32704, 357, 68, 13, 70, 1539, 8655, 559, 11, 4333, 20068, 11, 24134, 8, 201, 198, 12, 16363, 42517, 1112, 201, 198, 12, 14392, 13905, 3725, 201, 198, 12, 26117, 290, 6447, 4678, 201, 198, 220, 201, 198, 220, 201, 12, 30297, 357, 37906, 11, 7349, 11, 38334, 11, 16363, 8, 201, 198, 12, 24047, 1486, 290, 4542, 201, 198, 12, 12152, 43, 4899, 357, 68, 13, 70, 1539, 24843, 3701, 11125, 11, 24843, 11556, 10547, 8, 201, 198, 12, 10130, 1366, 6143, 357, 68, 13, 70, 1539, 30865, 2297, 30846, 11, 3012, 4403, 20746, 8, 201, 198, 12, 4403, 1366, 4899, 357, 39, 4533, 404, 11, 17732, 8, 201, 198, 12, 6060, 11523, 22771, 290, 17771, 12401, 201, 198, 220, 201, 198, 220, 201, 17227, 20003, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 12, 449, 929, 88, 353, 5740, 2070, 201, 198, 12, 11361, 357, 79, 392, 292, 11, 31835, 20519, 11, 629, 1134, 270, 12, 35720, 11, 309, 22854, 37535, 11, 9485, 15884, 354, 8, 201, 198, 12, 371, 201, 198, 12, 16363, 201, 198, 12, 367, 4533, 404, 11, 17732, 201, 198, 12, 15612, 1634, 4899, 357, 19044, 29487, 8019, 11, 1001, 397, 1211, 11, 28114, 306, 11, 3503, 2014, 201, 198, 12, 10130, 14492, 9554, 201, 198, 12, 21722, 11, 25716, 201, 12, 24134, 201, 198, 12, 16363, 201, 198, 12, 8655, 559, 11, 4333, 20068, 201, 198, 12, 11361, 357, 79, 392, 292, 11, 6550, 29487, 8019, 8, 201, 198, 12, 3012, 30437, 201, 198, 12, 7320, 9345, 357, 3483, 8, 4899, 201, 198, 12, 35516, 201, 198, 12, 371, 201, 12, 16363, 20083, 357, 6307, 47701, 11, 33476, 11, 18650, 8, 201, 198, 12, 1400, 17861, 20083, 357, 44, 25162, 11012, 11, 46750, 8, 201, 198, 12, 12152, 43, 4899, 357, 25189, 4891, 3701, 11125, 11, 7193, 437, 8, 201, 198, 12, 24843, 367, 4533, 404, 11, 24843, 17732, 201, 198, 12, 10130, 9554, 357, 12298, 50, 11, 3012, 10130, 11, 22134, 8, 201, 198, 12, 6060, 17771, 12401, 4899, 357, 42, 18478, 3262, 274, 11, 25716, 8, 201, 198, 220, 201, 198, 220, 201, 26410, 201, 47, 17407, 425, 4981, 11, 9552, 12, 15808, 17218, 11, 10763, 11, 23392, 16113, 329, 2551, 12, 8601, 11, 22771, 3341, 11, 393, 2267, 6373, 13, 201, 37844, 11, 14470, 12821, 11, 45814, 3792, 11, 35644, 17218, 11, 290, 5874, 24612, 286, 1366, 284, 5698, 1597, 5370, 13, 201, 27871, 320, 1143, 11, 20793, 11, 290, 13686, 40522, 11, 1366, 31108, 11, 1366, 24768, 11, 290, 1366, 43675, 3492, 329, 3781, 393, 21128, 13, 201, 33380, 864, 25353, 201, 198, 220, 201, 198, 220, 201, 12, 13435, 7370, 357, 18254, 338, 393, 16394, 8, 287, 13851, 5800, 11, 14370, 11, 39448, 11, 393, 3519, 7032, 13, 201, 198, 12, 6093, 1143, 5051, 6637, 287, 1366, 3783, 290, 4572, 4673, 13, 201, 198, 12, 13535, 16200, 3725, 287, 9552, 290, 10373, 10838, 13, 201, 12, 33399, 338, 4922, 287, 7869, 11, 12446, 11, 393, 257, 1597, 12, 5363, 2214, 13, 201, 198, 12, 14965, 6637, 287, 23696, 4899, 357, 10962, 559, 11, 4333, 20068, 737, 201, 198, 12, 14392, 4547, 286, 13905, 7605, 290, 4899, 13, 201, 12, 33399, 338, 393, 5599, 338, 4922, 287, 13851, 5800, 11, 14044, 11, 393, 3519, 7032, 13, 201, 198, 12, 14965, 6637, 287, 1263, 1366, 8514, 11, 6279, 1366, 4542, 357, 12298, 50, 11, 3012, 10130, 11, 3503, 12179, 290, 12152, 43, 3341, 13, 201, 198, 12, 13535, 3788, 8705, 4469, 13, 201, 13383, 44495, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 12, 1024, 4272, 351, 555, 7249, 1522, 290, 1588, 40522, 13, 201, 198, 12, 9104, 625, 32232, 14, 4625, 32232, 13, 201, 198, 12, 4225, 3866, 889, 3716, 16113, 13, 201, 198, 12, 33311, 510, 351, 3049, 12, 1990, 10890, 9552, 290, 10373, 8514, 13, 201, 198, 12, 26117, 286, 3716, 4981, 284, 1729, 12, 47944, 26138, 13, 201, 12, 6060, 3081, 2428, 357, 259, 20751, 393, 21873, 1366, 737, 201, 198, 12, 15612, 2890, 3716, 1366, 287, 21977, 2842, 13, 201, 198, 12, 24390, 278, 10690, 287, 1366, 10794, 13, 201, 198, 12, 48221, 870, 326, 3136, 10548, 351, 1597, 15221, 13, 201, 198, 12, 37108, 10084, 1366, 4237, 13, 201, 12, 37108, 1263, 1366, 6143, 290, 45069, 379, 5046, 13, 201, 198, 12, 48221, 870, 1366, 2324, 290, 11540, 13, 201, 198, 12, 1024, 4272, 351, 1366, 3313, 418, 290, 3716, 1366, 45619, 13, 201, 198, 12, 6416, 12, 2435, 1366, 7587, 290, 13359, 1877, 24812, 287, 1588, 12, 9888, 3341, 13, 201, 198, 220, 201, 17784, 263, 10644, 201, 6601, 33374, 11, 9552, 14, 5805, 23164, 11, 4992, 33374, 11, 7320, 9345, 17340, 13, 201, 6601, 44600, 11, 7320, 44600, 11, 20068, 44600, 11, 16205, 44600, 13, 201, 6601, 23164, 11, 6060, 17340, 11, 24047, 22998, 11, 12152, 43, 23836, 11, 4403, 6060, 23164, 13, 201, 26287, 46860, 201, 3, 10232, 11, 830, 532, 720, 14198, 11, 830, 583, 614, 357, 2937, 737, 201, 3, 1899, 11, 830, 532, 720, 3829, 11, 830, 583, 614, 357, 2937, 737, 201, 3, 11442, 11, 830, 532, 720, 8628, 11, 830, 583, 614, 357, 2937, 737, 201, 35848, 563, 34479, 201, 11922, 3512, 2233, 284, 9552, 290, 22771, 11257, 13, 18948, 7194, 706, 287, 7261, 11, 9604, 11, 11409, 11, 290, 2267, 7032, 13, 201, 33004, 3512, 287, 11798, 10616, 6447, 11, 5182, 3781, 11, 1597, 17218, 11, 304, 12, 27061, 11, 290, 9604, 13, 201, 11922, 3512, 287, 11798, 326, 8814, 319, 1263, 1366, 7587, 11, 6279, 6884, 11, 290, 1366, 12, 15808, 2551, 12, 8601, 11, 1390, 7261, 11, 9604, 11, 26355, 11, 290, 6308, 13, 201, 201, 198, 201, 198, 201, 198, 32, 7208, 1022, 6060, 14691, 11, 6060, 15612, 1634, 11, 290, 6060, 9104, 278, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 6601, 14691, 201, 6601, 15612, 1634, 201, 6601, 9104, 278, 201, 36621, 201, 464, 1429, 286, 49889, 11, 12724, 11, 25449, 11, 290, 35391, 1366, 284, 7073, 7572, 11, 11257, 11, 290, 17218, 284, 1104, 2551, 12, 8601, 13, 201, 464, 27831, 10552, 286, 1366, 1262, 5874, 4847, 588, 15907, 11, 28770, 11, 290, 8739, 284, 1037, 26138, 3538, 1833, 3716, 40522, 13, 201, 464, 1429, 286, 16215, 290, 16924, 262, 4645, 286, 257, 27039, 11, 6032, 416, 4441, 6958, 1022, 1180, 1366, 12066, 284, 2380, 1597, 7767, 13, 201, 9218, 20549, 7992, 201, 12, 16213, 2736, 1366, 284, 1064, 11257, 11, 35811, 11, 290, 35907, 13, 201, 198, 12, 35006, 13905, 3781, 290, 14078, 4856, 13, 201, 198, 12, 11440, 1958, 2223, 540, 17218, 13, 201, 12, 8495, 290, 2251, 5874, 24612, 286, 1366, 13, 201, 198, 12, 26981, 8246, 1366, 656, 11570, 15907, 290, 28770, 13, 201, 198, 12, 21662, 1366, 4084, 290, 6840, 284, 26138, 13, 201, 12, 8495, 262, 4645, 286, 1366, 3341, 357, 2411, 864, 11, 38958, 11, 393, 3127, 4981, 737, 201, 198, 12, 2896, 500, 1366, 12066, 11, 12608, 11, 290, 6958, 13, 201, 198, 12, 30011, 1096, 20083, 329, 8748, 13, 201, 49045, 201, 11627, 974, 11570, 17218, 422, 8246, 1366, 284, 4175, 2551, 12, 8601, 290, 1037, 5692, 787, 1366, 12, 15808, 5370, 13, 201, 3103, 3304, 17218, 11, 11257, 11, 290, 7572, 832, 5874, 4847, 284, 15570, 1365, 4547, 290, 20061, 2551, 12, 8601, 416, 1729, 12, 47944, 26138, 13, 201, 16447, 257, 20793, 9355, 329, 703, 1366, 318, 8574, 11, 17535, 11, 290, 973, 1626, 3341, 284, 4155, 15794, 11, 9922, 11, 290, 16578, 1799, 13, 201, 15739, 2171, 20906, 201, 12, 34931, 3725, 357, 36362, 313, 8497, 4856, 11, 20683, 11, 3503, 15729, 201, 198, 12, 6060, 12724, 290, 662, 36948, 13, 201, 198, 12, 4415, 19777, 287, 42517, 1112, 357, 17861, 737, 201, 198, 12, 11361, 14, 49, 329, 3781, 13, 201, 12, 20414, 286, 1486, 7811, 357, 8043, 11, 12461, 11, 16287, 737, 201, 198, 12, 4415, 19777, 287, 32704, 4899, 13, 201, 198, 12, 28491, 286, 1180, 8262, 3858, 290, 618, 284, 779, 606, 13, 201, 12, 24047, 1486, 357, 1137, 37067, 11, 32815, 1486, 737, 201, 198, 12, 20414, 286, 16363, 11, 1400, 17861, 13, 201, 198, 12, 6060, 3487, 1634, 290, 23989, 7605, 13, 201, 198, 12, 20885, 12, 39468, 1056, 21128, 357, 1137, 10128, 737, 201, 17227, 20003, 201, 12, 24134, 201, 198, 12, 16363, 201, 198, 12, 11361, 357, 79, 392, 292, 11, 31835, 20519, 8, 201, 198, 12, 371, 201, 198, 12, 35516, 201, 198, 12, 3012, 30437, 201, 12, 8655, 559, 201, 198, 12, 4333, 20068, 201, 198, 12, 6550, 29487, 8019, 11, 1001, 397, 1211, 357, 37906, 8, 201, 198, 12, 24134, 201, 198, 12, 3012, 6060, 11733, 201, 12, 16363, 20083, 357, 6307, 47701, 11, 33476, 8, 201, 198, 12, 13793, 16362, 4899, 357, 22946, 312, 40926, 11, 3197, 13, 952, 8, 201, 198, 12, 6060, 21128, 4899, 357, 9139, 5404, 11, 19764, 14151, 38882, 6060, 17340, 8, 201, 198, 12, 1400, 17861, 20083, 357, 44, 25162, 11012, 737, 201, 26410, 201, 17126, 19929, 3136, 11, 1366, 30114, 3166, 11, 5182, 13523, 11, 14470, 12821, 11, 290, 1994, 2854, 21337, 357, 42, 47, 3792, 737, 201, 1925, 5889, 11, 28770, 11, 14470, 12821, 11, 1167, 24188, 11, 290, 14333, 27329, 326, 4084, 3359, 1994, 20731, 11, 7572, 11, 290, 11257, 13, 201, 32398, 12, 39468, 1056, 37067, 357, 1137, 35, 828, 32815, 17336, 11, 6831, 8893, 11, 290, 257, 20793, 1366, 2746, 329, 2252, 3781, 393, 1080, 8748, 13, 201, 34888, 201, 43467, 262, 366, 10919, 1, 290, 366, 22850, 1, 2157, 262, 1366, 11, 9489, 1111, 35644, 290, 1167, 33369, 3781, 13, 201, 8890, 489, 4035, 3716, 1366, 290, 17728, 340, 22632, 284, 10996, 17218, 287, 257, 517, 16274, 856, 5794, 284, 257, 3154, 5386, 13, 201, 44909, 870, 262, 366, 4919, 1, 1366, 481, 307, 8574, 11, 8389, 11, 290, 3519, 11, 4441, 257, 1598, 30881, 329, 6942, 290, 7187, 1366, 4542, 290, 8748, 13, 201, 12915, 18987, 201, 6601, 13440, 11, 2551, 12, 6620, 11, 1597, 5352, 1023, 11, 290, 4837, 508, 761, 284, 787, 7981, 5370, 1912, 319, 6754, 290, 1103, 12, 2435, 1366, 13, 201, 24749, 12353, 11, 1720, 11663, 11, 7124, 3466, 11, 290, 1729, 12, 47944, 26138, 508, 761, 284, 2952, 13180, 11257, 11, 17218, 11, 290, 1366, 12, 15808, 3923, 13, 201, 6601, 12037, 11, 6831, 18618, 11, 6505, 11, 290, 29518, 508, 3494, 290, 5529, 262, 1366, 4645, 287, 20083, 393, 1366, 43675, 13, 201, 26410, 21066, 201, 12, 2935, 6519, 425, 7869, 357, 32604, 11, 14288, 11, 4235, 737, 201, 198, 12, 2744, 49501, 3781, 13, 201, 198, 12, 22836, 3781, 625, 640, 13, 201, 198, 12, 14322, 45279, 4981, 357, 68, 13, 70, 1539, 41164, 2003, 11257, 737, 201, 12, 6910, 15907, 329, 11257, 13, 201, 198, 12, 2409, 15907, 329, 17909, 13, 201, 198, 12, 21690, 15907, 329, 23250, 13, 201, 198, 12, 12308, 31803, 329, 12109, 13, 201, 198, 12, 21365, 14470, 12821, 329, 1103, 12, 2435, 1366, 13, 201, 12, 13793, 37067, 284, 2380, 6958, 13, 201, 198, 12, 24047, 32815, 6770, 357, 83, 2977, 11, 15180, 737, 201, 198, 12, 5972, 605, 290, 3518, 1366, 4981, 13, 201, 198, 12, 6060, 3487, 1634, 3897, 5356, 13, 201, 13383, 44495, 201, 12, 1024, 4272, 351, 17503, 393, 24097, 40522, 13, 201, 198, 12, 14136, 27381, 341, 286, 13905, 2482, 13, 201, 198, 12, 48221, 870, 326, 17218, 389, 2223, 540, 290, 5981, 284, 262, 1597, 4661, 13, 201, 12, 10031, 2752, 262, 826, 2099, 286, 32704, 329, 262, 1366, 13, 201, 198, 12, 48221, 870, 326, 5874, 4582, 389, 407, 15850, 13, 201, 198, 12, 45157, 4035, 3716, 1366, 1231, 6078, 1994, 17218, 13, 201, 12, 48221, 870, 326, 262, 1366, 2746, 318, 43865, 11, 6414, 11, 290, 6971, 477, 1597, 4560, 13, 201, 198, 12, 49500, 1588, 11, 3716, 40522, 981, 45780, 2854, 13, 201, 17784, 263, 10644, 201, 6601, 44600, 11, 7320, 44600, 11, 11302, 44600, 11, 16205, 44600, 13, 201, 6601, 15612, 1634, 36816, 11, 20068, 44600, 11, 16189, 3526, 31716, 11, 38179, 14, 10080, 6060, 15612, 1634, 31716, 13, 201, 6601, 9104, 263, 11, 24047, 17340, 11, 6060, 23164, 11, 6060, 45927, 17340, 13, 201, 33380, 864, 25353, 201, 49321, 257, 33399, 338, 287, 14370, 11, 13851, 5800, 11, 18963, 11, 393, 7320, 11, 1690, 41610, 416, 1366, 3781, 5051, 6637, 13, 201, 32, 4469, 287, 6060, 30437, 11, 13851, 5800, 11, 6188, 8495, 11, 393, 43029, 8495, 11, 351, 281, 12476, 319, 6946, 290, 1366, 32704, 4899, 13, 201, 35, 1533, 6037, 287, 13851, 5800, 11, 6188, 11998, 11, 393, 24047, 8549, 11, 351, 2041, 4582, 287, 6831, 1486, 290, 1366, 10959, 13, 201, 35848, 563, 34479, 201, 33004, 3512, 1973, 11798, 326, 761, 2223, 540, 17218, 422, 1366, 284, 2987, 4560, 11, 7124, 11, 290, 9604, 13, 201, 43964, 3512, 11, 2592, 287, 5692, 326, 2421, 19933, 1366, 14470, 12821, 290, 27329, 284, 4175, 2551, 12, 8601, 287, 3006, 588, 7124, 11, 4200, 11, 290, 1720, 2478, 13, 201, 11922, 3512, 287, 11798, 351, 3716, 1366, 45619, 290, 3341, 11, 884, 355, 7261, 11, 9604, 11, 26355, 11, 290, 304, 12, 27061, 11, 810, 1366, 4542, 318, 4688, 284, 4560, 13, 201, 201, 198, 201, 198, 201, 198, 32, 34872, 2122, 282, 47986, 7311, 357, 18474, 13219, 30297, 10692, 8581, 201, 29391, 201, 11828, 201, 20560, 7412, 201, 464, 3127, 4940, 351, 281, 5128, 2939, 11, 290, 262, 3061, 318, 284, 36509, 393, 5911, 5563, 393, 7572, 1626, 262, 2939, 13, 201, 3103, 85, 2122, 201, 32, 1402, 8106, 357, 7174, 257, 9720, 8, 6100, 625, 262, 2939, 11, 2045, 379, 1402, 9004, 379, 257, 640, 13, 632, 39382, 4096, 7572, 588, 13015, 11, 20028, 11, 393, 3124, 3915, 2334, 13, 201, 38816, 9347, 201, 464, 5072, 286, 262, 3063, 2122, 4905, 318, 257, 3895, 3975, 11, 543, 11330, 1593, 3033, 422, 262, 2939, 287, 257, 27009, 11, 15246, 1927, 20793, 5794, 13, 201, 27201, 278, 201, 27201, 278, 12850, 262, 2546, 286, 262, 3895, 3975, 416, 15676, 2890, 262, 749, 1593, 1321, 11, 5742, 262, 2746, 1716, 517, 6942, 290, 1342, 8564, 284, 1402, 13991, 13, 201, 27201, 276, 27018, 9347, 201, 464, 1255, 706, 5933, 278, 318, 257, 866, 12, 37687, 10137, 2196, 286, 262, 3895, 3975, 326, 27452, 262, 6393, 9695, 286, 262, 2939, 13, 201, 3103, 85, 2122, 1222, 19850, 278, 357, 17776, 8, 201, 4711, 4831, 389, 5100, 3294, 1661, 284, 7925, 517, 3716, 290, 2440, 12, 5715, 3033, 422, 262, 2939, 13, 201, 7414, 41769, 34398, 201, 464, 44762, 3895, 8739, 389, 45096, 656, 257, 530, 12, 19577, 15879, 11, 543, 460, 307, 3804, 656, 262, 1306, 11685, 329, 2252, 7587, 393, 17923, 13, 201, 37, 2132, 8113, 276, 34398, 201, 464, 45096, 3895, 15879, 318, 5884, 284, 257, 3938, 5884, 7679, 11, 810, 477, 3033, 389, 5929, 290, 15475, 284, 787, 257, 2457, 17724, 13, 201, 26410, 201, 464, 3127, 23862, 257, 900, 286, 39522, 329, 1123, 1744, 1398, 13, 383, 1398, 351, 262, 4511, 12867, 318, 6163, 355, 262, 2746, 338, 17724, 13, 201, 201, 198, 201, 198, 1722, 806, 201, 3103, 85, 2122, 282, 406, 6962, 201, 7414, 41769, 406, 6962, 201, 37, 2132, 8113, 276, 406, 6962, 201, 36621, 201, 5990, 23914, 257, 3063, 2122, 4905, 416, 22292, 16628, 357, 74, 44930, 8, 625, 262, 5128, 284, 7925, 3033, 13, 201, 3103, 24040, 5021, 12, 19577, 26515, 357, 68, 13, 70, 1539, 362, 35, 393, 513, 35, 8, 656, 257, 352, 35, 15879, 13, 201, 32, 4569, 17019, 3127, 7679, 810, 790, 43164, 318, 5884, 284, 790, 43164, 287, 262, 2180, 7679, 13, 201, 35170, 15553, 201, 11627, 974, 82, 1957, 3033, 588, 13015, 11, 20028, 11, 290, 7572, 422, 262, 5128, 1366, 11, 23934, 21739, 6958, 13, 201, 37534, 3565, 5021, 12, 19577, 1366, 357, 68, 13, 70, 1539, 3895, 8739, 422, 3063, 2122, 282, 11685, 8, 329, 5128, 656, 3938, 5884, 11685, 13, 201, 20575, 1127, 477, 3033, 284, 787, 2457, 16277, 393, 5370, 11, 9489, 3298, 3895, 46500, 13, 201, 20560, 25959, 201, 29800, 12, 19577, 5128, 357, 68, 13, 70, 1539, 362, 35, 329, 4263, 25, 367, 12906, 12418, 3467, 22355, 7655, 12906, 54, 11, 513, 35, 351, 9619, 25, 367, 12906, 54, 12906, 3398, 3467, 22355, 370, 3467, 22355, 5870, 12906, 54, 12906, 34, 737, 201, 29800, 12, 19577, 5128, 357, 17, 35, 11, 513, 35, 11, 3503, 12179, 45096, 656, 257, 352, 35, 15879, 13, 201, 16, 35, 15879, 355, 5128, 357, 22915, 422, 262, 1610, 41769, 7679, 393, 257, 2180, 3938, 5884, 7679, 737, 201, 26410, 25959, 201, 29800, 12, 19577, 5072, 357, 445, 19513, 6001, 290, 9647, 706, 3063, 2122, 11, 475, 27452, 3294, 9619, 737, 201, 16, 35, 15879, 357, 2704, 1078, 2945, 10552, 286, 262, 5128, 737, 201, 16, 35, 15879, 286, 43164, 1753, 602, 11, 2546, 8338, 319, 262, 1271, 286, 16890, 287, 262, 7679, 13, 201, 11041, 8913, 201, 38816, 22236, 11, 2592, 287, 2939, 290, 2008, 7587, 8861, 13, 201, 39317, 1022, 3063, 2122, 282, 11685, 290, 3938, 5884, 11685, 13, 201, 19006, 17923, 11, 20683, 11, 393, 17724, 8861, 287, 2769, 4673, 4981, 13, 201, 48944, 201, 18712, 1271, 286, 10007, 357, 43775, 389, 4888, 1973, 262, 5128, 1262, 50207, 737, 201, 2949, 10007, 12, 3137, 27179, 7916, 262, 1366, 13, 201, 21968, 1271, 286, 10007, 11, 355, 1123, 43164, 20417, 284, 790, 5128, 422, 262, 2180, 7679, 357, 67, 1072, 8787, 737, 201, 4561, 34961, 6188, 201, 25460, 11184, 21739, 6958, 357, 9150, 290, 4645, 8, 287, 262, 5128, 1366, 13, 201, 15642, 1371, 21739, 1321, 290, 27179, 7916, 1366, 656, 257, 6228, 7177, 13, 201, 13921, 407, 12201, 21739, 6958, 11, 2427, 13692, 319, 3298, 3895, 10552, 13, 201, 17227, 26622, 201, 198, 220, 201, 12, 8100, 82, 329, 2939, 17923, 290, 2134, 13326, 13, 201, 198, 12, 399, 19930, 351, 2420, 24612, 357, 68, 13, 70, 1539, 352, 35, 3063, 14191, 737, 201, 8291, 653, 278, 422, 3895, 8739, 357, 6738, 8100, 82, 8, 284, 3938, 5884, 11685, 329, 2457, 17923, 13, 201, 198, 220, 201, 12, 8125, 17724, 287, 8100, 82, 13, 201, 198, 12, 16718, 287, 15993, 17019, 7686, 329, 17923, 393, 20683, 13, 201, 47445, 287, 262, 7311, 201, 47504, 82, 1877, 12, 5715, 284, 1029, 12, 5715, 3033, 28398, 1146, 416, 11524, 3294, 16628, 13, 201, 37534, 3565, 262, 21242, 3033, 329, 17923, 393, 17724, 416, 27179, 9269, 606, 13, 201, 20575, 1127, 477, 3033, 290, 17706, 262, 2457, 2551, 12, 8601, 357, 22915, 7679, 287, 17923, 14, 2301, 2234, 737, 201, 36301, 36644, 201, 5297, 11, 19590, 389, 4888, 1973, 262, 5128, 2272, 284, 4646, 262, 1271, 286, 10007, 13, 201, 2949, 7373, 2391, 27179, 7916, 1366, 13, 201, 2949, 11507, 7373, 26, 1123, 43164, 468, 663, 898, 900, 286, 19590, 11, 3756, 284, 257, 1588, 1271, 286, 10007, 13, 201, 5377, 1996, 341, 6446, 201, 37058, 2793, 3688, 284, 3938, 5884, 11685, 2233, 284, 3463, 7373, 11, 475, 460, 307, 1029, 329, 2769, 11685, 13, 201, 16371, 1877, 11, 355, 340, 691, 27179, 7916, 262, 5128, 1366, 13, 201, 11922, 31350, 1575, 2233, 284, 15715, 8787, 357, 27379, 43164, 5884, 284, 477, 17311, 737, 201, 16281, 201, 18, 87, 18, 3063, 2122, 8106, 5625, 284, 281, 2939, 284, 4886, 13015, 13, 201, 7414, 1078, 3101, 257, 3933, 12906, 2624, 12906, 18, 1661, 3895, 3975, 656, 257, 15879, 286, 2546, 1542, 4761, 13, 201, 37, 2132, 5884, 7679, 351, 8576, 16890, 6464, 5128, 422, 257, 45096, 15879, 13, 201, 220, 201, 198, 201, 198, 35, 1072, 357, 37, 2132, 8113, 276, 8, 406, 6962, 290, 34872, 2122, 282, 406, 6962, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 35, 1072, 357, 37, 2132, 8113, 276, 8, 406, 6962, 201, 3103, 85, 2122, 282, 406, 6962, 201, 1273, 5620, 201, 6109, 43164, 318, 5884, 284, 790, 584, 43164, 287, 262, 1306, 7679, 13, 201, 10049, 257, 24637, 286, 16890, 357, 33885, 393, 8106, 8, 318, 5884, 284, 262, 5128, 1366, 11, 22292, 625, 340, 15246, 1927, 13, 201, 20560, 25959, 201, 7414, 1078, 2945, 15879, 357, 68, 13, 70, 1539, 257, 352, 35, 7177, 286, 17465, 3815, 422, 281, 2939, 737, 201, 17, 35, 393, 513, 35, 1366, 357, 68, 13, 70, 1539, 4263, 351, 6001, 11, 9647, 11, 290, 9619, 737, 201, 1135, 2337, 201, 10871, 4637, 468, 663, 898, 3463, 11, 3756, 284, 257, 1588, 1271, 286, 10007, 13, 201, 2484, 1144, 19590, 1626, 257, 9720, 14, 24455, 1973, 21739, 7064, 13, 201, 15057, 286, 40117, 201, 43, 32270, 1271, 286, 10007, 355, 1123, 5128, 318, 5884, 284, 790, 5072, 13, 201, 32351, 263, 10007, 2233, 284, 3463, 7373, 357, 68, 13, 70, 1539, 257, 513, 87, 18, 8106, 329, 281, 2104, 2939, 737, 201, 35170, 15553, 201, 14961, 5907, 3298, 3033, 416, 13622, 262, 2104, 5128, 355, 257, 2187, 13, 201, 14961, 5907, 1957, 3033, 588, 13015, 11, 20028, 11, 393, 7572, 287, 1402, 7652, 13, 201, 29271, 3004, 1483, 201, 49321, 973, 329, 352, 35, 5128, 14, 22915, 357, 8499, 27172, 3101, 737, 201, 23044, 351, 362, 35, 357, 68, 13, 70, 1539, 2939, 1366, 8, 393, 513, 35, 357, 68, 13, 70, 1539, 2008, 1366, 737, 201, 25526, 341, 40480, 201, 37887, 3940, 416, 257, 1729, 12, 29127, 14916, 588, 797, 41596, 393, 264, 17225, 1868, 13, 201, 37288, 3940, 416, 281, 14916, 2163, 588, 797, 41596, 11, 290, 5933, 278, 11685, 329, 21838, 321, 11347, 13, 201, 11041, 35536, 201, 38052, 287, 2457, 17923, 393, 20683, 8861, 13, 201, 38052, 329, 3895, 22236, 11, 2592, 351, 21739, 1366, 588, 4263, 393, 2008, 13, 201, 9492, 5310, 1799, 201, 17309, 263, 284, 6179, 26, 645, 1598, 2792, 1022, 5128, 2272, 290, 4499, 3033, 13, 201, 5167, 6179, 540, 26, 16628, 460, 7766, 7572, 588, 13015, 393, 20028, 4499, 422, 262, 5128, 13, 201, 5377, 1996, 864, 45728, 201, 22058, 6942, 2233, 284, 1588, 1271, 286, 10007, 290, 3092, 286, 21739, 9359, 13, 201, 5167, 6942, 2233, 284, 1957, 8787, 290, 3463, 7373, 11, 543, 12850, 31350, 1575, 13, 201, 6601, 24422, 201, 39618, 45096, 1366, 357, 17, 35, 4263, 761, 284, 307, 27179, 5813, 284, 352, 35, 30104, 737, 201, 13470, 306, 2499, 351, 20793, 5128, 588, 4263, 11, 23934, 21739, 6958, 13, 201, 16281, 29566, 201, 37288, 973, 287, 2457, 11685, 286, 257, 2746, 284, 5072, 39522, 393, 8198, 13, 201, 38052, 287, 1903, 11685, 329, 37895, 21739, 3033, 422, 4263, 393, 10425, 13, 201, 201, 198, 3103, 85, 2122, 282, 34398, 11, 19850, 278, 34398, 11, 1610, 41769, 34398, 11, 290, 360, 1072, 34398, 532, 30297, 10692, 8581, 201, 38816, 201, 3103, 85, 2122, 282, 34398, 357, 3103, 85, 17, 35, 8, 201, 27201, 278, 34398, 357, 11518, 27201, 278, 17, 35, 8, 201, 7414, 41769, 34398, 201, 35, 1072, 34398, 357, 37, 2132, 8113, 276, 34398, 8, 201, 30026, 3455, 201, 11627, 974, 82, 3033, 422, 262, 5128, 416, 1262, 16628, 357, 74, 44930, 8, 284, 9367, 290, 4886, 7572, 588, 13015, 290, 20028, 13, 201, 7738, 26873, 262, 21739, 15225, 416, 21838, 321, 11347, 11, 10068, 31350, 9332, 290, 8868, 625, 32232, 13, 201, 3103, 24040, 257, 1963, 312, 16198, 11192, 273, 656, 257, 352, 35, 15879, 284, 8335, 340, 329, 3938, 5884, 357, 67, 1072, 8, 11685, 13, 201, 18709, 274, 21242, 3033, 284, 36509, 17311, 393, 787, 16277, 11, 810, 1123, 43164, 20417, 284, 477, 16890, 287, 262, 2180, 290, 1306, 11685, 13, 201, 20560, 14, 26410, 25959, 201, 20560, 25, 513, 35, 357, 17015, 11, 9647, 11, 9619, 8, 201, 20560, 25, 513, 35, 357, 17015, 11, 9647, 11, 9619, 8, 201, 20560, 25, 513, 35, 357, 17015, 11, 9647, 11, 9619, 8, 201, 20560, 25, 352, 35, 15879, 357, 6738, 27172, 268, 7679, 8, 201, 220, 201, 26410, 25, 39963, 513, 35, 357, 17015, 11, 9647, 11, 16628, 8, 201, 26410, 25, 7735, 5322, 513, 35, 357, 17015, 11, 9647, 11, 9619, 8, 201, 26410, 25, 352, 35, 15879, 201, 26410, 25, 352, 35, 15879, 357, 17618, 286, 16890, 287, 262, 15715, 7679, 8, 201, 9218, 16205, 201, 4677, 13508, 16628, 326, 3063, 6442, 1973, 262, 5128, 13, 36965, 35002, 11, 24511, 11, 290, 14916, 5499, 357, 68, 13, 70, 1539, 797, 41596, 737, 201, 5990, 23914, 3509, 12, 7742, 278, 393, 2811, 12, 7742, 278, 284, 35743, 1321, 1626, 7652, 357, 68, 13, 70, 1539, 362, 87, 17, 5933, 278, 737, 201, 7414, 1078, 641, 262, 5072, 286, 262, 2180, 7679, 416, 30792, 21739, 15225, 656, 257, 2060, 15879, 13, 201, 4677, 13508, 257, 26356, 2160, 286, 17311, 290, 29275, 11, 3940, 416, 281, 14916, 2163, 357, 68, 13, 70, 1539, 797, 41596, 11, 2705, 9806, 8, 284, 787, 16277, 13, 201, 48944, 201, 15057, 286, 16628, 11, 8106, 2546, 357, 33885, 828, 35002, 11, 14916, 2163, 11, 24511, 201, 2949, 4512, 540, 10007, 357, 3137, 12850, 5128, 15225, 737, 201, 2949, 4512, 540, 10007, 357, 3447, 7916, 1366, 737, 201, 15057, 286, 16890, 11, 14916, 2163, 11, 19590, 11, 290, 29275, 201, 17227, 5765, 35536, 201, 38816, 22236, 422, 4263, 11, 6597, 11, 393, 21739, 1366, 11, 8811, 973, 287, 8100, 82, 329, 8861, 588, 2939, 9465, 13, 201, 49321, 973, 706, 3063, 2122, 282, 11685, 284, 4646, 21739, 15225, 981, 23934, 1593, 3033, 13, 201, 6398, 82, 355, 257, 7696, 1022, 3063, 2122, 282, 14, 7742, 278, 11685, 290, 15715, 11685, 11, 10629, 262, 1366, 329, 2457, 17923, 393, 20683, 8861, 13, 201, 19006, 2551, 12, 8601, 7679, 7, 82, 8, 286, 257, 2746, 11, 6032, 973, 706, 27172, 3101, 287, 8100, 82, 13, 20549, 856, 329, 17923, 393, 20683, 13, 201, 201, 198, 50249, 1653, 286, 20615, 8100, 17340, 942, 290, 5334, 5765, 35536, 2142, 1881, 12, 30297, 10692, 8581, 201, 6030, 201, 11828, 201, 11041, 8913, 201, 16281, 201, 23615, 8100, 357, 44015, 1843, 8100, 8, 201, 26416, 1296, 286, 8100, 810, 11685, 389, 24167, 287, 8379, 13, 29581, 34872, 2122, 282, 11, 19850, 278, 11, 1610, 41769, 11, 290, 360, 1072, 406, 6962, 13, 201, 5159, 17923, 11, 2134, 13326, 201, 3123, 7934, 12, 20, 329, 16839, 17923, 201, 37, 2132, 34872, 2122, 282, 27862, 357, 4851, 45, 8, 201, 3041, 23625, 3938, 5884, 11685, 351, 3063, 2122, 282, 11685, 284, 5412, 15874, 5128, 10620, 13, 25235, 82, 17465, 12, 3083, 16277, 13, 201, 5159, 10618, 341, 11, 37865, 10618, 341, 201, 4851, 45, 329, 37865, 10618, 341, 201, 29744, 8100, 357, 16371, 10766, 8100, 8, 201, 7934, 5225, 351, 867, 11685, 357, 4598, 8247, 393, 5179, 8, 284, 8006, 3716, 7572, 13, 12205, 1158, 9922, 475, 4433, 517, 31350, 1176, 13, 201, 5377, 11141, 8861, 588, 2939, 9465, 11, 2134, 13326, 201, 53, 11190, 7934, 11, 1874, 7934, 357, 929, 284, 24848, 11685, 8, 201, 6690, 6657, 8100, 357, 7397, 6144, 8, 201, 20575, 1127, 8100, 82, 351, 371, 6144, 82, 329, 9041, 35582, 1366, 588, 2008, 13431, 13, 8100, 329, 3895, 22236, 11, 371, 6144, 329, 21964, 20086, 13, 201, 10798, 17923, 11, 2223, 9465, 201, 7397, 6144, 329, 2008, 3781, 201, 15205, 396, 1476, 8100, 201, 31217, 8100, 15190, 1429, 17311, 422, 1180, 4237, 11, 884, 355, 25228, 290, 6795, 4263, 287, 10730, 11, 10068, 9922, 13, 201, 15205, 320, 375, 282, 1366, 7587, 11, 19771, 17311, 422, 1180, 15736, 201, 7571, 12, 5532, 8100, 329, 2223, 9465, 201, 201, 198, 50249, 1653, 286, 20615, 8100, 17340, 942, 290, 5334, 5765, 35536, 2142, 4930, 12, 30297, 10692, 8581, 201, 6030, 201, 11828, 201, 11041, 8913, 201, 16281, 201, 4965, 312, 723, 27862, 357, 4965, 7934, 8, 201, 5842, 274, 29598, 7021, 284, 8494, 262, 49047, 31312, 1917, 287, 2769, 7686, 13, 34333, 32953, 357, 411, 312, 723, 8787, 8, 284, 2987, 3047, 9332, 13, 201, 5159, 17923, 11, 2134, 13326, 201, 4965, 7934, 12, 1120, 11, 1874, 7934, 12, 17827, 329, 1588, 12, 9888, 8861, 201, 35, 40080, 357, 2953, 7596, 8, 8100, 201, 5842, 274, 11844, 515, 3063, 14191, 284, 4292, 262, 41062, 2214, 1231, 6078, 6323, 11, 21430, 3298, 4732, 287, 4263, 13, 201, 13900, 5109, 10618, 341, 11, 4046, 3781, 201, 29744, 17822, 329, 37865, 10618, 341, 201, 47371, 12, 15001, 8100, 357, 49, 12, 18474, 8, 201, 20575, 1127, 8100, 82, 351, 3814, 6961, 5050, 329, 2134, 13326, 13, 3125, 6190, 6300, 588, 12549, 290, 38996, 371, 12, 18474, 2987, 2866, 290, 9922, 13, 201, 10267, 13326, 11, 2939, 10618, 341, 201, 49, 12, 18474, 11, 12549, 371, 12, 18474, 11, 38996, 371, 12, 18474, 201, 52, 12, 7934, 201, 27195, 12342, 12, 12501, 12342, 10959, 973, 329, 10618, 341, 13, 17453, 278, 3108, 23007, 4732, 11, 11581, 3108, 13536, 7141, 42842, 13, 201, 23286, 35914, 2939, 10618, 341, 11, 3315, 19560, 201, 52, 12, 7934, 329, 3315, 2939, 10618, 341, 201, 34, 1686, 2261, 27862, 357, 34, 1686, 7934, 8, 201, 5842, 274, 43882, 357, 24432, 286, 16890, 8, 284, 3650, 45203, 1321, 290, 6958, 1022, 3033, 11, 23934, 21739, 28398, 444, 13, 201, 5159, 17923, 11, 2134, 13326, 11, 21739, 6958, 201, 34, 1686, 2261, 7311, 329, 22650, 32997, 19561, 201, 201, 198, 10962, 326, 23007, 262, 2104, 8100, 1429, 422, 262, 2939, 5128, 284, 262, 2457, 5072, 12, 7841, 1881, 532, 30297, 10692, 8581, 201, 8600, 201, 49925, 5994, 201, 18709, 12489, 201, 26410, 201, 16, 13, 23412, 7412, 201, 20560, 34398, 201, 464, 2939, 357, 68, 13, 70, 1539, 286, 257, 2095, 588, 24205, 2963, 8, 318, 11672, 656, 262, 3127, 13, 383, 5128, 468, 257, 1728, 9647, 11, 6001, 11, 290, 6795, 357, 68, 13, 70, 1539, 3933, 87, 2624, 87, 18, 329, 257, 3124, 2939, 351, 513, 25228, 9619, 737, 201, 27369, 2939, 1366, 286, 2546, 357, 68, 13, 70, 1539, 3933, 87, 2624, 87, 18, 737, 201, 17, 13, 34872, 2122, 201, 3103, 85, 2122, 282, 34398, 201, 11928, 1010, 357, 74, 44930, 8, 10649, 625, 262, 2939, 11, 31521, 2829, 7572, 588, 13015, 290, 14371, 13, 5501, 8106, 11073, 257, 3895, 3975, 416, 11524, 3063, 2122, 625, 262, 2939, 13, 201, 38816, 8739, 286, 2546, 357, 68, 13, 70, 1539, 1542, 87, 1270, 87, 2624, 8, 810, 3933, 318, 262, 1271, 286, 16628, 290, 1542, 87, 1270, 318, 262, 5322, 21739, 15793, 357, 23301, 284, 645, 24511, 737, 201, 18, 13, 27018, 20347, 201, 26410, 286, 34872, 2122, 201, 464, 5072, 286, 262, 3063, 2122, 282, 7679, 318, 3294, 3895, 8739, 326, 2380, 1180, 7572, 12326, 422, 262, 5128, 2939, 13, 201, 38816, 8739, 4478, 13015, 11, 14371, 11, 290, 4096, 3033, 21242, 422, 262, 2939, 13, 201, 19, 13, 19850, 278, 201, 27201, 278, 34398, 201, 27201, 278, 11685, 357, 23073, 5436, 27201, 278, 8, 4646, 262, 21739, 15225, 357, 10394, 290, 6001, 8, 286, 262, 3895, 8739, 981, 5291, 262, 6795, 357, 17618, 286, 16628, 8, 16572, 13, 201, 27201, 276, 3895, 8739, 286, 4833, 2546, 357, 68, 13, 70, 1539, 1315, 87, 1314, 87, 2624, 828, 810, 262, 21739, 15225, 423, 587, 10284, 1079, 475, 262, 6795, 3793, 262, 976, 13, 201, 20, 13, 5498, 34872, 2122, 201, 3103, 85, 2122, 282, 34398, 201, 6610, 900, 286, 16628, 318, 5625, 11, 31521, 517, 3716, 3033, 416, 22292, 625, 262, 44762, 3895, 8739, 13, 2312, 16628, 4886, 7572, 286, 2440, 13357, 357, 68, 13, 70, 1539, 15268, 737, 201, 3791, 3895, 8739, 351, 517, 3716, 7572, 12326, 357, 68, 13, 70, 1539, 1511, 87, 1485, 87, 2414, 11, 810, 5598, 318, 262, 1271, 286, 16628, 737, 201, 201, 198, 10962, 326, 23007, 262, 2104, 8100, 1429, 422, 262, 2939, 5128, 284, 262, 2457, 5072, 12, 7841, 4930, 532, 30297, 10692, 8581, 201, 8600, 201, 49925, 5994, 201, 18709, 12489, 201, 26410, 201, 21, 13, 5498, 19850, 278, 201, 27201, 278, 34398, 201, 6610, 5933, 278, 7679, 12850, 262, 21739, 15225, 2252, 11, 7106, 4035, 262, 1366, 981, 26645, 6393, 3033, 13, 201, 18712, 263, 44762, 3895, 8739, 357, 68, 13, 70, 1539, 718, 87, 21, 87, 2414, 737, 201, 22, 13, 10467, 34872, 2122, 201, 3103, 85, 2122, 282, 34398, 201, 32, 9211, 3063, 2122, 7679, 351, 517, 16628, 318, 5625, 11, 31521, 772, 517, 12531, 7572, 11, 39634, 262, 4499, 3033, 13, 201, 5167, 3716, 3895, 8739, 357, 68, 13, 70, 1539, 604, 87, 19, 87, 2414, 8, 10200, 517, 6496, 7572, 13, 201, 23, 13, 1610, 1078, 3101, 201, 7414, 41769, 34398, 201, 464, 513, 35, 3895, 8739, 389, 45096, 656, 257, 352, 35, 15879, 523, 326, 484, 460, 307, 973, 416, 262, 3938, 5884, 357, 35, 1072, 8, 11685, 329, 17923, 13, 201, 32, 45096, 15879, 357, 68, 13, 70, 1539, 2546, 28119, 8, 10200, 477, 4499, 3033, 422, 262, 2180, 11685, 13, 201, 24, 13, 40234, 8113, 276, 34398, 201, 35, 1072, 34398, 201, 464, 45096, 3033, 389, 3804, 832, 257, 3938, 5884, 357, 67, 1072, 8, 7679, 810, 1123, 10139, 318, 5884, 284, 790, 3895, 13, 2312, 8787, 423, 19590, 290, 29275, 13, 201, 32, 15879, 286, 1753, 602, 357, 68, 13, 70, 1539, 2546, 5598, 828, 810, 1123, 10139, 338, 14916, 8338, 319, 262, 4499, 19590, 290, 29275, 13, 201, 940, 13, 25235, 34398, 357, 9487, 2649, 8, 201, 35, 1072, 34398, 201, 464, 2457, 15715, 7679, 11073, 257, 1861, 14991, 2569, 5072, 329, 1123, 1398, 1262, 281, 14916, 2163, 588, 2705, 9806, 13, 5501, 5072, 6870, 262, 12867, 286, 262, 2939, 16686, 284, 257, 2176, 1398, 13, 201, 32, 1861, 14991, 2569, 6082, 329, 17923, 357, 68, 13, 70, 1539, 657, 13, 17, 329, 3759, 11, 657, 13, 16, 329, 1514, 1659, 88, 11, 657, 13, 22, 329, 24205, 2963, 11, 12739, 24205, 2963, 355, 262, 4511, 12867, 737, 201, 1157, 13, 8125, 46690, 201, 26410, 201, 464, 4511, 12867, 318, 6163, 355, 262, 11001, 1398, 357, 68, 13, 70, 1539, 24205, 2963, 828, 1912, 319, 262, 1861, 14991, 2569, 6082, 4635, 287, 262, 2457, 7679, 13, 201, 39156, 5722, 1398, 329, 262, 5128, 2939, 357, 68, 13, 70, 1539, 24205, 2963, 737, 201, 201, 198, 201, 198, 201, 198, 464, 41062, 2214, 22269, 9720, 357, 273, 8106, 8, 532, 30297, 10692, 8581, 201, 3103, 984, 201, 3041, 25867, 7663, 201, 42, 7948, 14, 22417, 201, 36621, 201, 464, 6903, 286, 262, 5128, 2939, 326, 257, 2176, 43164, 287, 257, 7679, 366, 325, 274, 1, 393, 318, 12824, 416, 13, 632, 13676, 355, 345, 1445, 9211, 656, 262, 3127, 13, 201, 32, 1402, 17593, 357, 23073, 513, 87, 18, 393, 642, 87, 20, 8, 973, 284, 10649, 625, 262, 5128, 2939, 290, 24061, 3033, 588, 13015, 11, 20028, 11, 393, 7572, 13, 201, 43642, 201, 8134, 364, 284, 262, 1989, 286, 262, 5128, 2939, 14329, 284, 257, 43164, 338, 14916, 11, 290, 340, 27513, 355, 262, 8100, 11685, 8931, 13, 201, 32, 900, 286, 19590, 973, 287, 3063, 2122, 284, 7925, 2176, 3033, 422, 262, 5128, 13, 201, 10699, 201, 38, 8516, 355, 11685, 389, 2087, 13, 1114, 1672, 11, 706, 1811, 11685, 286, 3063, 14191, 11, 257, 43164, 743, 366, 3826, 1, 257, 4025, 636, 286, 262, 2656, 2939, 13, 201, 13715, 2546, 357, 68, 13, 70, 1539, 513, 87, 18, 393, 642, 87, 20, 737, 360, 13221, 274, 703, 867, 17848, 287, 262, 5128, 389, 13686, 379, 1123, 3063, 2122, 2239, 13, 201, 47445, 201, 12621, 862, 8006, 6481, 4025, 7572, 393, 8573, 287, 262, 5128, 355, 345, 467, 9211, 656, 262, 3127, 13, 201, 11627, 974, 82, 1957, 7572, 357, 2339, 13015, 11, 20028, 8, 422, 262, 5128, 416, 14492, 26356, 21784, 287, 1402, 7652, 13, 201, 9492, 2673, 201, 35, 23444, 416, 703, 867, 11685, 262, 2939, 468, 3804, 832, 13, 5501, 3063, 2122, 7679, 351, 16628, 6673, 284, 262, 2546, 286, 262, 41062, 2214, 13, 201, 4677, 13508, 15726, 1973, 262, 5128, 284, 24061, 3895, 8739, 13, 383, 1271, 286, 16628, 15947, 262, 6795, 286, 262, 3895, 8739, 13, 201, 9771, 14902, 201, 35, 8682, 319, 262, 9720, 2546, 11, 33769, 11, 24511, 11, 290, 1271, 286, 11685, 287, 262, 3127, 13, 317, 43164, 287, 9211, 11685, 468, 257, 4025, 41062, 2214, 13, 201, 464, 9720, 19392, 625, 262, 5128, 2939, 11, 11524, 19590, 379, 1123, 2292, 284, 24061, 3895, 8739, 379, 326, 7679, 13, 201, 201, 198, 201, 198, 32, 7208, 14307, 1610, 1078, 3101, 6550, 45977, 290, 3602, 15464, 606, 656, 16038, 41265, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 7414, 1078, 3101, 6550, 45977, 201, 8291, 15464, 6550, 45977, 656, 16038, 41265, 201, 36621, 201, 7414, 1078, 3101, 10229, 284, 23202, 257, 17593, 357, 273, 2440, 12, 19577, 11192, 273, 8, 656, 257, 2060, 15879, 416, 37825, 4924, 663, 4847, 656, 530, 12948, 8379, 13, 201, 8291, 15464, 656, 2440, 15225, 1724, 23202, 1366, 656, 257, 517, 3716, 2272, 351, 3224, 3033, 393, 15225, 11, 1690, 832, 7605, 588, 9720, 5499, 393, 3895, 22236, 13, 201, 30026, 3455, 201, 8890, 489, 6945, 1366, 329, 16113, 326, 2421, 257, 352, 35, 5128, 357, 68, 13, 70, 1539, 13017, 2939, 1366, 656, 257, 17019, 3127, 737, 201, 34934, 16113, 357, 2339, 311, 15996, 8, 284, 4553, 1366, 517, 6840, 416, 16855, 340, 284, 257, 2440, 12, 19577, 2272, 13, 201, 23004, 278, 25959, 201, 3103, 24040, 257, 17593, 286, 5485, 357, 76, 837, 299, 8, 656, 257, 15879, 286, 5485, 357, 76, 837, 299, 737, 201, 3103, 24040, 281, 5128, 286, 299, 20471, 12, 27740, 5736, 656, 281, 5128, 286, 517, 621, 299, 20471, 12, 27740, 5736, 357, 68, 13, 70, 1539, 16855, 362, 35, 1366, 656, 513, 35, 393, 2440, 737, 201, 11041, 8913, 201, 12, 7412, 7587, 25, 27172, 3101, 362, 35, 4263, 329, 5128, 656, 257, 3938, 5884, 17019, 3127, 13, 201, 198, 12, 45157, 4035, 5128, 329, 4981, 326, 1607, 30104, 13, 201, 12, 16718, 287, 1729, 12, 29127, 4572, 4673, 4981, 357, 68, 13, 70, 1539, 311, 15996, 351, 9720, 6908, 8, 284, 787, 1366, 9493, 11458, 2880, 540, 13, 201, 198, 12, 27018, 8705, 393, 15793, 1483, 2620, 287, 2769, 4673, 13, 201, 26950, 529, 319, 6060, 201, 464, 4645, 286, 262, 17593, 318, 2626, 1201, 340, 318, 783, 257, 352, 35, 15879, 11, 475, 477, 262, 3815, 389, 17232, 13, 201, 464, 1366, 318, 35601, 416, 4375, 649, 15225, 11, 5086, 329, 517, 3716, 6958, 284, 307, 4499, 13, 201, 17227, 49686, 201, 4965, 71, 1758, 2163, 287, 12782, 588, 31835, 20519, 393, 309, 22854, 37535, 357, 68, 13, 70, 1539, 27179, 1758, 19510, 12, 16, 11, 4008, 737, 201, 42, 7948, 38226, 357, 68, 13, 70, 1539, 745, 6213, 49070, 9720, 11, 17986, 37, 9720, 828, 3895, 7118, 11, 17019, 3127, 11685, 357, 20521, 12083, 11685, 737, 201, 5377, 1996, 864, 19157, 414, 201, 37058, 15836, 290, 2653, 15208, 7026, 357, 3137, 257, 37825, 648, 972, 286, 4847, 737, 201, 6747, 2620, 262, 31350, 1575, 5566, 2233, 284, 262, 2087, 13357, 286, 517, 15225, 357, 46503, 12, 19577, 4560, 737, 201, 43, 793, 286, 6188, 201, 2949, 2994, 286, 1321, 8833, 26, 477, 4847, 389, 17383, 287, 262, 15879, 13, 2102, 11, 262, 21739, 2776, 286, 262, 17593, 4847, 743, 307, 2626, 13, 201, 2949, 2994, 286, 1321, 8833, 13, 5455, 11, 340, 1690, 6673, 517, 1321, 416, 11525, 12083, 1366, 656, 257, 2440, 12, 19577, 2272, 13, 201, 27730, 201, 12, 1610, 1078, 3101, 257, 2579, 12906, 2078, 2939, 656, 257, 767, 5705, 12, 13664, 15879, 13, 201, 198, 12, 1610, 1078, 3101, 11192, 669, 329, 3938, 5884, 11685, 287, 17019, 7686, 13, 201, 12, 337, 5912, 362, 35, 5128, 1366, 656, 257, 513, 35, 2272, 1262, 745, 6213, 49070, 393, 17986, 37, 50207, 287, 311, 15996, 13, 201, 198, 12, 34872, 2122, 11685, 287, 8100, 82, 460, 3975, 362, 35, 1366, 656, 2440, 12, 19577, 3895, 9029, 13, 201, 201, 198, 201, 198, 2437, 311, 15996, 43707, 262, 26789, 1022, 30104, 1262, 1180, 10581, 11, 1390, 14174, 290, 1729, 12, 29127, 50207, 12, 30297, 10692, 8581, 201, 6030, 286, 32169, 201, 19044, 10024, 605, 19639, 201, 3109, 11578, 341, 201, 14993, 451, 32169, 201, 198, 220, 201, 198, 220, 201, 74, 16193, 55, 41052, 72, 220, 837, 220, 1267, 220, 1395, 41052, 73, 220, 1267, 28, 1395, 41052, 72, 220, 1267, 9, 220, 220, 1395, 62, 73, 8, 201, 12, 383, 16605, 1720, 286, 734, 30104, 13, 201, 198, 12, 45040, 262, 26789, 1912, 319, 262, 9848, 1022, 262, 30104, 13, 201, 198, 12, 3334, 26789, 611, 30104, 966, 287, 262, 976, 4571, 26, 1877, 393, 4633, 611, 29617, 519, 20996, 393, 6697, 13, 201, 34220, 26601, 498, 32169, 201, 198, 220, 201, 198, 220, 201, 42, 7, 29992, 11, 87, 73, 35793, 29992, 9129, 87, 73, 10, 66, 8, 67, 42, 38016, 11018, 19881, 90, 87, 92, 62, 72, 11, 3467, 11018, 19881, 90, 87, 92, 62, 73, 8, 796, 357, 59, 11018, 19881, 90, 87, 92, 62, 72, 3467, 10210, 313, 3467, 11018, 19881, 90, 87, 92, 62, 73, 1343, 269, 8, 61, 67, 42, 7, 29992, 21747, 87, 73, 10091, 16193, 29992, 30, 9129, 87, 73, 30, 10, 66, 8, 67, 201, 198, 220, 201, 198, 220, 201, 12, 7567, 2696, 262, 16605, 1720, 284, 257, 1176, 288, 1860, 290, 6673, 257, 6937, 269, 535, 13, 201, 198, 12, 40402, 311, 15996, 284, 4197, 517, 3716, 2551, 13215, 13, 201, 198, 12, 16038, 4922, 288, 1860, 5732, 13688, 286, 262, 2746, 13, 201, 15546, 498, 6455, 271, 15553, 357, 27912, 37, 8, 32169, 1220, 12822, 31562, 32169, 201, 198, 220, 201, 198, 220, 201, 42, 7, 29992, 11, 87, 73, 47505, 11201, 30, 32590, 30, 29992, 12, 87, 73, 30, 1828, 82, 17, 8, 42, 38016, 11018, 19881, 90, 87, 92, 62, 72, 11, 3467, 11018, 19881, 90, 87, 92, 62, 73, 8, 796, 3467, 11201, 59, 9464, 32590, 59, 31944, 31478, 91, 59, 11018, 19881, 90, 87, 92, 62, 72, 532, 3467, 11018, 19881, 90, 87, 92, 62, 73, 59, 91, 61, 17, 18477, 17, 59, 82, 13495, 61, 17, 32239, 3506, 8, 42, 7, 29992, 21747, 87, 73, 10091, 28, 11201, 32590, 17, 82, 17, 30, 29992, 30, 12, 87, 73, 3548, 17, 10091, 201, 198, 220, 201, 198, 220, 201, 12, 45040, 26789, 1912, 319, 262, 48862, 485, 272, 5253, 1022, 30104, 13, 201, 198, 12, 3334, 26789, 329, 30104, 1969, 287, 2272, 26, 1877, 26789, 329, 12899, 30104, 13, 201, 198, 12, 264, 59, 82, 328, 5356, 6973, 262, 4104, 286, 262, 12822, 31562, 12133, 13, 201, 50, 17225, 1868, 32169, 201, 198, 220, 201, 198, 220, 201, 42, 7, 29992, 11, 87, 73, 47505, 38006, 71, 30, 7, 64, 7, 29992, 9129, 87, 73, 47762, 66, 8, 42, 38016, 11018, 19881, 90, 87, 92, 62, 72, 11, 3467, 11018, 19881, 90, 87, 92, 62, 73, 8, 796, 3467, 38006, 71, 38016, 26591, 357, 59, 11018, 19881, 90, 87, 92, 62, 72, 3467, 10210, 313, 3467, 11018, 19881, 90, 87, 92, 62, 73, 8, 1343, 269, 8, 42, 7, 29992, 21747, 87, 73, 10091, 28, 38006, 71, 7, 64, 7, 29992, 30, 9129, 87, 73, 10091, 10, 66, 8, 201, 198, 220, 201, 198, 220, 201, 12, 11014, 284, 262, 14916, 2163, 287, 17019, 7686, 13, 201, 198, 12, 36965, 262, 25706, 71, 2163, 284, 24061, 26789, 13, 201, 198, 12, 40117, 257, 59, 17307, 7252, 290, 269, 535, 1630, 262, 5485, 286, 262, 26789, 2163, 13, 201, 201, 198, 201, 198, 201, 198, 32, 9815, 1351, 286, 3895, 3858, 973, 287, 9552, 290, 4572, 4673, 532, 2142, 1881, 532, 30297, 10692, 8581, 201, 38816, 5994, 201, 36621, 201, 16281, 201, 23416, 201, 4561, 34961, 17571, 201, 19209, 942, 45203, 393, 1179, 864, 1366, 201, 14749, 286, 13015, 287, 4263, 201, 5159, 17923, 11, 2134, 13326, 201, 22289, 17571, 201, 13065, 3876, 4340, 4045, 4645, 286, 1366, 201, 26287, 17465, 12245, 201, 36542, 9465, 11, 15598, 3781, 201, 14565, 17571, 201, 24564, 22090, 9695, 286, 4833, 7652, 201, 40809, 8529, 10200, 257, 5228, 201, 32388, 9465, 11, 11743, 3781, 201, 12966, 35738, 17571, 201, 19209, 942, 640, 12, 3106, 2458, 201, 26207, 4536, 625, 640, 201, 10798, 3781, 11, 4046, 9465, 201, 37, 28707, 17571, 201, 15001, 319, 8373, 7386, 201, 37, 280, 5277, 44036, 201, 21206, 7587, 11, 12694, 1366, 201, 21947, 723, 17571, 201, 19209, 942, 7346, 2858, 393, 4732, 201, 26449, 3616, 422, 7346, 2456, 201, 45, 19930, 11, 15602, 3341, 201, 44909, 1523, 17571, 201, 24564, 22090, 10238, 4645, 393, 6958, 201, 13313, 507, 287, 1919, 3127, 4823, 201, 37065, 3781, 11, 5931, 21128, 201, 13900, 5109, 17571, 201, 9914, 1678, 23355, 3616, 422, 1366, 201, 26449, 11525, 67, 654, 588, 347, 17395, 201, 45, 19930, 11, 4572, 11059, 201, 17126, 19929, 17571, 201, 28532, 1572, 422, 13905, 6608, 201, 5308, 272, 11, 24198, 201, 2025, 24335, 13326, 11, 3895, 8705, 201, 39, 959, 998, 605, 17571, 201, 19209, 942, 7572, 379, 1180, 34651, 2974, 201, 7407, 3212, 287, 2793, 8100, 11685, 11, 5563, 287, 2440, 11685, 201, 29744, 4673, 11, 2134, 13326, 201, 201, 198, 201, 198, 32, 9815, 1351, 286, 3895, 3858, 973, 287, 9552, 290, 4572, 4673, 532, 2142, 4930, 532, 30297, 10692, 8581, 201, 38816, 5994, 201, 36621, 201, 16281, 201, 23416, 201, 32742, 17571, 201, 24564, 22090, 4417, 6608, 393, 7572, 201, 13587, 282, 624, 11743, 3033, 201, 37158, 19560, 11, 2587, 17923, 201, 10258, 17571, 201, 24564, 22090, 3124, 6608, 201, 36982, 3815, 11, 3124, 1554, 26836, 201, 5159, 45069, 11, 2134, 13326, 201, 33383, 17571, 201, 19209, 942, 38445, 6608, 201, 4264, 454, 12145, 669, 11, 367, 7730, 201, 10267, 13326, 11, 44396, 9465, 201, 28532, 1572, 17571, 201, 13798, 1068, 422, 38226, 201, 34220, 26601, 498, 3033, 201, 38816, 8705, 11, 2746, 23989, 201, 24220, 298, 17571, 201, 41691, 3033, 4499, 416, 4981, 201, 24220, 298, 5087, 287, 17593, 5766, 1634, 201, 29744, 4673, 11, 15602, 3341, 201, 34, 2397, 12409, 17571, 201, 6207, 6629, 28810, 9376, 201, 41394, 11, 1720, 6536, 201, 9487, 2649, 11, 15602, 3341, 201, 45, 6975, 605, 17571, 201, 6207, 6629, 26610, 3815, 201, 23396, 11, 3739, 201, 8081, 2234, 11, 33344, 21128, 201, 33, 3219, 17571, 201, 19242, 691, 734, 1744, 3815, 201, 5297, 14, 2949, 11, 6407, 14, 25101, 201, 9487, 2649, 11, 32172, 13326, 201, 35422, 1292, 17571, 201, 35422, 1068, 475, 1231, 5969, 20016, 201, 41183, 1241, 201, 9487, 2649, 11, 12759, 3341, 201, 50, 29572, 17571, 201, 4264, 1299, 867, 1976, 27498, 393, 4814, 3815, 201, 3198, 12, 8940, 30240, 30104, 201, 8206, 17923, 11, 399, 19930, 201, 7575, 12, 27996, 17571, 201, 15732, 276, 416, 640, 11, 23007, 35582, 20086, 201, 16541, 420, 273, 49501, 287, 4283, 4536, 201, 43621, 41164, 11, 33344, 9262, 201, 10606, 49501, 17571, 201, 24915, 6945, 2776, 1022, 9633, 201, 46262, 1559, 16096, 35381, 201, 38816, 6356, 11, 47368, 692, 259, 451, 414, 10627, 201, 9492, 2673, 17571, 201, 41972, 416, 19771, 2656, 3033, 201, 33, 8895, 422, 6001, 290, 3463, 201, 38816, 8705, 11, 1729, 12, 29127, 4981, 201, 29271, 3004, 1483, 12, 7738, 19513, 17571, 201, 7738, 19513, 15793, 1483, 981, 26645, 7508, 201, 5662, 32, 6805, 11, 256, 12, 50, 12161, 201, 11922, 12, 19577, 1366, 3781, 201, 49738, 1373, 17571, 201, 28532, 1572, 422, 37410, 10552, 201, 13434, 37410, 12109, 11, 32850, 4093, 201, 21206, 7587, 11, 4046, 9465, 201, 201, 198, 201, 198, 201, 198, 32, 15264, 287, 3084, 5794, 35328, 262, 6954, 286, 262, 3381, 366, 6601, 5800, 1, 262, 2723, 1058, 22720, 12, 30297, 10692, 8581, 201, 17688, 201, 9237, 12489, 201, 1129, 5237, 201, 7554, 370, 13, 16749, 2539, 6797, 366, 464, 10898, 286, 6060, 14691, 553, 36360, 1366, 3781, 355, 281, 21594, 3783, 2138, 621, 6974, 19473, 13, 16749, 2539, 635, 33115, 262, 3381, 366, 2545, 1, 287, 21709, 13, 201, 40828, 201, 19727, 399, 2899, 34134, 366, 3103, 37561, 13084, 286, 13851, 25458, 553, 16118, 262, 3381, 366, 7890, 3783, 1, 290, 16215, 340, 355, 262, 3783, 286, 7219, 351, 4920, 1366, 13, 201, 37781, 201, 464, 4037, 5396, 329, 34931, 38589, 357, 40, 42643, 8, 318, 4920, 11, 17272, 284, 2792, 13905, 20411, 351, 3660, 3644, 3037, 13, 201, 25475, 201, 25025, 652, 350, 5375, 1039, 2584, 12, 2484, 499, 7058, 1618, 4340, 262, 717, 20414, 23455, 287, 16092, 18826, 357, 42, 16458, 8, 20243, 11, 543, 43576, 656, 262, 5079, 7125, 44, 33993, 42, 16458, 8785, 319, 20414, 23455, 290, 6060, 29269, 13, 201, 22666, 201, 24749, 20916, 34134, 257, 3002, 1621, 319, 366, 38105, 22137, 553, 11142, 2706, 6, 779, 286, 1366, 284, 4331, 7172, 4069, 13, 201, 22288, 201, 464, 3381, 366, 7890, 3783, 1, 318, 3017, 287, 262, 3670, 286, 262, 4037, 11937, 286, 40984, 3345, 9545, 4495, 329, 262, 717, 640, 13, 201, 22288, 201, 5842, 1689, 35136, 88, 324, 11, 20653, 350, 5375, 1039, 2584, 12, 2484, 499, 7058, 11, 290, 15744, 45056, 291, 45083, 400, 7715, 366, 4863, 6060, 29269, 284, 20414, 23455, 287, 16092, 18826, 553, 16215, 1366, 9691, 355, 257, 2239, 287, 262, 20414, 23455, 1429, 13, 201, 21498, 201, 34, 13, 376, 13, 5502, 18027, 3848, 329, 262, 8851, 3723, 286, 7869, 355, 1366, 3783, 287, 465, 26851, 19143, 379, 262, 2059, 286, 7055, 13, 201, 21498, 201, 464, 3989, 366, 6601, 29269, 290, 20414, 23455, 1, 318, 5611, 11, 20252, 262, 7396, 11533, 286, 1366, 9691, 13, 201, 18946, 201, 46751, 49063, 15820, 3651, 319, 262, 6459, 286, 1366, 9691, 287, 366, 44, 3191, 6060, 329, 33978, 286, 20414, 526, 201, 14585, 201, 17121, 311, 13, 10306, 34134, 366, 6601, 5800, 25, 1052, 7561, 5224, 553, 24634, 262, 9323, 286, 1366, 3783, 355, 257, 649, 12883, 13, 201, 14585, 201, 3123, 78, 3719, 24086, 19451, 734, 13817, 286, 13905, 21128, 11, 24950, 329, 8385, 9383, 21128, 284, 8494, 2761, 351, 1366, 13, 201, 16942, 201, 38296, 286, 262, 366, 6601, 5800, 4913, 553, 10759, 319, 1366, 4542, 287, 3783, 290, 3037, 13, 201, 16088, 201, 38296, 286, 262, 366, 25296, 286, 6060, 5800, 553, 8998, 379, 17728, 5479, 286, 13905, 5050, 13, 201, 14315, 201, 22405, 367, 13, 360, 4005, 634, 290, 1854, 6901, 262, 22106, 286, 5449, 1912, 319, 23696, 287, 366, 7293, 13629, 319, 30437, 526, 201, 14315, 201, 464, 2351, 5800, 5926, 3848, 329, 262, 2478, 286, 257, 3451, 3108, 329, 1366, 5519, 287, 511, 989, 319, 4875, 1366, 17268, 13, 201, 12726, 201, 464, 4992, 3337, 329, 6060, 1435, 290, 6060, 5800, 318, 4920, 379, 376, 463, 272, 2059, 11, 21865, 13, 201, 11528, 201, 41, 37719, 34134, 257, 2050, 16215, 262, 2597, 290, 3451, 2478, 286, 1366, 5519, 290, 1090, 2024, 13, 201, 10531, 201, 464, 989, 366, 13587, 1108, 278, 262, 4333, 286, 10231, 6060, 329, 5800, 290, 7023, 1, 31648, 262, 761, 329, 1366, 3783, 6154, 13, 201, 10531, 201, 40202, 569, 3699, 8477, 24696, 1547, 355, 262, 366, 325, 5431, 1693, 1, 286, 262, 1306, 5707, 2233, 284, 511, 2694, 284, 1833, 290, 7925, 1988, 422, 1366, 13, 201, 10531, 201, 42, 14232, 360, 13, 347, 8553, 290, 1854, 12811, 329, 3047, 287, 1366, 3783, 329, 1111, 22447, 290, 1729, 12, 20887, 1023, 13, 201, 10531, 201, 16073, 1583, 2304, 692, 19451, 262, 4678, 2622, 329, 1366, 11153, 287, 366, 464, 7683, 49131, 20389, 286, 6060, 402, 32201, 526, 201, 10333, 201, 39324, 2788, 327, 2724, 959, 6797, 546, 262, 22106, 286, 1366, 5519, 508, 12082, 4678, 287, 8300, 11, 7869, 11, 290, 23689, 13, 201, 10333, 201, 16073, 4768, 74, 1460, 8477, 1366, 5519, 355, 20108, 987, 40625, 11, 6007, 286, 26348, 2972, 7612, 286, 1366, 2761, 13, 201, 10333, 201, 39, 346, 560, 14737, 290, 5180, 44633, 18077, 257, 1687, 30565, 286, 1366, 3783, 4568, 13, 201, 10333, 201, 35, 1809, 26608, 20718, 262, 366, 6601, 5800, 569, 1697, 6031, 6713, 553, 44000, 262, 5032, 900, 2672, 329, 1366, 5519, 13, 201, 9804, 201, 47, 14471, 38498, 11330, 262, 33985, 287, 16215, 1366, 3783, 11, 9524, 340, 9018, 987, 40625, 670, 1973, 2972, 1366, 12, 5363, 7032, 13, 201, 9804, 201, 11006, 4176, 19451, 262, 7396, 11533, 290, 3586, 286, 262, 2846, 366, 6601, 5800, 1, 290, 366, 6601, 33374, 526, 201, 6999, 201, 13787, 360, 4005, 634, 290, 360, 13, 41, 13, 3208, 346, 7715, 366, 6601, 33374, 25, 383, 14419, 6386, 15768, 286, 262, 2310, 301, 13641, 1, 287, 262, 11131, 7320, 6602, 11, 21292, 262, 3957, 3512, 290, 6817, 286, 1366, 5519, 287, 262, 3660, 16433, 13, 201, 201, 198, 201, 198, 201, 198, 32, 3084, 15676, 2890, 262, 1994, 41926, 287, 262, 2106, 286, 1263, 1366, 12, 262, 2723, 25, 22720, 532, 30297, 10692, 8581, 201, 17688, 201, 24857, 13631, 201, 1129, 3901, 201, 464, 3381, 366, 17018, 11278, 1, 318, 717, 973, 13, 201, 1129, 2598, 201, 37, 2787, 756, 16414, 7746, 471, 13, 50, 13, 6403, 12782, 481, 4274, 287, 2546, 790, 1467, 812, 11, 25539, 5909, 2003, 6143, 2476, 13, 201, 1129, 5333, 201, 35, 18238, 7886, 34134, 366, 26959, 4619, 28028, 553, 10820, 39682, 3349, 287, 5654, 22790, 290, 3725, 13, 201, 42830, 201, 7676, 1313, 290, 390, 15080, 2112, 262, 761, 329, 11353, 1366, 19794, 2233, 284, 262, 1321, 11278, 13, 201, 41208, 201, 29874, 7920, 3651, 319, 15964, 3925, 416, 511, 1366, 6143, 5339, 13, 201, 38449, 201, 16504, 6140, 262, 6188, 27782, 20962, 11, 9646, 1321, 6115, 1973, 2056, 13, 201, 23664, 201, 40, 13, 32, 13, 309, 73, 3150, 1044, 19451, 703, 1366, 27513, 284, 6070, 1695, 6143, 13, 201, 35411, 201, 39505, 560, 4940, 257, 2267, 1628, 319, 1321, 11798, 11, 15964, 1321, 6115, 13, 201, 29279, 201, 40, 400, 8207, 390, 4294, 64, 19850, 34134, 6373, 319, 262, 3349, 286, 6946, 2056, 290, 1321, 5202, 13, 201, 28054, 201, 40202, 347, 13, 40765, 7746, 2383, 5732, 287, 1366, 8296, 12109, 13, 201, 19891, 201, 19727, 449, 13, 5601, 768, 19451, 262, 6459, 15459, 416, 262, 9721, 6115, 286, 1321, 13, 201, 22288, 201, 27640, 6143, 4329, 517, 1575, 12, 16803, 621, 3348, 13, 201, 21498, 201, 13256, 18014, 290, 3271, 7122, 30567, 10400, 262, 3381, 366, 14261, 1366, 1, 287, 257, 9207, 319, 503, 12, 1659, 12, 7295, 32704, 13, 201, 21113, 201, 7554, 371, 13, 11066, 20342, 10969, 319, 366, 12804, 6060, 1, 379, 257, 1294, 1677, 10426, 3249, 13, 201, 11024, 201, 19727, 9334, 805, 290, 11023, 371, 13, 569, 3699, 7715, 257, 2050, 5554, 4035, 649, 1321, 6282, 11, 39539, 352, 13, 20, 409, 38346, 4635, 287, 7358, 13, 201, 14585, 201, 42297, 15016, 88, 34134, 319, 513, 35, 1366, 4542, 11, 15010, 6115, 11, 15432, 11, 290, 4996, 355, 15225, 286, 1263, 1366, 13, 201, 14315, 201, 14967, 440, 6, 25819, 19451, 262, 12085, 286, 1366, 287, 5313, 362, 13, 15, 13, 201, 12726, 201, 2389, 34, 26119, 326, 4875, 1366, 481, 1663, 5566, 11, 39539, 860, 3459, 409, 38346, 416, 3050, 13, 201, 11528, 201, 34, 4861, 26334, 6101, 4979, 481, 4274, 790, 734, 812, 11, 8978, 2063, 257, 1976, 3087, 37828, 287, 2321, 13, 201, 10531, 201, 32, 989, 7228, 3399, 13529, 1321, 329, 352, 13, 18, 12989, 2250, 287, 3648, 11, 37066, 513, 13, 21, 1976, 3087, 38346, 13, 201, 9804, 201, 24778, 47718, 290, 1736, 2304, 10102, 22593, 8636, 257, 6482, 422, 15075, 284, 4875, 6143, 11, 351, 10048, 4, 286, 6143, 5339, 852, 4875, 416, 4343, 13, 201, 9804, 201, 9742, 49681, 4397, 8060, 5136, 3136, 319, 262, 5801, 3349, 286, 8574, 1366, 1973, 16020, 11, 39539, 767, 13, 19, 409, 38346, 286, 649, 1366, 8574, 416, 23941, 287, 3050, 13, 201, 6999, 201, 24274, 4913, 286, 14620, 34134, 257, 2041, 2665, 319, 1321, 5339, 13, 201, 201, 198, 201, 198, 32, 10073, 7443, 286, 6188, 8987, 357, 2043, 13219, 383, 2723, 1058, 22720, 12, 30297, 10692, 8581, 201, 17688, 201, 9237, 201, 21, 14, 1270, 14, 41931, 201, 7554, 26985, 3169, 40062, 34134, 262, 3274, 13650, 286, 257, 6358, 319, 262, 8392, 53, 2246, 11, 16118, 262, 8574, 12, 23065, 3721, 11, 543, 2627, 262, 8489, 286, 3660, 3644, 10959, 13, 201, 20, 14, 1828, 14, 40220, 220, 201, 18861, 3395, 66, 1604, 68, 800, 658, 31903, 379, 44264, 1140, 350, 25793, 11, 15882, 10714, 9498, 27862, 357, 25697, 82, 8, 290, 5854, 2890, 3644, 19843, 287, 9730, 13, 201, 18, 14, 16, 14, 25475, 220, 201, 14967, 6206, 364, 12, 24338, 2498, 15968, 366, 21918, 8549, 25, 317, 1041, 40007, 1, 379, 327, 28778, 11, 543, 27430, 262, 4096, 9355, 329, 262, 2159, 23399, 5313, 11, 3756, 284, 257, 3298, 8718, 5239, 1080, 329, 7373, 1321, 13, 201, 23664, 82, 201, 464, 4485, 286, 262, 2614, 3644, 357, 5662, 8, 3578, 3925, 284, 670, 34491, 11, 475, 2081, 2928, 25457, 618, 21869, 389, 5884, 2884, 24192, 82, 11, 1642, 3053, 262, 366, 32156, 598, 1, 290, 3649, 1366, 5270, 35529, 13, 201, 19891, 82, 201, 464, 2159, 23399, 5313, 8810, 11533, 11, 36045, 262, 5801, 4104, 286, 1321, 3675, 13953, 12, 18143, 4568, 290, 3756, 284, 262, 16839, 1634, 286, 867, 7612, 286, 4445, 1204, 13, 968, 7283, 2706, 588, 8180, 11, 5413, 11, 18650, 11, 28289, 11, 23617, 11, 290, 412, 9655, 14740, 355, 11410, 1938, 13, 201, 21, 14, 1314, 14, 1129, 2713, 201, 34, 28778, 10050, 262, 2159, 23399, 5313, 284, 262, 1171, 11, 1642, 262, 5230, 6768, 9857, 290, 47711, 262, 835, 329, 257, 3298, 3127, 286, 5884, 9061, 13, 201, 11024, 82, 201, 464, 7283, 2831, 15381, 3371, 43135, 11, 351, 16021, 1938, 10759, 319, 1994, 6805, 588, 39290, 12920, 669, 357, 24123, 828, 5361, 3341, 357, 15905, 828, 20083, 357, 48625, 828, 290, 19140, 357, 34, 4861, 737, 201, 11024, 82, 12, 10333, 82, 201, 464, 6279, 14492, 5854, 290, 262, 4485, 286, 366, 14261, 1366, 1, 1487, 262, 7283, 10747, 13, 383, 4455, 286, 11597, 357, 40, 78, 51, 8, 20417, 13188, 286, 4410, 11, 22056, 3649, 262, 2033, 286, 1366, 7560, 290, 4888, 13, 201, 21, 14, 1065, 14, 1129, 2713, 201, 19727, 449, 13, 5601, 768, 19451, 262, 6459, 15459, 416, 262, 9721, 6115, 286, 1321, 13, 201, 21, 14, 1507, 14, 1129, 2713, 201, 27640, 6143, 4329, 517, 1575, 12, 16803, 621, 3348, 13, 201, 21, 14, 1129, 14, 1129, 2713, 201, 13256, 18014, 290, 3271, 7122, 30567, 10400, 262, 3381, 366, 14261, 1366, 1, 287, 257, 9207, 319, 503, 12, 1659, 12, 7295, 32704, 13, 201, 21, 14, 1238, 14, 1129, 2713, 201, 7554, 371, 13, 11066, 20342, 10969, 319, 366, 12804, 6060, 1, 379, 257, 1294, 1677, 10426, 3249, 13, 201, 21, 14, 1828, 14, 1129, 2713, 201, 19727, 9334, 805, 290, 11023, 371, 13, 569, 3699, 7715, 257, 2050, 5554, 4035, 649, 1321, 6282, 11, 39539, 352, 13, 20, 409, 38346, 4635, 287, 7358, 13, 201, 21, 14, 1954, 14, 1129, 2713, 201, 42297, 15016, 88, 34134, 319, 513, 35, 1366, 4542, 11, 15010, 6115, 11, 15432, 11, 290, 4996, 355, 15225, 286, 1263, 1366, 13, 201, 21, 14, 1983, 14, 1129, 2713, 201, 14967, 440, 6, 25819, 19451, 262, 12085, 286, 1366, 287, 5313, 362, 13, 15, 13, 201, 21, 14, 1959, 14, 1129, 2713, 201, 2389, 34, 26119, 326, 4875, 1366, 481, 1663, 5566, 11, 39539, 860, 3459, 409, 38346, 416, 3050, 13, 201, 21, 14, 1270, 14, 1129, 2713, 201, 34, 4861, 26334, 6101, 4979, 481, 4274, 790, 734, 812, 11, 8978, 2063, 257, 1976, 3087, 37828, 287, 2321, 13, 201, 22, 14, 16, 14, 1129, 2713, 201, 32, 989, 7228, 3399, 13529, 1321, 329, 352, 13, 18, 12989, 2250, 287, 3648, 11, 37066, 513, 13, 21, 1976, 3087, 38346, 13, 201, 22, 14, 18, 14, 1129, 2713, 201, 24778, 47718, 290, 1736, 2304, 10102, 22593, 8636, 257, 6482, 422, 15075, 284, 4875, 6143, 11, 351, 10048, 4, 286, 6143, 5339, 852, 4875, 416, 4343, 13, 201, 22, 14, 18, 14, 1129, 2713, 201, 9742, 49681, 4397, 8060, 5136, 3136, 319, 262, 5801, 3349, 286, 8574, 1366, 1973, 16020, 11, 39539, 767, 13, 19, 409, 38346, 286, 649, 1366, 8574, 416, 23941, 287, 3050, 13, 201, 22, 14, 19, 14, 1129, 2713, 201, 24274, 4913, 286, 14620, 34134, 257, 2041, 2665, 319, 1321, 5339, 13, 201, 201, 198, 201, 198, 32, 7208, 286, 10286, 15813, 12, 35720, 11, 309, 22854, 37535, 11, 290, 9485, 15884, 354, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 50, 979, 15813, 12, 35720, 201, 51, 22854, 37535, 201, 20519, 15884, 354, 201, 35170, 5765, 201, 48485, 4572, 4673, 357, 5805, 8, 16113, 588, 17923, 11, 20683, 11, 32966, 1586, 11, 290, 662, 36948, 13, 201, 29744, 4673, 11, 17019, 7686, 11, 1588, 12, 9888, 10373, 13, 201, 29744, 4673, 11, 17019, 7686, 11, 2267, 12, 17107, 2769, 4673, 8861, 13, 201, 17614, 17738, 201, 26437, 11, 1029, 12, 5715, 11, 2562, 284, 779, 11, 290, 880, 12, 2385, 863, 329, 31729, 13, 201, 37, 2588, 856, 475, 460, 307, 3716, 11, 3544, 1029, 12, 5715, 17337, 292, 7824, 329, 10152, 13, 201, 5167, 11361, 291, 290, 19933, 11, 2592, 329, 4837, 290, 6505, 13, 201, 14055, 15417, 201, 37906, 357, 14508, 6971, 11812, 351, 584, 8950, 737, 201, 37906, 357, 39754, 828, 635, 6971, 327, 4880, 11, 11933, 11, 7349, 11, 1514, 11, 290, 1854, 13, 201, 37906, 357, 39754, 828, 635, 6971, 327, 4880, 13, 201, 6030, 286, 10373, 36848, 201, 48485, 10373, 357, 50, 15996, 11, 26423, 34925, 11, 14534, 9115, 11, 3503, 15729, 201, 29744, 4673, 357, 18474, 82, 11, 371, 6144, 82, 11, 39185, 828, 617, 1104, 329, 4569, 10373, 13, 201, 29744, 4673, 357, 18474, 82, 11, 371, 6144, 82, 11, 39185, 737, 201, 33346, 7929, 201, 2949, 1277, 1104, 329, 11362, 20309, 13, 201, 5297, 11, 1913, 1104, 329, 11362, 2884, 29369, 5631, 290, 309, 5105, 13, 201, 5297, 11, 6868, 11362, 1104, 2884, 29369, 5631, 13, 201, 20344, 6169, 38589, 201, 3673, 3562, 329, 9387, 3341, 11, 996, 11812, 351, 17732, 318, 1744, 13, 201, 15979, 2096, 9387, 3047, 1973, 3294, 32516, 290, 8217, 13, 201, 15979, 2096, 9387, 3047, 11, 1390, 1366, 10730, 1042, 290, 2746, 10730, 1042, 13, 201, 36, 589, 286, 5765, 201, 16371, 2562, 284, 779, 329, 15993, 10373, 8861, 351, 6414, 23113, 13, 201, 11922, 12, 5715, 7824, 357, 42, 263, 292, 8, 318, 2562, 11, 475, 1877, 12, 5715, 7824, 460, 307, 517, 3716, 13, 201, 5317, 33740, 329, 2769, 4673, 26, 8925, 31350, 28770, 787, 340, 4577, 284, 14257, 13, 201, 32273, 201, 36, 5632, 329, 1402, 284, 7090, 40522, 475, 407, 23392, 329, 4858, 40522, 393, 2769, 4673, 13, 201, 27871, 320, 1143, 329, 1588, 12, 9888, 2769, 4673, 290, 1029, 12, 26585, 14492, 13, 201, 11922, 306, 1620, 415, 329, 2769, 4673, 11, 3573, 319, 32516, 13, 201, 17633, 34706, 434, 201, 37214, 14833, 3689, 357, 68, 13, 70, 1539, 2884, 2298, 293, 393, 1693, 8019, 329, 10373, 4981, 737, 201, 51, 22854, 37535, 27395, 11, 309, 22854, 37535, 49208, 329, 3227, 12493, 13, 23399, 306, 973, 329, 5175, 290, 3992, 13, 201, 15979, 2096, 34868, 50, 3760, 329, 2746, 7351, 11, 475, 14833, 318, 1342, 7667, 621, 309, 22854, 37535, 13, 201, 36259, 1634, 20003, 201, 37214, 26, 48105, 351, 6550, 29487, 8019, 393, 1001, 397, 1211, 329, 32704, 13, 201, 51, 22854, 29828, 329, 2746, 3047, 32704, 290, 28769, 13, 201, 15979, 2096, 11812, 351, 309, 22854, 29828, 11, 475, 7380, 3170, 12, 259, 4899, 3688, 284, 309, 22854, 37535, 13, 201, 20012, 1222, 412, 12541, 201, 21968, 2055, 329, 4569, 10373, 26, 867, 662, 12, 18780, 16113, 290, 18366, 13, 201, 16371, 1588, 2055, 11, 1913, 3012, 12285, 11, 7667, 13187, 357, 51, 22854, 37535, 14699, 11, 27395, 11, 3503, 15729, 201, 43964, 2055, 11, 1913, 287, 34326, 290, 2267, 11, 2592, 2233, 284, 8925, 29964, 4823, 290, 13688, 13, 201, 41730, 46300, 201, 20535, 11, 3573, 329, 31729, 287, 10373, 13, 201, 31205, 284, 1029, 26, 17337, 292, 7824, 7106, 6945, 2769, 4673, 475, 309, 22854, 37535, 338, 1336, 13688, 318, 3716, 13, 201, 20535, 284, 7090, 26, 8925, 29964, 4823, 290, 11361, 291, 1486, 787, 340, 4577, 284, 2193, 329, 4837, 13, 201, 44090, 3691, 36125, 29681, 82, 201, 45442, 29964, 4823, 357, 1662, 16662, 329, 8925, 2769, 4673, 8861, 737, 201, 23828, 3093, 3544, 9037, 29964, 4823, 357, 2016, 6971, 11069, 9706, 737, 201, 44090, 29964, 4823, 11, 5086, 517, 13688, 290, 10152, 329, 28769, 13, 201, 6719, 12, 35311, 32329, 201, 32351, 11, 4632, 329, 4569, 10373, 357, 10698, 4351, 4673, 3689, 737, 201, 21968, 2746, 26626, 357, 51, 22854, 37535, 14699, 8, 329, 2769, 4673, 8861, 357, 68, 13, 70, 1539, 347, 17395, 11, 1874, 7934, 737, 201, 20519, 15884, 354, 14699, 4394, 257, 3957, 5888, 286, 662, 12, 35311, 4981, 357, 68, 13, 70, 1539, 1874, 7934, 11, 402, 11571, 737, 201, 16979, 33737, 201, 42559, 306, 973, 287, 2831, 290, 34326, 329, 4569, 10373, 13, 201, 43417, 415, 287, 3227, 12493, 329, 2769, 4673, 26, 973, 416, 867, 1588, 2706, 357, 11708, 11, 12024, 11, 3503, 15729, 201, 15562, 2313, 306, 2968, 287, 34326, 290, 2267, 11, 2592, 329, 7720, 12, 14907, 2769, 4673, 4981, 13, 201, 201, 198, 201, 198, 201, 198, 32, 7208, 1022, 6550, 29487, 8019, 290, 1001, 397, 1211, 12, 30297, 10692, 8581, 201, 38816, 201, 19044, 29487, 8019, 201, 4653, 397, 1211, 201, 30026, 3455, 201, 12218, 12, 29983, 29353, 5888, 201, 11922, 12, 5715, 7071, 329, 13905, 29353, 201, 36, 589, 286, 5765, 201, 5167, 1630, 475, 4433, 6496, 2438, 201, 36, 292, 959, 284, 779, 351, 3170, 12, 259, 13460, 290, 26235, 201, 15022, 1634, 201, 11922, 306, 38322, 475, 10107, 201, 39582, 319, 6550, 29487, 8019, 11, 4394, 11353, 35517, 290, 6596, 35431, 201, 43328, 24897, 201, 26416, 21528, 25, 1627, 11, 41058, 11, 2318, 11, 1554, 21857, 11, 3503, 13, 201, 13409, 1143, 329, 13905, 21528, 588, 4894, 31803, 11, 5166, 21528, 11, 38283, 21528, 201, 41730, 46300, 201, 7447, 5723, 329, 3716, 5874, 4582, 201, 36, 292, 959, 329, 2068, 11, 41696, 5874, 4582, 201, 19463, 317, 395, 24965, 201, 9452, 4402, 416, 4277, 201, 18716, 1836, 11, 20239, 11, 290, 10966, 416, 4277, 201, 17126, 19929, 15612, 1634, 201, 39618, 10107, 9058, 329, 13905, 21528, 201, 39582, 12, 259, 1104, 329, 13905, 21528, 357, 68, 13, 70, 1539, 36632, 11, 20683, 8, 201, 34500, 1358, 201, 23044, 880, 351, 1877, 12, 5715, 12782, 588, 31835, 20519, 201, 34500, 9700, 33681, 351, 16492, 292, 6060, 35439, 290, 31835, 20519, 201, 464, 6880, 14, 18716, 829, 201, 15022, 13821, 475, 4433, 10107, 6460, 201, 39156, 18156, 13460, 357, 68, 13, 70, 1539, 366, 21953, 25928, 1600, 366, 11186, 25928, 4943, 329, 2562, 35517, 201, 32273, 201, 11922, 306, 1620, 415, 329, 1588, 40522, 201, 50, 30945, 13611, 2233, 284, 3224, 34651, 625, 6550, 29487, 8019, 201, 2025, 30078, 1222, 3498, 1424, 201, 39618, 10107, 22097, 290, 16895, 201, 8890, 489, 6945, 27393, 351, 3170, 12, 259, 11244, 201, 6601, 49500, 201, 6090, 7110, 1366, 422, 26515, 11, 8341, 11, 6060, 35439, 201, 23828, 3093, 3562, 329, 16492, 292, 6060, 35439, 290, 43044, 1366, 201, 20012, 1222, 7929, 201, 21968, 2055, 11, 7667, 10314, 201, 18712, 263, 475, 991, 4075, 2055, 11, 1912, 319, 6550, 29487, 8019, 201, 201, 198, 201, 198, 201, 198, 32, 7208, 1022, 262, 13614, 5345, 11, 3254, 24765, 5345, 11, 290, 23983, 5345, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 44357, 5345, 201, 7762, 24765, 5345, 201, 44154, 5345, 201, 30026, 3455, 201, 38052, 284, 4512, 262, 2746, 416, 22000, 663, 19590, 290, 4673, 7572, 13, 201, 38052, 284, 14009, 262, 2746, 338, 8718, 17143, 7307, 290, 2948, 625, 32232, 13, 201, 38052, 284, 13446, 262, 2457, 2746, 338, 2854, 319, 29587, 1366, 13, 201, 6601, 955, 9150, 201, 5377, 18166, 262, 4387, 6903, 286, 262, 27039, 357, 68, 13, 70, 1539, 4317, 12, 1795, 18823, 201, 49321, 11, 4833, 621, 262, 3047, 900, 357, 68, 13, 70, 1539, 838, 12, 1238, 18823, 201, 7583, 11, 257, 1402, 6903, 286, 262, 27039, 357, 68, 13, 70, 1539, 838, 12, 1238, 18823, 201, 17633, 37774, 201, 464, 2746, 22974, 422, 428, 1366, 290, 5992, 663, 10007, 1912, 319, 262, 3047, 1429, 13, 201, 464, 2746, 11583, 7538, 319, 663, 2854, 284, 1037, 2987, 290, 27183, 1141, 3047, 13, 201, 464, 2746, 857, 407, 3328, 7538, 1141, 428, 3800, 26, 340, 5260, 2276, 1634, 13, 201, 28350, 287, 13614, 26993, 201, 13470, 306, 2950, 287, 262, 3047, 6772, 810, 262, 2746, 22974, 262, 1366, 3033, 13, 201, 38052, 1141, 3047, 284, 26571, 2746, 2854, 290, 4532, 8718, 17143, 7307, 11, 1690, 2884, 7605, 588, 3272, 12, 12102, 341, 13, 201, 38052, 706, 262, 2746, 318, 3938, 8776, 284, 4659, 9922, 290, 2854, 20731, 13, 201, 26950, 529, 319, 9104, 201, 13470, 306, 12751, 703, 880, 262, 2746, 22974, 422, 262, 1366, 13, 201, 32, 4812, 82, 262, 3572, 286, 8718, 17143, 7307, 11, 40288, 2746, 10959, 290, 2276, 1634, 2694, 13, 201, 5497, 16856, 703, 880, 262, 2746, 481, 1620, 287, 1103, 12, 6894, 5479, 13, 201, 16281, 33956, 201, 2154, 12, 1795, 4, 286, 262, 2472, 27039, 13, 201, 940, 12, 1238, 4, 286, 262, 2472, 27039, 13, 201, 940, 12, 1238, 4, 286, 262, 2472, 27039, 13, 201, 6601, 1004, 461, 496, 19602, 201, 49, 1984, 286, 625, 32232, 611, 1165, 1178, 6096, 389, 973, 393, 611, 262, 2746, 318, 1165, 3716, 13, 201, 5886, 32232, 460, 991, 3051, 611, 21201, 900, 318, 407, 8852, 393, 611, 8718, 17143, 7307, 389, 407, 16524, 7773, 13, 201, 19926, 307, 3190, 4553, 284, 2148, 281, 46735, 12660, 286, 2746, 2854, 13, 201, 201, 198, 201, 198, 201, 198, 32, 7208, 1022, 3827, 32232, 11, 4698, 32232, 11, 290, 257, 38984, 9104, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 5886, 32232, 201, 9203, 32232, 201, 24597, 2903, 9104, 201, 36621, 201, 464, 2746, 22974, 262, 3047, 1366, 1165, 880, 11, 21430, 7838, 290, 41528, 3183, 11, 7186, 287, 3595, 2276, 1634, 284, 649, 1366, 13, 201, 464, 2746, 318, 1165, 2829, 284, 8006, 262, 10238, 7572, 287, 262, 1366, 11, 3756, 284, 3595, 2854, 319, 1111, 3047, 290, 4856, 40522, 13, 201, 464, 2746, 22668, 23007, 262, 10238, 7572, 287, 262, 1366, 11, 9489, 880, 319, 1111, 3047, 290, 4856, 40522, 13, 201, 17633, 19157, 414, 201, 11922, 306, 3716, 351, 1165, 867, 10007, 3585, 284, 262, 2033, 286, 1366, 13, 201, 23307, 2829, 351, 1165, 1178, 10007, 393, 3033, 13, 201, 4677, 9219, 378, 13357, 11, 22486, 10690, 290, 24198, 13, 201, 44357, 15193, 201, 11922, 9922, 319, 262, 3047, 900, 13, 201, 20535, 9922, 319, 1111, 3047, 290, 4856, 5621, 13, 201, 10248, 9922, 319, 1111, 3047, 290, 4856, 5621, 13, 201, 44154, 15193, 201, 43920, 9922, 319, 262, 4856, 900, 2233, 284, 3092, 286, 2276, 1634, 13, 201, 43920, 9922, 319, 262, 4856, 900, 11, 2092, 284, 3047, 900, 2854, 13, 201, 10248, 9922, 319, 262, 4856, 900, 11, 12739, 4050, 2276, 1634, 13, 201, 5497, 44549, 201, 21968, 3580, 1022, 3047, 290, 4856, 9922, 357, 8929, 3047, 9922, 11, 1877, 4856, 9922, 737, 201, 22253, 3580, 287, 2854, 1022, 3047, 290, 4856, 5621, 11, 475, 1111, 389, 1877, 13, 201, 18925, 290, 29962, 2854, 20731, 329, 1111, 3047, 290, 4856, 5621, 13, 201, 17227, 46865, 201, 3109, 45428, 3047, 640, 11, 1165, 867, 3033, 11, 393, 3092, 286, 3218, 1634, 13, 201, 20376, 15267, 3047, 11, 17698, 35010, 2746, 11, 393, 3595, 3895, 6356, 13, 201, 2964, 525, 2746, 6356, 11, 6751, 3047, 11, 290, 5035, 3895, 8705, 13, 201, 16281, 201, 32, 2746, 326, 16181, 4340, 790, 1366, 966, 287, 262, 3047, 900, 11, 9894, 284, 4331, 649, 1366, 14351, 13, 201, 32, 14174, 2746, 2111, 284, 4197, 257, 3716, 11, 1729, 12, 29127, 2776, 287, 262, 1366, 13, 201, 32, 2551, 5509, 326, 7228, 257, 6397, 5236, 1022, 13357, 290, 2276, 1634, 13, 201, 201, 198, 201, 198, 201, 198, 201, 198, 32, 3084, 14176, 402, 5362, 12901, 11, 7232, 28338, 11, 290, 30964, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 38, 5362, 12901, 201, 14539, 28338, 201, 2949, 786, 201, 36621, 201, 32, 18663, 326, 5260, 262, 848, 1684, 286, 257, 6626, 13, 201, 32, 18663, 422, 1321, 4583, 326, 5260, 13479, 393, 848, 1684, 287, 1366, 13, 201, 3118, 86, 4126, 4738, 12291, 393, 38622, 287, 1366, 11, 19938, 284, 262, 10238, 6737, 13, 201, 17257, 201, 15, 284, 657, 13, 20, 357, 39491, 17923, 737, 657, 796, 5899, 10139, 11, 657, 13, 20, 796, 5415, 848, 1684, 13, 201, 15, 284, 352, 357, 39491, 17923, 737, 657, 796, 5899, 10139, 11, 352, 796, 5415, 13479, 13, 201, 12156, 2412, 319, 262, 1366, 26, 3221, 16069, 422, 1402, 22146, 5945, 602, 284, 2383, 44365, 13, 201, 9492, 5310, 341, 201, 31426, 3815, 7603, 5899, 81, 13760, 357, 1203, 848, 1684, 737, 201, 31426, 3815, 7603, 1342, 8967, 393, 13479, 13, 16038, 3815, 905, 517, 8967, 13, 201, 6207, 6629, 262, 1241, 286, 25100, 287, 262, 1366, 11, 543, 460, 1085, 284, 21873, 16277, 13, 201, 6892, 341, 284, 6060, 201, 5308, 13846, 262, 12867, 326, 257, 15456, 7147, 5002, 561, 307, 2984, 31691, 13, 201, 5308, 13846, 262, 2033, 286, 1321, 2622, 284, 36509, 1366, 2173, 13, 201, 6207, 6629, 1366, 48302, 326, 743, 18611, 7572, 393, 2689, 2746, 9922, 13, 201, 47445, 287, 26423, 34925, 201, 38052, 284, 5004, 262, 1266, 6626, 416, 41366, 848, 1684, 287, 1200, 13760, 13, 201, 38052, 284, 2922, 262, 1266, 6626, 416, 48350, 1321, 4461, 13, 201, 9444, 3089, 257, 2723, 286, 4049, 393, 25100, 26, 2551, 7150, 4031, 284, 4646, 262, 2928, 286, 7838, 319, 26021, 9987, 13, 201, 201, 198, 201, 198, 201, 198, 2437, 24846, 18243, 5419, 287, 4917, 262, 1266, 8718, 17143, 7307, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 41339, 18243, 318, 281, 36049, 2989, 2446, 284, 1064, 262, 1266, 6087, 286, 8718, 17143, 7307, 329, 257, 4572, 4673, 2746, 13, 201, 18709, 201, 1026, 8404, 790, 6087, 286, 8718, 17143, 2357, 3815, 7368, 287, 257, 2747, 18156, 10706, 290, 47850, 262, 2746, 329, 1123, 6087, 13, 201, 41339, 21582, 201, 32, 10706, 286, 8718, 17143, 2357, 3815, 318, 5447, 416, 262, 2836, 11, 17747, 286, 2972, 1744, 3815, 329, 1123, 8718, 17143, 2357, 13, 201, 18243, 20561, 201, 3109, 42456, 425, 2989, 625, 262, 2104, 10706, 286, 8718, 17143, 7307, 26, 1123, 6087, 318, 16726, 14799, 13, 201, 36, 2100, 2288, 201, 1890, 1123, 6087, 11, 262, 2746, 318, 8776, 290, 16726, 357, 23073, 1262, 3272, 12, 12102, 341, 8, 284, 24061, 2854, 20731, 13, 201, 32273, 3395, 1173, 201, 464, 2836, 26052, 257, 2854, 18663, 357, 68, 13, 70, 1539, 9922, 11, 376, 16, 12, 26675, 11, 3503, 2014, 284, 13446, 1123, 8718, 17143, 2357, 6087, 13, 201, 13014, 15079, 17143, 2357, 29538, 201, 3260, 22232, 477, 17790, 11, 24846, 18243, 40573, 262, 8718, 17143, 2357, 900, 326, 12991, 4340, 262, 7147, 2854, 18663, 13, 201, 7575, 19157, 414, 201, 6090, 307, 2653, 15208, 5789, 780, 340, 47850, 790, 1744, 6087, 286, 8718, 17143, 7307, 357, 16480, 351, 1588, 50000, 737, 201, 2782, 4520, 1095, 201, 26437, 284, 3494, 11, 19047, 477, 17790, 389, 6789, 11, 290, 19026, 4917, 262, 1266, 6087, 1626, 262, 1813, 10706, 13, 201, 19352, 20597, 201, 5377, 1996, 15208, 5789, 11, 2592, 351, 867, 8718, 17143, 7307, 290, 1588, 2989, 9029, 26, 1595, 470, 5046, 880, 284, 3716, 4981, 13, 201, 11041, 8913, 201, 13014, 16662, 329, 4981, 351, 4833, 8718, 17143, 2357, 9029, 393, 618, 31350, 4133, 389, 6751, 329, 36049, 2989, 13, 201, 201, 198, 201, 198, 201, 198, 32, 7208, 1022, 5972, 2569, 3310, 2234, 11, 7929, 20650, 5016, 7483, 357, 50, 15922, 828, 26423, 34925, 11, 290, 509, 12, 8199, 12423, 22505, 32289, 357, 42, 6144, 13219, 30297, 10692, 8581, 201, 1722, 806, 201, 11187, 2569, 3310, 2234, 201, 15514, 20650, 5016, 7483, 357, 50, 15922, 8, 201, 10707, 1166, 34925, 201, 42, 12, 8199, 12423, 22505, 32289, 357, 42, 6144, 8, 201, 6030, 286, 978, 42289, 201, 14993, 451, 2746, 11, 1861, 14991, 2569, 1398, 7483, 201, 15419, 12, 29127, 357, 5171, 635, 307, 14174, 351, 1728, 50207, 8, 201, 15419, 12, 17143, 19482, 11, 5509, 12, 3106, 2746, 201, 33384, 12, 3106, 4673, 357, 75, 12582, 4673, 11862, 8, 201, 19044, 10024, 605, 6455, 271, 201, 5842, 274, 257, 2604, 2569, 357, 82, 17225, 1868, 8, 2163, 284, 2746, 262, 12867, 286, 13934, 10906, 201, 11518, 320, 4340, 262, 10330, 1022, 1366, 2173, 290, 2551, 18645, 357, 49229, 14382, 8, 201, 5842, 274, 257, 5509, 4645, 284, 6626, 1366, 1912, 319, 3895, 6817, 201, 9487, 6945, 1912, 319, 262, 3741, 1398, 286, 16936, 12020, 201, 11041, 8913, 201, 33, 3219, 17923, 2761, 357, 5171, 307, 7083, 284, 5021, 12, 4871, 8, 201, 9487, 2649, 286, 1111, 14174, 290, 1729, 12, 29127, 1366, 201, 5606, 4674, 329, 1111, 17923, 290, 20683, 8861, 201, 5606, 4674, 329, 17923, 8861, 11, 460, 635, 307, 973, 329, 20683, 201, 8021, 388, 8544, 201, 8021, 8139, 257, 14174, 2776, 1022, 3033, 290, 2604, 12, 5088, 82, 286, 262, 8055, 201, 8021, 8139, 1366, 460, 307, 11266, 351, 257, 8718, 14382, 287, 1029, 12, 19577, 2272, 201, 8021, 8139, 645, 2176, 6082, 11, 645, 5772, 19482, 14895, 201, 2949, 14895, 546, 262, 6082, 286, 1366, 201, 9492, 5310, 1799, 201, 11922, 11, 44036, 905, 262, 2928, 286, 1123, 3895, 201, 31205, 11, 517, 2408, 284, 6179, 2233, 284, 9720, 38226, 201, 31205, 11, 2562, 284, 6179, 262, 2551, 3108, 287, 262, 5509, 201, 20535, 11, 355, 340, 691, 16507, 319, 262, 5253, 18663, 422, 262, 16936, 12020, 201, 32273, 319, 10452, 6060, 201, 10248, 329, 1402, 284, 7090, 12, 13982, 40522, 201, 23044, 880, 351, 4833, 40522, 475, 460, 5046, 351, 9720, 6908, 201, 23044, 880, 351, 1402, 40522, 11, 17592, 284, 625, 32232, 201, 44831, 329, 1402, 40522, 475, 460, 307, 3105, 319, 4025, 3392, 201, 32273, 319, 13601, 6060, 201, 36, 5632, 329, 1588, 40522, 201, 6090, 307, 2653, 15208, 5789, 351, 1588, 40522, 201, 1273, 622, 32723, 351, 1588, 11, 3716, 40522, 357, 46330, 284, 625, 32232, 8, 201, 39649, 2586, 2653, 15208, 5789, 351, 1588, 40522, 201, 12885, 1359, 286, 8504, 12, 29127, 414, 201, 3673, 11080, 329, 1729, 12, 29127, 2761, 1231, 3895, 13389, 201, 45675, 329, 1729, 12, 29127, 2761, 1262, 50207, 201, 12885, 829, 1729, 12, 29127, 1366, 8752, 832, 49526, 201, 6090, 5412, 1729, 12, 29127, 2761, 880, 6906, 319, 262, 5253, 18663, 973, 201, 5886, 32232, 48664, 1387, 201, 22058, 17592, 284, 625, 32232, 611, 3218, 1634, 318, 5625, 201, 6090, 625, 11147, 611, 262, 9720, 318, 1165, 3716, 201, 11922, 306, 17592, 284, 625, 32232, 11, 2592, 1231, 778, 46493, 201, 6836, 505, 284, 625, 32232, 611, 479, 357, 17618, 286, 12020, 8, 318, 1402, 201, 38197, 17143, 7307, 201, 40164, 1634, 4202, 357, 34, 828, 1540, 332, 201, 42, 7948, 357, 29127, 11, 745, 6213, 49070, 11, 17986, 37, 828, 327, 357, 3617, 6017, 11507, 828, 34236, 201, 11518, 6795, 11, 949, 8405, 583, 6626, 11, 26021, 34054, 201, 15057, 286, 12020, 357, 74, 828, 5253, 18663, 357, 36, 36616, 485, 272, 11, 13458, 11, 3503, 2014, 201, 5377, 1996, 864, 19157, 414, 201, 20535, 11, 2653, 15208, 6942, 201, 48708, 13357, 2233, 284, 1104, 30104, 290, 9720, 29964, 201, 20535, 13357, 329, 1402, 7150, 475, 13676, 351, 5509, 6795, 201, 11922, 11, 355, 340, 9018, 14492, 18868, 422, 1123, 966, 284, 477, 584, 2173, 201, 12885, 1359, 3806, 75, 3183, 201, 6090, 307, 8564, 284, 41528, 3183, 201, 14350, 436, 284, 41528, 3183, 611, 10330, 318, 3094, 201, 50, 18464, 284, 41528, 3183, 26, 460, 1085, 284, 37543, 30778, 201, 50, 18464, 284, 41528, 3183, 1201, 5253, 12, 3106, 5050, 389, 5676, 416, 606, 201, 30871, 29566, 201, 20535, 11, 691, 4433, 23069, 44036, 201, 31205, 284, 1029, 11, 8338, 319, 262, 1271, 286, 1104, 30104, 201, 20535, 284, 10768, 11, 8338, 319, 5509, 6795, 201, 11922, 11, 1201, 340, 2476, 284, 3650, 477, 3047, 1366, 329, 7208, 201, 39156, 2867, 8729, 201, 22968, 201, 22968, 706, 3047, 11, 475, 3047, 460, 307, 3105, 329, 3716, 50207, 201, 22968, 1752, 262, 5509, 318, 3170, 201, 36423, 11, 355, 340, 552, 1769, 262, 5253, 284, 477, 2173, 379, 17724, 640, 201, 16281, 201, 15333, 18084, 13326, 11, 4369, 13669, 201, 5159, 17923, 11, 2420, 17923, 201, 23690, 9689, 11, 6491, 28488, 17724, 201, 12885, 16502, 9465, 11, 15602, 3341, 201, 201, 198, 201, 198, 201, 198, 201, 198, 32, 7208, 1022, 24846, 18243, 290, 24846, 18243, 33538, 12, 30297, 10692, 8581, 201, 38816, 201, 41339, 18243, 201, 41339, 18243, 33538, 201, 36621, 201, 32, 1429, 326, 47850, 477, 17790, 286, 8718, 17143, 7307, 625, 257, 1813, 900, 475, 857, 407, 6211, 3272, 12, 12102, 341, 13, 201, 32, 2446, 422, 1341, 35720, 13, 19849, 62, 49283, 326, 17706, 36049, 2989, 625, 7368, 8718, 17143, 2357, 3815, 351, 3170, 12, 259, 3272, 12, 12102, 341, 13, 201, 35170, 5765, 201, 5124, 935, 9177, 284, 1064, 262, 1266, 8718, 17143, 7307, 11, 3221, 1231, 11353, 3272, 12, 12102, 341, 13, 201, 38052, 284, 6338, 14009, 8718, 17143, 7307, 351, 3272, 12, 12102, 341, 3170, 287, 11, 13359, 2746, 12373, 1108, 13, 201, 21544, 12, 7762, 24765, 201, 13921, 407, 1620, 3272, 12, 12102, 341, 416, 4277, 13, 921, 1276, 14500, 6626, 262, 1366, 393, 779, 3224, 21201, 7605, 13, 201, 5990, 23914, 3272, 12, 12102, 341, 357, 33538, 8, 6338, 1912, 319, 262, 2810, 269, 85, 11507, 357, 68, 13, 70, 1539, 479, 12, 69, 10119, 737, 201, 23377, 7929, 201, 3673, 3264, 4855, 416, 12782, 588, 629, 1134, 270, 12, 35720, 13, 27095, 4433, 10107, 19617, 329, 11507, 2989, 13, 201, 13470, 306, 4855, 416, 629, 1134, 270, 12, 35720, 351, 262, 1398, 24846, 18243, 33538, 13, 201, 17633, 34959, 201, 36, 2100, 12632, 2746, 2854, 1912, 319, 257, 1813, 21201, 900, 11, 407, 1262, 3294, 30778, 329, 26196, 13, 201, 5842, 274, 3272, 12, 12102, 341, 11, 22232, 262, 2746, 1973, 3294, 38744, 286, 3047, 1366, 284, 1577, 257, 517, 9314, 2854, 8636, 13, 201, 5886, 32232, 19602, 201, 48708, 2526, 286, 625, 32232, 1201, 340, 743, 13446, 262, 2746, 691, 319, 257, 2060, 21201, 900, 13, 201, 31426, 2526, 286, 625, 32232, 2233, 284, 3272, 12, 12102, 341, 11, 355, 340, 5254, 262, 2746, 1973, 1180, 1366, 38744, 13, 201, 36, 35590, 201, 22058, 6942, 287, 2846, 286, 13359, 2276, 1634, 1201, 340, 743, 2962, 319, 257, 2176, 27039, 6626, 13, 201, 5167, 6942, 287, 22232, 262, 2276, 1634, 286, 262, 2746, 416, 4856, 319, 3294, 1366, 30778, 13, 201, 26410, 201, 15946, 1460, 262, 1266, 10007, 1912, 319, 262, 7368, 21201, 900, 13, 201, 15946, 1460, 262, 1266, 10007, 1912, 319, 3272, 12, 12102, 515, 2854, 1973, 1180, 38744, 201, 201, 198, 201, 198, 32, 3084, 15676, 2890, 262, 7767, 262, 2746, 2925, 832, 1141, 530, 36835, 12, 30297, 10692, 8581, 201, 8600, 201, 11828, 201, 16, 13, 911, 1648, 1359, 201, 19722, 453, 36273, 262, 27039, 284, 4738, 1096, 262, 1502, 286, 8405, 878, 7587, 357, 35194, 2948, 10690, 737, 201, 17, 13, 347, 963, 6282, 201, 24095, 485, 262, 27039, 656, 4833, 6352, 1039, 1444, 37830, 329, 6942, 7587, 1141, 3047, 13, 201, 18, 13, 19530, 8772, 363, 341, 201, 1890, 1123, 15458, 11, 262, 2746, 8318, 262, 5128, 1366, 832, 262, 3127, 11685, 284, 4439, 16277, 13, 201, 19, 13, 22014, 2199, 14902, 201, 41488, 262, 2746, 338, 16277, 284, 262, 4036, 14722, 1262, 257, 2994, 2163, 284, 36336, 262, 4049, 13, 201, 20, 13, 5157, 22930, 363, 341, 201, 7293, 1133, 3915, 2334, 357, 47172, 28486, 286, 262, 2994, 8, 416, 8928, 803, 262, 4049, 19528, 832, 262, 3127, 13, 201, 21, 13, 14331, 28090, 201, 10260, 262, 2746, 338, 19590, 1262, 262, 29231, 3915, 2334, 290, 281, 23989, 11862, 357, 68, 13, 70, 1539, 26147, 35, 11, 7244, 737, 201, 22, 13, 34959, 201, 19722, 453, 13446, 262, 2746, 338, 2854, 319, 20731, 588, 9922, 11, 1262, 2035, 3047, 393, 21201, 1366, 13, 201, 23, 13, 30021, 329, 1439, 6577, 2052, 201, 40322, 4831, 513, 284, 767, 329, 1123, 15458, 287, 262, 27039, 1626, 262, 36835, 13, 201, 24, 13, 4551, 5374, 12, 12915, 24439, 201, 30719, 4028, 588, 4938, 803, 319, 257, 4553, 27039, 11, 22000, 262, 4673, 2494, 11, 393, 8914, 2746, 36628, 13, 201, 201, 198, 201, 198, 201, 198, 201, 198, 201, 198, 32, 3084, 11170, 262, 2846, 3519, 284, 2769, 4673, 532, 30297, 10692, 8581, 201, 40596, 201, 3109, 11578, 341, 201, 13807, 5374, 201, 3198, 1844, 1208, 832, 262, 2104, 3047, 27039, 13, 5856, 281, 36835, 11, 262, 2746, 7767, 790, 1366, 966, 287, 262, 27039, 11, 46094, 663, 19590, 11, 290, 22974, 422, 262, 1366, 13, 201, 41730, 14806, 201, 32, 8718, 17143, 2357, 326, 6973, 703, 881, 284, 4532, 262, 2746, 338, 19590, 287, 2882, 284, 262, 10488, 3915, 2334, 13, 317, 2440, 4673, 2494, 12055, 510, 3047, 475, 743, 625, 30408, 11, 981, 257, 2793, 2494, 4394, 517, 7141, 16895, 475, 13611, 40826, 13, 201, 25526, 341, 201, 464, 2163, 5625, 379, 1123, 43164, 287, 257, 17019, 3127, 7679, 11, 16118, 1729, 12, 29127, 414, 656, 262, 2746, 13, 8070, 14916, 5499, 2291, 797, 41596, 11, 264, 17225, 1868, 11, 290, 25706, 71, 13, 201, 40164, 1634, 201, 32, 8173, 973, 284, 2948, 625, 32232, 416, 4375, 257, 7389, 357, 273, 32315, 8, 284, 262, 2994, 2163, 13, 8070, 5050, 2291, 406, 16, 357, 43, 28372, 8, 290, 406, 17, 357, 49, 3130, 8, 3218, 1634, 13, 201, 40164, 1634, 14806, 201, 32, 8718, 17143, 2357, 326, 6973, 262, 4202, 286, 262, 3218, 1634, 5625, 13, 317, 2440, 3218, 1634, 2494, 5732, 262, 7389, 319, 4025, 19590, 11, 5742, 284, 30276, 262, 2746, 290, 4646, 625, 32232, 13, 201, 29665, 952, 286, 13614, 284, 6208, 12849, 201, 464, 9823, 286, 262, 27039, 973, 329, 3047, 9051, 4856, 357, 273, 21201, 737, 317, 7226, 6626, 1244, 307, 4019, 4, 3047, 290, 1160, 4, 4856, 13, 383, 3047, 900, 318, 973, 329, 4673, 11, 981, 262, 1332, 900, 47850, 2746, 2854, 319, 29587, 1366, 13, 201, 2949, 786, 201, 3118, 86, 4126, 13991, 393, 4738, 1108, 287, 262, 1366, 326, 460, 30867, 262, 4673, 1429, 13, 554, 2769, 4673, 11, 7838, 460, 15058, 422, 1366, 4947, 8563, 11, 393, 340, 460, 307, 14593, 5495, 355, 257, 3218, 1634, 8173, 357, 68, 13, 70, 1539, 4268, 448, 11, 4375, 12822, 31562, 7838, 737, 201, 33, 963, 12849, 201, 464, 1271, 286, 3047, 6096, 13686, 287, 530, 24415, 13, 10452, 263, 15458, 10620, 1085, 284, 645, 271, 959, 5992, 475, 2421, 1342, 4088, 290, 460, 2276, 1096, 1365, 13, 406, 32270, 15458, 10620, 460, 307, 517, 8245, 475, 743, 1085, 284, 13611, 2276, 1634, 13, 8070, 10620, 2291, 3933, 11, 5598, 11, 290, 13108, 13, 201, 201, 198, 201, 198, 201, 198, 32, 3084, 11170, 262, 33449, 286, 257, 17019, 3127, 287, 2769, 4673, 12, 30297, 10692, 8581, 201, 21950, 201, 3109, 11578, 341, 201, 20560, 34398, 201, 464, 717, 7679, 287, 262, 17019, 3127, 326, 11583, 262, 8246, 1366, 357, 40890, 8, 284, 307, 13686, 13, 5501, 43164, 287, 428, 7679, 24866, 284, 530, 3895, 422, 262, 5128, 1366, 13, 201, 41691, 406, 6962, 201, 9492, 13857, 11685, 1022, 262, 5128, 290, 5072, 11685, 11, 810, 749, 286, 262, 2653, 602, 3051, 13, 383, 3127, 460, 423, 530, 393, 517, 7104, 11685, 13, 5501, 7104, 7679, 8991, 19590, 11, 29275, 11, 290, 14916, 5499, 284, 6121, 262, 5128, 656, 1223, 11570, 329, 262, 2746, 13, 201, 8199, 333, 684, 201, 464, 4096, 4991, 287, 1123, 7679, 13, 5501, 43164, 2753, 17311, 11, 8991, 19590, 290, 29275, 11, 21784, 606, 11, 290, 8318, 262, 1255, 832, 281, 14916, 2163, 13, 383, 5072, 318, 788, 3804, 284, 16890, 287, 262, 1306, 7679, 13, 201, 1135, 2337, 201, 48944, 326, 5004, 262, 4202, 290, 4571, 286, 262, 4637, 1022, 16890, 287, 15909, 11685, 13, 775, 2337, 389, 4499, 1141, 3047, 290, 12328, 284, 17775, 262, 2994, 2163, 13, 201, 23286, 1386, 201, 17699, 10007, 2087, 284, 262, 26356, 2160, 286, 17311, 878, 11524, 262, 14916, 2163, 13, 8436, 1386, 1037, 262, 2746, 4197, 262, 1366, 1365, 416, 15852, 262, 14916, 2163, 13, 201, 25526, 341, 40480, 201, 24629, 2733, 5625, 379, 1123, 43164, 284, 10400, 1729, 12, 29127, 414, 13, 8070, 5499, 2291, 797, 41596, 11, 264, 17225, 1868, 11, 290, 25706, 71, 11, 543, 1249, 262, 2746, 284, 8006, 3716, 7572, 287, 262, 1366, 13, 201, 26410, 34398, 201, 464, 2457, 7679, 326, 3769, 262, 16277, 393, 5072, 286, 262, 2746, 13, 383, 1271, 286, 16890, 287, 262, 5072, 7679, 6032, 24866, 284, 262, 1271, 286, 17724, 6097, 393, 262, 2496, 9633, 287, 20683, 13, 201, 39746, 8772, 363, 341, 201, 464, 1429, 810, 5128, 1366, 8318, 832, 262, 3127, 422, 262, 5128, 7679, 11, 832, 7104, 11685, 11, 284, 262, 5072, 7679, 11, 15453, 16277, 1912, 319, 1459, 19590, 290, 29275, 13, 201, 43, 793, 15553, 201, 32, 2163, 326, 5260, 262, 4049, 393, 3580, 1022, 262, 11001, 5072, 290, 262, 4036, 2496, 1988, 13, 8070, 2994, 5499, 2291, 22728, 5056, 1144, 13047, 329, 20683, 290, 6372, 12, 14539, 28338, 22014, 329, 17923, 13, 201, 7282, 22930, 363, 341, 201, 464, 1429, 286, 26019, 262, 31312, 286, 262, 2994, 2163, 351, 2461, 284, 262, 19590, 290, 29275, 287, 262, 3127, 11, 3599, 422, 262, 5072, 7679, 290, 3867, 19528, 832, 262, 11685, 13, 770, 3578, 262, 3127, 284, 2193, 416, 19698, 262, 19590, 284, 17775, 262, 2994, 13, 201, 27871, 320, 1634, 978, 42289, 201, 2025, 11862, 357, 10508, 355, 520, 5374, 3477, 17701, 1153, 2935, 1087, 393, 7244, 8, 973, 284, 4296, 262, 3127, 338, 19590, 290, 29275, 1912, 319, 262, 3915, 2334, 29231, 1141, 736, 22930, 363, 341, 11, 284, 17775, 262, 2994, 2163, 13, 201, 41730, 14806, 201, 32, 8718, 17143, 2357, 326, 6973, 703, 881, 284, 4532, 262, 19590, 1141, 1123, 4296, 13, 317, 2793, 4673, 2494, 1838, 4833, 16895, 290, 6718, 3212, 6364, 11, 981, 257, 2440, 4673, 2494, 460, 2866, 510, 3047, 475, 743, 625, 30408, 262, 16586, 4610, 13, 201, 13807, 5374, 201, 3198, 1336, 1208, 832, 262, 2104, 3047, 27039, 13, 5856, 281, 36835, 11, 262, 2746, 7767, 262, 1366, 11, 43707, 262, 2994, 11, 17706, 736, 22930, 363, 341, 11, 290, 5992, 262, 19590, 13, 20401, 36835, 82, 389, 2622, 329, 262, 2746, 284, 2193, 6840, 13, 201, 33, 963, 12849, 201, 464, 1271, 286, 3047, 8405, 13686, 287, 530, 2651, 290, 19528, 1208, 13, 10452, 15458, 10620, 1085, 284, 645, 271, 959, 5992, 475, 460, 1037, 2276, 1096, 1365, 11, 981, 1588, 15458, 10620, 2148, 517, 8245, 5992, 475, 743, 2421, 517, 4088, 13, 201, 40164, 1634, 201, 25574, 6368, 357, 10508, 355, 406, 16, 290, 406, 17, 3218, 1634, 393, 4268, 448, 8, 973, 284, 2948, 625, 32232, 416, 4375, 257, 7389, 284, 262, 2994, 2163, 393, 15456, 34909, 16890, 1141, 3047, 13, 201, 26932, 448, 201, 32, 3218, 1634, 8173, 810, 257, 4738, 900, 286, 16890, 318, 9514, 357, 22285, 1496, 503, 8, 1141, 1123, 3047, 24415, 13, 770, 3386, 262, 2746, 284, 2193, 517, 12373, 3033, 290, 5419, 2948, 625, 32232, 13, 201, 18380, 9806, 201, 32, 2163, 1690, 973, 287, 262, 5072, 7679, 329, 5021, 12, 4871, 17923, 8861, 13, 632, 26161, 262, 5072, 8198, 656, 39522, 11, 1642, 340, 4577, 284, 6179, 543, 1398, 262, 2746, 26334, 13, 201, 201, 198, 201, 198, 64, 29270, 3084, 36995, 262, 1994, 5400, 1022, 10850, 18252, 290, 10766, 18252, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 37573, 18252, 201, 29744, 18252, 201, 36621, 201, 32, 24637, 286, 11666, 4430, 326, 13536, 3341, 284, 2193, 422, 1366, 290, 2987, 511, 2854, 1231, 7952, 8300, 13, 201, 32, 24637, 286, 4572, 4673, 326, 3544, 17019, 7686, 351, 867, 11685, 284, 2746, 3716, 7572, 287, 1588, 40522, 13, 201, 6601, 24422, 201, 49321, 4433, 1342, 1366, 284, 4512, 4981, 6840, 13, 201, 39618, 1588, 6867, 286, 1366, 284, 4620, 1029, 2854, 11, 1690, 18139, 4138, 284, 5242, 286, 8405, 13, 201, 38816, 14044, 201, 37288, 4433, 10107, 3895, 22236, 290, 6356, 284, 5911, 5981, 5128, 3033, 329, 2746, 3047, 13, 201, 38062, 4142, 22974, 3033, 422, 8246, 1366, 11, 18591, 262, 761, 329, 7667, 10107, 3895, 8705, 13, 201, 5377, 11141, 414, 201, 37058, 18599, 16113, 357, 68, 13, 70, 1539, 14174, 20683, 11, 2551, 7150, 8, 326, 389, 4577, 284, 6179, 13, 201, 5167, 3716, 45619, 357, 68, 13, 70, 1539, 8100, 82, 11, 371, 6144, 82, 8, 326, 389, 1690, 1775, 355, 366, 13424, 10559, 1, 2233, 284, 511, 13357, 13, 201, 44357, 3862, 201, 49321, 5443, 3047, 1661, 2233, 284, 18599, 16113, 290, 1342, 1366, 13, 201, 14617, 263, 3047, 1661, 11, 1690, 10616, 3665, 6890, 357, 33346, 82, 8, 284, 5412, 1588, 40522, 290, 3716, 2653, 602, 13, 201, 32273, 201, 5990, 23914, 880, 319, 20793, 1366, 357, 2339, 8893, 8, 475, 743, 6531, 351, 555, 7249, 1522, 1366, 357, 2339, 4263, 290, 2420, 737, 201, 3109, 5276, 82, 379, 7587, 555, 7249, 1522, 1366, 11, 1642, 340, 11080, 329, 8861, 884, 355, 2939, 9465, 11, 3288, 3303, 7587, 11, 290, 4046, 9465, 13, 201, 17633, 48907, 1799, 201, 5841, 1424, 389, 4143, 517, 6179, 540, 11, 5086, 4577, 4547, 286, 703, 5370, 389, 925, 13, 201, 5841, 1424, 389, 1342, 6179, 540, 11, 1642, 340, 2408, 284, 1833, 703, 5370, 389, 10944, 422, 5128, 1366, 13, 201, 27730, 201, 2348, 7727, 907, 2291, 14174, 20683, 11, 2604, 2569, 20683, 11, 2551, 7150, 11, 4738, 17039, 11, 290, 1104, 15879, 8217, 13, 201, 17227, 45619, 2291, 3063, 2122, 282, 17019, 7686, 357, 18474, 82, 828, 42465, 17019, 7686, 357, 49, 6144, 82, 828, 290, 6121, 364, 13, 201, 23416, 38662, 201, 6935, 8807, 973, 287, 5479, 588, 15602, 3341, 11, 7394, 13326, 11, 290, 33344, 23696, 13, 201, 42559, 306, 973, 287, 6190, 5479, 884, 355, 3644, 5761, 11, 4046, 9465, 11, 290, 3288, 3303, 7587, 13, 201, 49865, 37947, 1387, 201, 37887, 4539, 18306, 319, 3210, 32340, 13, 201, 37288, 4433, 16976, 6890, 11, 884, 355, 32516, 393, 309, 5105, 82, 11, 284, 22636, 262, 3047, 286, 2769, 17019, 7686, 13, 201, 201, 198, 201, 198, 32, 3084, 15676, 2890, 262, 19304, 82, 287, 17219, 11, 15568, 11, 290, 1692, 1535, 1262, 9552, 4981, 12, 30297, 10692, 8581, 201, 15878, 201, 31737, 9579, 201, 26950, 529, 201, 47, 35574, 376, 33266, 201, 38077, 37, 727, 416, 10766, 28478, 201, 17320, 15537, 17724, 286, 513, 35, 7532, 8573, 11, 5854, 2890, 2563, 9412, 11, 4369, 4547, 11, 290, 7532, 1486, 13, 201, 37943, 23455, 1222, 7712, 201, 20185, 12, 15808, 2563, 1486, 357, 68, 13, 70, 1539, 33102, 3083, 11, 33, 1734, 29078, 20185, 8, 201, 12832, 7015, 689, 262, 11795, 286, 2563, 5871, 290, 23989, 286, 4683, 5010, 13, 201, 201, 47, 17407, 425, 11422, 1435, 201, 8021, 44667, 2563, 3747, 290, 28152, 878, 8668, 9867, 11, 8868, 3484, 290, 640, 13, 201, 13746, 10179, 14691, 201, 6719, 16005, 9007, 201, 20185, 4284, 12271, 8513, 1366, 284, 35280, 28949, 13820, 1912, 319, 281, 1981, 338, 8513, 7034, 13, 201, 201, 28008, 12, 3846, 25897, 32841, 201, 12621, 862, 1833, 19824, 9573, 290, 262, 2597, 286, 2176, 10812, 287, 10040, 13, 201, 35, 786, 589, 31549, 5958, 201, 20185, 287, 3315, 19560, 357, 68, 13, 70, 1539, 2511, 12371, 11, 40572, 8, 201, 23028, 1158, 9922, 287, 31521, 34615, 884, 355, 31155, 290, 35258, 287, 23824, 11, 26347, 510, 13669, 13, 201, 201, 47, 17407, 425, 23696, 201, 37702, 12271, 1535, 4406, 284, 4331, 4369, 21228, 290, 17085, 357, 68, 13, 70, 1539, 12593, 11, 21134, 10040, 737, 201, 43467, 17344, 13438, 6583, 201, 44, 2305, 10440, 17262, 27785, 201, 20185, 12, 16550, 2903, 27785, 2897, 17218, 656, 7532, 12213, 290, 4369, 22963, 13, 201, 201, 11964, 82, 17219, 201, 34500, 9700, 1366, 422, 2429, 31994, 11, 5915, 31994, 11, 3503, 1539, 284, 1833, 3716, 4369, 11701, 290, 5911, 649, 21546, 6670, 13, 201, 53, 4134, 500, 7712, 201, 35230, 312, 50059, 17724, 201, 20185, 26334, 12319, 5871, 416, 21128, 10900, 9109, 11, 32253, 12319, 2478, 13, 201, 18081, 37484, 1222, 8549, 201, 54, 40816, 3037, 201, 20185, 4284, 12271, 1103, 12, 2435, 1535, 20731, 422, 5806, 2977, 11, 37221, 1903, 4369, 13326, 290, 2614, 1535, 17218, 13, 201, 201, 31709, 1150, 291, 500, 290, 9552, 8537, 42478, 201, 35476, 1817, 5827, 10375, 11, 3769, 25993, 8922, 11, 290, 42699, 6569, 11409, 2594, 13, 201, 44, 2470, 3893, 201, 31837, 3681, 3781, 287, 1919, 2056, 201, 47504, 82, 1903, 5895, 286, 5110, 1535, 2428, 416, 22712, 3303, 7572, 11, 15882, 19376, 9572, 13, 201, 201, 198, 201, 198, 201, 198, 32, 3084, 326, 27430, 262, 749, 2219, 23989, 16113, 973, 287, 2769, 4673, 12, 30297, 10692, 8581, 201, 2348, 42289, 201, 11828, 201, 17227, 36965, 201, 1273, 5374, 3477, 17701, 1153, 2935, 1087, 357, 38475, 35, 8, 201, 4933, 19581, 19590, 706, 1123, 3047, 1672, 2138, 621, 262, 2104, 27039, 11, 1642, 3047, 5443, 13, 201, 21968, 40522, 26, 2829, 4981, 26, 973, 329, 5443, 3047, 11, 996, 40826, 743, 307, 21354, 13, 201, 39234, 12, 33, 963, 17701, 1153, 2935, 1087, 201, 5842, 274, 1402, 37830, 286, 1366, 329, 3463, 5992, 11, 19771, 262, 4034, 286, 26147, 35, 290, 1336, 12, 43501, 5050, 13, 201, 6719, 18186, 287, 749, 2769, 4673, 5479, 329, 8245, 40826, 290, 9332, 13, 201, 29252, 298, 388, 201, 12832, 7015, 689, 31312, 18598, 416, 4375, 257, 13390, 286, 262, 2180, 4296, 284, 262, 1459, 530, 13, 201, 38052, 284, 3368, 1957, 949, 8083, 290, 2866, 510, 40826, 287, 2769, 7686, 13, 201, 29138, 4303, 1773, 201, 48003, 82, 262, 4673, 2494, 416, 27241, 340, 416, 257, 2491, 2811, 286, 262, 7842, 10455, 286, 2274, 3915, 2334, 13, 201, 16979, 934, 329, 42465, 17019, 7686, 357, 49, 6144, 82, 8, 290, 31210, 1366, 26, 922, 329, 1729, 12, 17529, 560, 12493, 13, 201, 23159, 357, 48003, 425, 29278, 10062, 18991, 8, 201, 20575, 1127, 29820, 4303, 1773, 290, 29278, 388, 26, 552, 1769, 29605, 4673, 3965, 329, 1123, 11507, 1262, 717, 290, 1218, 7188, 13, 201, 42559, 306, 973, 287, 2769, 4673, 329, 3049, 11, 6942, 3047, 1973, 867, 8861, 11, 1390, 399, 19930, 290, 5761, 13, 201, 45, 324, 321, 201, 23159, 351, 399, 7834, 709, 12858, 26, 3073, 4058, 290, 46094, 31312, 18598, 16062, 13, 201, 5167, 6942, 621, 7244, 287, 617, 2663, 26, 973, 329, 5443, 40826, 287, 2939, 17923, 290, 399, 19930, 13, 201, 2782, 64, 42731, 201, 48003, 82, 4673, 2494, 329, 1123, 11507, 17033, 1912, 319, 262, 2160, 286, 6754, 44345, 3915, 2334, 13, 201, 5606, 4674, 329, 29877, 1366, 26, 973, 287, 2420, 17923, 11, 15602, 3341, 11, 290, 399, 19930, 8861, 13, 201, 2782, 324, 12514, 201, 23028, 1158, 47395, 42731, 416, 15637, 262, 24106, 286, 1613, 3915, 2334, 284, 257, 5969, 4324, 2546, 13, 201, 12885, 829, 49240, 4673, 3965, 1365, 621, 47395, 42731, 26, 4465, 287, 2939, 9465, 290, 4046, 8861, 13, 201, 43, 12, 29499, 14313, 201, 32, 1218, 12, 2875, 23989, 11862, 326, 5561, 26748, 262, 46305, 666, 17593, 329, 5443, 40826, 13, 201, 18712, 40522, 26, 19337, 7686, 26, 3360, 973, 287, 3734, 12, 28286, 278, 662, 12, 35311, 4981, 13, 201, 4535, 38, 357, 45, 7834, 709, 29805, 515, 17701, 1153, 8, 201, 32, 15304, 286, 29278, 388, 326, 3544, 257, 804, 12, 38204, 9030, 284, 4532, 3915, 2334, 13, 201, 37, 1603, 40826, 3688, 284, 16858, 12858, 26, 8811, 973, 287, 2769, 7686, 13, 201, 201, 198, 201, 198, 201, 198, 32, 9815, 3084, 326, 27430, 262, 2219, 23989, 16113, 287, 2769, 4673, 11, 1863, 351, 511, 18929, 290, 20256, 12, 15167, 2229, 10692, 8581, 201, 2348, 42289, 201, 30611, 782, 9998, 201, 44898, 47556, 201, 1273, 5374, 3477, 17701, 1153, 2935, 1087, 357, 38475, 35, 8, 201, 198, 220, 201, 198, 220, 201, 12, 12549, 5992, 351, 1123, 3047, 1672, 201, 198, 12, 4599, 329, 1588, 40522, 201, 198, 12, 1680, 6654, 1957, 949, 8083, 201, 12, 3334, 24198, 287, 5992, 201, 198, 12, 1737, 47873, 6364, 393, 24969, 378, 611, 4673, 2494, 318, 1165, 1029, 201, 198, 220, 201, 39234, 12, 33, 963, 17701, 1153, 2935, 1087, 201, 198, 220, 201, 198, 220, 201, 12, 8528, 1817, 2866, 290, 40826, 201, 198, 12, 2297, 26873, 24198, 287, 5992, 201, 198, 12, 38996, 621, 1336, 12, 43501, 201, 12, 26848, 8161, 24549, 286, 15458, 2546, 201, 198, 12, 1737, 991, 6531, 351, 1957, 949, 8083, 201, 198, 220, 201, 29252, 298, 388, 201, 198, 220, 201, 198, 220, 201, 12, 2531, 5379, 510, 40826, 201, 198, 12, 2297, 26873, 24969, 341, 201, 198, 12, 5053, 862, 6654, 1957, 949, 8083, 201, 12, 1680, 625, 30408, 949, 8083, 201, 198, 12, 26848, 24549, 286, 12858, 11507, 201, 198, 220, 201, 29138, 4303, 1773, 201, 198, 220, 201, 12, 30019, 425, 4673, 2494, 5419, 351, 15874, 3915, 2334, 201, 198, 12, 29455, 329, 1729, 12, 17529, 560, 15221, 201, 12, 26848, 24549, 286, 22119, 5766, 201, 198, 12, 1680, 307, 8564, 284, 8718, 17143, 2357, 7747, 201, 23159, 357, 48003, 425, 29278, 10062, 18991, 8, 201, 198, 220, 201, 198, 220, 201, 12, 14336, 1127, 13391, 286, 29820, 4303, 1773, 290, 29278, 388, 201, 198, 12, 12549, 40826, 201, 198, 12, 3894, 12, 2385, 863, 329, 1588, 40522, 201, 12, 1680, 307, 17698, 8361, 351, 4673, 3965, 201, 198, 12, 1737, 1085, 284, 850, 8738, 4402, 8136, 287, 617, 2663, 201, 198, 220, 201, 45, 324, 321, 201, 198, 220, 201, 12, 3457, 31150, 689, 399, 7834, 709, 12858, 329, 5443, 40826, 201, 198, 12, 18023, 17706, 1365, 621, 7244, 201, 12, 3125, 3716, 621, 7244, 201, 198, 12, 7831, 4433, 8161, 24549, 286, 8718, 17143, 7307, 201, 2782, 64, 42731, 201, 198, 220, 201, 12, 4599, 329, 29877, 1366, 290, 3033, 201, 198, 12, 30019, 82, 4673, 2494, 1912, 319, 11507, 6817, 201, 12, 18252, 2494, 460, 10070, 1165, 2952, 11, 3756, 284, 19905, 40826, 201, 198, 220, 201, 2782, 324, 12514, 201, 198, 220, 201, 12, 3060, 16746, 4673, 2494, 22119, 2071, 287, 47395, 42731, 201, 198, 12, 9175, 82, 2610, 286, 2274, 3915, 2334, 201, 12, 7831, 4433, 24549, 286, 22119, 10007, 201, 198, 12, 1737, 47873, 6364, 329, 617, 8861, 201, 43, 12, 29499, 14313, 201, 198, 220, 201, 12, 12549, 40826, 351, 7380, 34820, 201, 198, 12, 4599, 329, 1402, 40522, 201, 12, 14059, 12, 38096, 2233, 284, 1218, 12, 2875, 40874, 201, 198, 12, 12892, 11080, 329, 1588, 40522, 201, 4535, 38, 357, 45, 7834, 709, 29805, 515, 17701, 1153, 8, 201, 198, 220, 201, 12, 3125, 21802, 284, 2458, 287, 262, 2994, 10747, 201, 198, 12, 23904, 5443, 40826, 3688, 284, 29278, 388, 201, 12, 3125, 3716, 7822, 201, 198, 12, 26848, 8161, 24549, 286, 10007, 201, 201, 198, 201, 198, 32, 7208, 1022, 4673, 2494, 11, 2994, 2494, 11, 625, 32232, 11, 290, 739, 32232, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 41730, 14806, 201, 43, 793, 14806, 201, 5886, 32232, 201, 9203, 32232, 201, 36621, 201, 464, 4673, 2494, 318, 257, 8718, 17143, 2357, 326, 15947, 262, 2239, 2546, 379, 1123, 24415, 981, 3867, 3812, 257, 5288, 286, 262, 2994, 2163, 13, 201, 464, 2994, 2494, 10229, 284, 262, 4049, 393, 3580, 1022, 262, 11001, 5072, 290, 262, 4036, 5072, 11, 6032, 8630, 416, 257, 2994, 2163, 357, 68, 13, 70, 1539, 337, 5188, 11, 6372, 12, 14539, 28338, 737, 201, 5886, 32232, 8833, 618, 257, 2746, 22974, 262, 3047, 1366, 1165, 880, 11, 1390, 7838, 290, 3307, 11, 3756, 284, 3595, 2276, 1634, 319, 29587, 1366, 13, 201, 9203, 32232, 4325, 618, 257, 2746, 318, 1165, 2829, 290, 10143, 284, 8006, 262, 10238, 7572, 287, 262, 1366, 13, 201, 18610, 319, 9104, 201, 15988, 82, 703, 3049, 262, 2746, 5992, 19590, 13, 317, 1029, 4673, 2494, 460, 2728, 262, 2746, 284, 47873, 1165, 2952, 284, 257, 850, 8738, 4402, 4610, 11, 981, 257, 1877, 4673, 2494, 460, 1085, 284, 3105, 40826, 393, 1972, 7819, 287, 1957, 949, 8083, 13, 201, 32, 1029, 2994, 2494, 9217, 3595, 2746, 2854, 11, 981, 257, 1877, 2994, 2494, 9217, 326, 262, 2746, 338, 16277, 389, 5699, 284, 262, 4036, 3815, 13, 201, 25468, 287, 1029, 9922, 319, 3047, 1366, 475, 3595, 2854, 319, 4856, 1366, 2233, 284, 16181, 1634, 286, 7838, 2138, 621, 4673, 262, 2276, 3912, 13, 201, 20451, 82, 284, 1111, 3595, 3047, 290, 4856, 9922, 780, 262, 2746, 318, 1165, 35010, 284, 8006, 3716, 7572, 13, 201, 24334, 2664, 201, 32, 1029, 4673, 2494, 743, 14267, 262, 16586, 4610, 11, 290, 257, 1877, 4673, 2494, 743, 1011, 1165, 890, 284, 47873, 13, 201, 11922, 2994, 2494, 8833, 2233, 284, 257, 13455, 8776, 2746, 11, 18992, 8718, 17143, 7307, 11, 393, 11491, 1366, 662, 36948, 13, 201, 23307, 3716, 4981, 351, 1165, 867, 10007, 11, 19022, 3218, 1634, 11, 393, 1165, 881, 3047, 640, 13, 201, 23307, 2829, 4981, 351, 19022, 13357, 11, 20577, 3047, 11, 393, 11491, 3033, 13, 201, 43094, 35533, 201, 11922, 4673, 2494, 25, 21354, 3047, 290, 1588, 2994, 24969, 602, 13, 7754, 4673, 2494, 25, 845, 3105, 40826, 13, 201, 11922, 2994, 2494, 25, 1588, 8563, 1022, 16277, 290, 2081, 3815, 13, 7754, 2994, 2494, 25, 16277, 389, 1969, 284, 2081, 3815, 13, 201, 11922, 3047, 9922, 290, 1877, 4856, 9922, 357, 11664, 7625, 737, 201, 20535, 9922, 319, 1111, 3047, 290, 4856, 1366, 357, 17470, 7625, 1022, 606, 737, 201, 47117, 1056, 351, 9104, 15193, 201, 27871, 4402, 4673, 2494, 19047, 6942, 40826, 290, 1365, 2746, 2854, 13, 201, 32, 1877, 2994, 2494, 9217, 922, 2746, 2854, 11, 981, 257, 1029, 2994, 2494, 9217, 3595, 2854, 290, 11491, 16277, 13, 201, 42633, 912, 2276, 1634, 416, 15830, 262, 2746, 1165, 7173, 284, 262, 3047, 1366, 11, 21430, 7838, 13, 201, 7738, 26873, 2854, 355, 262, 2746, 10143, 284, 2193, 1593, 7572, 11, 3756, 284, 1877, 33344, 9922, 13, 201, 6719, 4018, 201, 39668, 278, 262, 4673, 2494, 832, 7605, 588, 4673, 2494, 24025, 11, 29605, 4673, 3965, 11, 393, 10706, 2989, 13, 201, 47531, 2746, 3047, 11, 14009, 8718, 17143, 7307, 11, 779, 1365, 2994, 5499, 11, 290, 2987, 1366, 662, 36948, 13, 201, 11041, 7605, 884, 355, 3272, 12, 12102, 341, 11, 3218, 1634, 357, 43, 16, 14, 43, 17, 828, 4268, 448, 11, 393, 8868, 2746, 13357, 13, 201, 46890, 2746, 13357, 11, 751, 517, 3033, 11, 2987, 3895, 8705, 11, 393, 4512, 329, 517, 36835, 82, 13, 201, 16281, 201, 41730, 2494, 318, 257, 1994, 11507, 287, 31312, 18598, 16113, 973, 287, 2769, 4673, 290, 4572, 4673, 4981, 13, 201, 43, 793, 2494, 318, 8630, 832, 2994, 5499, 884, 355, 22728, 5056, 1144, 13047, 357, 44, 5188, 8, 393, 6372, 12, 14539, 28338, 287, 17923, 8861, 13, 201, 32, 17019, 3127, 351, 1165, 867, 11685, 393, 257, 2551, 5509, 351, 1165, 2769, 13737, 714, 625, 11147, 262, 1366, 13, 201, 32, 14174, 20683, 2746, 2111, 284, 4197, 1729, 29127, 1366, 393, 257, 19337, 2551, 5509, 460, 1085, 284, 739, 32232, 13, 201, 201, 198, 201, 198, 201, 198, 201, 198, 32, 7208, 1022, 40826, 290, 43366, 287, 262, 4732, 286, 9552, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 3103, 332, 12745, 201, 35, 1428, 12745, 201, 36621, 201, 3103, 332, 12745, 10229, 284, 262, 1429, 810, 257, 4572, 4673, 2746, 338, 3047, 14349, 4340, 625, 640, 11, 3616, 262, 2994, 2163, 393, 4049, 20638, 290, 10581, 257, 5288, 1988, 13, 201, 35, 1428, 12745, 10229, 284, 618, 257, 4572, 4673, 2746, 338, 3047, 4329, 21354, 11, 810, 262, 2994, 2163, 393, 4049, 5732, 393, 19180, 12632, 24391, 1231, 14349, 2890, 13, 201, 25267, 15759, 287, 13614, 201, 464, 2746, 338, 2854, 19575, 351, 1123, 24415, 355, 340, 6100, 3812, 262, 16586, 4610, 13, 201, 464, 2746, 10143, 284, 2193, 6840, 11, 7186, 287, 3649, 393, 4047, 24969, 803, 2994, 11, 290, 262, 2746, 6100, 1497, 422, 262, 16586, 4610, 13, 201, 26950, 529, 319, 9104, 201, 20451, 82, 284, 1365, 2276, 1634, 290, 6596, 33344, 9922, 319, 29587, 1366, 13, 201, 36854, 658, 262, 2746, 422, 4673, 11570, 7572, 11, 7186, 287, 3595, 9922, 290, 3595, 2276, 1634, 13, 201, 24334, 2664, 201, 12, 29857, 378, 4673, 2494, 201, 198, 12, 45989, 2746, 10959, 201, 198, 12, 22941, 1366, 662, 36948, 290, 2994, 2163, 201, 12, 18252, 2494, 318, 1165, 1029, 201, 198, 12, 9104, 318, 1165, 3716, 201, 198, 12, 12205, 525, 1366, 662, 36948, 393, 3895, 20796, 201, 198, 12, 3457, 47315, 31312, 18598, 11862, 201, 43094, 35533, 201, 12, 17701, 723, 290, 11831, 10070, 287, 2994, 2163, 201, 198, 12, 24125, 9922, 319, 1111, 3047, 290, 4856, 1366, 201, 198, 12, 47865, 1141, 23989, 201, 12, 38921, 393, 40583, 2994, 2163, 201, 198, 12, 36400, 2313, 9922, 319, 1111, 3047, 290, 4856, 1366, 201, 198, 12, 2262, 1799, 1141, 23989, 357, 35522, 310, 6055, 287, 2994, 8, 201, 47117, 1056, 284, 18252, 14806, 201, 32, 1774, 4673, 2494, 5419, 262, 2746, 47873, 284, 257, 3298, 393, 1957, 5288, 13, 201, 32, 1165, 12, 8929, 4673, 2494, 1690, 5640, 43366, 11, 810, 262, 2746, 625, 1477, 13880, 262, 16586, 4610, 7830, 13, 201, 17633, 338, 10644, 201, 464, 2746, 5679, 257, 3108, 326, 7135, 82, 866, 284, 262, 5288, 286, 262, 2994, 2163, 11, 6840, 45780, 663, 10007, 13, 201, 464, 2746, 338, 3108, 6100, 11454, 4142, 393, 1497, 422, 262, 5288, 286, 262, 2994, 2163, 11, 7186, 287, 3595, 11507, 23989, 13, 201, 6719, 4018, 201, 12, 5765, 281, 5035, 4673, 2494, 357, 4480, 24549, 8, 201, 198, 12, 45989, 1366, 3487, 1634, 290, 3218, 1634, 201, 198, 12, 6372, 12, 12102, 341, 284, 5671, 4371, 201, 12, 44048, 4673, 2494, 201, 198, 12, 45157, 1958, 2746, 10959, 201, 198, 12, 48987, 3376, 1366, 20796, 290, 7587, 201, 16281, 201, 32, 880, 12, 28286, 276, 2769, 17019, 3127, 8776, 1262, 31312, 18598, 326, 7675, 12850, 262, 2994, 290, 41885, 1029, 9922, 13, 201, 32, 2769, 17019, 3127, 8776, 351, 257, 845, 1029, 4673, 2494, 11, 6666, 262, 2746, 338, 2994, 2163, 284, 2620, 22188, 706, 1123, 24415, 13, 201, 201, 198, 201, 198, 201, 198, 32, 3084, 15676, 2890, 262, 4673, 2494, 11, 2994, 2163, 4069, 11, 40826, 11, 290, 43366, 2663, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 20535, 18252, 14806, 357, 3103, 332, 12745, 8, 201, 11922, 18252, 14806, 357, 35, 1428, 12745, 8, 201, 27871, 4402, 18252, 14806, 357, 3103, 332, 12745, 8, 201, 41730, 14806, 201, 16371, 1402, 357, 68, 13, 70, 1539, 657, 13, 8298, 8, 201, 23307, 1029, 357, 68, 13, 70, 1539, 838, 8, 201, 24597, 2903, 357, 68, 13, 70, 1539, 657, 13, 486, 532, 657, 13, 16, 8, 201, 43, 793, 15553, 20181, 201, 43198, 1386, 6364, 625, 867, 34820, 201, 43, 793, 19180, 12632, 393, 5732, 12034, 201, 43198, 1386, 18434, 290, 12229, 257, 5288, 201, 3103, 332, 12745, 201, 36423, 40826, 284, 262, 5288, 11, 8245, 475, 2753, 2392, 201, 2949, 40826, 11, 2746, 10143, 284, 1064, 262, 5288, 11, 21354, 201, 21063, 40826, 351, 8245, 23989, 201, 35, 1428, 12745, 201, 14202, 11, 475, 743, 1011, 13181, 640, 284, 27183, 201, 11922, 11, 2746, 10007, 24969, 378, 393, 625, 30408, 201, 14202, 11, 4673, 318, 12974, 290, 6942, 201, 44357, 22014, 201, 42731, 935, 20638, 475, 460, 38008, 329, 890, 9574, 201, 37, 2290, 310, 12632, 20278, 11, 5732, 393, 14768, 1029, 201, 43198, 1386, 18434, 11, 8978, 1877, 3815, 201, 14402, 22014, 201, 20535, 11, 475, 9025, 318, 3105, 201, 11922, 11, 12739, 3595, 2276, 1634, 201, 20535, 11, 351, 6942, 3047, 290, 922, 2276, 1634, 201, 10707, 1166, 30149, 560, 201, 7556, 5226, 290, 3376, 11, 475, 743, 307, 11038, 201, 9139, 81, 1512, 290, 21873, 11, 2746, 625, 21013, 393, 12312, 3212, 201, 5779, 12, 23211, 290, 16586, 329, 262, 27039, 201, 25844, 20292, 902, 201, 18712, 11, 11831, 16895, 11, 3105, 4371, 201, 21968, 16895, 11, 6666, 24842, 201, 4677, 9219, 378, 11, 12974, 16895, 201, 44357, 20181, 201, 1273, 540, 475, 3105, 4673, 1429, 201, 3118, 31284, 11, 5906, 284, 2193, 6840, 201, 36, 5632, 290, 8245, 4673, 1429, 201, 201, 198, 201, 198, 201, 198, 201, 198, 32, 7208, 286, 262, 749, 2219, 20731, 973, 284, 13446, 17923, 9552, 4981, 12, 30297, 10692, 8581, 201, 9171, 1173, 201, 36621, 201, 11041, 8913, 201, 8479, 4712, 201, 17257, 201, 9492, 5310, 341, 201, 17320, 23843, 201, 464, 9823, 286, 9380, 10090, 10245, 357, 16885, 2081, 38548, 290, 2081, 42510, 8, 503, 286, 262, 2472, 10245, 13, 201, 10248, 329, 12974, 40522, 13, 201, 17320, 23843, 28, 220, 357, 7250, 10, 46559, 20679, 7, 7250, 10, 46559, 10, 5837, 10, 43221, 8, 201, 15, 284, 352, 201, 48708, 3815, 7603, 1365, 2854, 13, 201, 6719, 16005, 201, 464, 9823, 286, 2081, 3967, 16277, 503, 286, 477, 3967, 16277, 13, 45040, 703, 867, 6163, 3709, 389, 5981, 13, 201, 33796, 618, 3991, 38548, 389, 16378, 13, 201, 6719, 16005, 796, 220, 24525, 29006, 7250, 1343, 31459, 8, 201, 15, 284, 352, 201, 11922, 15440, 1724, 7380, 3991, 38548, 13, 201, 6690, 439, 357, 50, 40545, 8, 201, 464, 9823, 286, 4036, 38548, 9380, 10090, 13, 45040, 703, 867, 5981, 3709, 389, 6163, 13, 201, 33796, 618, 3991, 42510, 389, 16378, 13, 201, 6690, 439, 357, 50, 40545, 47505, 220, 24525, 29006, 7250, 10, 43221, 8, 201, 15, 284, 352, 201, 11922, 10014, 1724, 7380, 3991, 42510, 13, 201, 37, 16, 15178, 201, 464, 49239, 1612, 286, 15440, 290, 10014, 13, 47081, 257, 12974, 3953, 618, 6097, 389, 545, 27753, 13, 201, 11041, 913, 287, 545, 27753, 40522, 13, 201, 37, 16, 15178, 28, 17, 2124, 220, 220, 357, 6719, 16005, 2124, 44536, 20679, 7, 6719, 16005, 10, 6690, 439, 8, 201, 15, 284, 352, 201, 48708, 376, 16, 9217, 257, 1365, 5236, 1022, 15440, 290, 10014, 13, 201, 32419, 414, 201, 464, 9823, 286, 2081, 42510, 9380, 10090, 13, 376, 420, 2664, 319, 703, 880, 262, 2746, 21079, 4633, 10245, 13, 201, 41000, 618, 31521, 42510, 318, 1593, 13, 201, 32419, 414, 28, 220, 29025, 29006, 46559, 10, 5837, 8, 201, 15, 284, 352, 201, 11922, 40763, 1724, 7380, 3991, 38548, 13, 201, 201, 198, 201, 198, 201, 198, 32, 7208, 286, 262, 749, 2219, 20731, 973, 284, 13446, 17923, 9552, 4981, 12, 30297, 10692, 8581, 201, 9171, 1173, 201, 36621, 201, 11041, 8913, 201, 8479, 4712, 201, 17257, 201, 9492, 5310, 341, 201, 49, 4503, 12, 32, 9598, 201, 464, 9498, 4698, 262, 39106, 24850, 15684, 2569, 46300, 13, 45040, 262, 2694, 286, 262, 2746, 284, 28433, 1022, 6097, 1973, 2972, 11387, 6460, 13, 201, 11041, 913, 284, 8996, 4981, 7692, 286, 40885, 13, 201, 7, 49, 4503, 12, 32, 9598, 857, 407, 423, 257, 2176, 10451, 329, 1277, 17952, 287, 17923, 11, 475, 340, 5260, 262, 1989, 739, 262, 371, 4503, 12133, 2014, 201, 15, 13, 20, 284, 352, 201, 48708, 317, 9598, 9217, 1365, 8839, 1022, 3967, 290, 4633, 6097, 13, 201, 11187, 22014, 357, 21544, 12, 14539, 28338, 22014, 8, 201, 5308, 13846, 262, 13479, 286, 16277, 416, 14176, 11001, 39522, 351, 4036, 1398, 14722, 13, 201, 33796, 287, 1861, 14991, 2569, 17923, 4981, 13, 201, 11187, 22014, 10779, 220, 352, 14, 45, 220, 5633, 126, 99, 30, 7, 88, 2604, 7, 79, 47762, 30, 220, 357, 16, 12, 88, 8, 6404, 7, 16, 12, 79, 8, 201, 15, 284, 807, 201, 31426, 3815, 7603, 517, 7187, 1861, 14991, 2569, 16277, 13, 201, 24597, 2903, 33222, 201, 32, 17613, 286, 9922, 11, 543, 2753, 262, 2811, 286, 10014, 6492, 319, 1123, 1398, 11, 2592, 4465, 287, 545, 27753, 40522, 13, 201, 13014, 329, 545, 27753, 40522, 13, 201, 3629, 272, 2286, 33222, 28, 220, 357, 6690, 439, 33733, 10, 6690, 439, 36183, 20679, 17, 201, 15, 284, 352, 201, 48708, 12974, 9922, 1724, 1365, 2854, 1973, 477, 6097, 13, 201, 7222, 831, 338, 39553, 201, 32, 18663, 326, 5504, 329, 2863, 4381, 1022, 2081, 290, 11001, 1398, 6637, 13, 201, 11041, 329, 24171, 4381, 3675, 2863, 13, 201, 42, 28, 220, 357, 18833, 12, 6435, 20679, 7, 16, 12, 6435, 8, 201, 12, 16, 284, 352, 201, 32, 1988, 286, 352, 6870, 2818, 4381, 26, 657, 6870, 645, 4381, 3675, 2863, 26, 4633, 3815, 7603, 4785, 12, 14813, 12, 25120, 2854, 13, 201, 201, 198, 201, 198, 201, 198, 32, 7208, 286, 262, 749, 2219, 20731, 973, 284, 13446, 17923, 9552, 4981, 12, 30297, 10692, 8581, 201, 9171, 1173, 201, 36621, 201, 11041, 8913, 201, 8479, 4712, 201, 17257, 201, 9492, 5310, 341, 201, 25372, 82, 2744, 49501, 1766, 16814, 357, 44, 4093, 8, 201, 9444, 4157, 2081, 290, 3991, 38548, 290, 42510, 13, 38984, 290, 30304, 772, 329, 545, 27753, 40522, 13, 201, 11041, 913, 329, 4047, 545, 27753, 40522, 13, 201, 44, 4093, 28, 220, 14808, 7250, 2124, 29025, 13219, 7, 5837, 2124, 44260, 4008, 14, 85, 19510, 7250, 10, 5837, 5769, 7250, 10, 43221, 5769, 46559, 10, 5837, 5769, 46559, 10, 43221, 4008, 201, 12, 16, 284, 352, 201, 16, 9217, 2818, 17724, 26, 657, 318, 645, 1365, 621, 4738, 26, 532, 16, 9217, 2472, 25800, 1022, 17724, 290, 3950, 13, 201, 38, 12, 5308, 272, 201, 464, 38445, 1612, 286, 14233, 357, 8344, 439, 8, 290, 40763, 13, 376, 420, 2664, 319, 1111, 262, 2081, 3967, 290, 2081, 4633, 3965, 13, 201, 33796, 287, 545, 27753, 40522, 13, 201, 38, 12, 11682, 1565, 28, 410, 7, 6690, 439, 2124, 14173, 11365, 8, 201, 15, 284, 352, 201, 32, 2440, 402, 12, 5308, 272, 9217, 1365, 5236, 1022, 13720, 38548, 290, 42510, 13, 201, 37, 17, 15178, 201, 32, 12291, 286, 262, 376, 16, 4776, 11, 3501, 517, 3463, 284, 10014, 621, 15440, 13, 49511, 618, 3991, 42510, 389, 517, 4688, 621, 3991, 38548, 13, 201, 10161, 5902, 4340, 10014, 625, 15440, 13, 201, 37, 17, 28, 30, 7, 16, 10, 17, 19427, 61, 17, 220, 2124, 220, 357, 6719, 16005, 2124, 44536, 20679, 7, 30, 7, 17, 30, 61, 7, 17, 1267, 2124, 39281, 47762, 6690, 439, 8, 201, 15, 284, 352, 201, 32, 2440, 1988, 9217, 257, 1365, 5236, 11, 351, 517, 6817, 319, 10014, 13, 201, 25101, 33733, 14806, 357, 37, 4805, 8, 201, 464, 9823, 286, 4036, 42510, 326, 389, 23175, 10090, 355, 3967, 13, 201, 11041, 913, 618, 3991, 38548, 389, 4047, 38117, 13, 201, 37, 4805, 28, 220, 31459, 29006, 5837, 10, 46559, 8, 201, 15, 284, 352, 201, 31426, 376, 4805, 9217, 7380, 3991, 38548, 13, 201, 25101, 36183, 14806, 357, 37, 24723, 8, 201, 464, 9823, 286, 4036, 38548, 326, 389, 23175, 10090, 355, 4633, 13, 201, 11041, 913, 618, 3991, 42510, 389, 16378, 13, 201, 37, 24723, 796, 220, 44260, 29006, 43221, 10, 7250, 8, 201, 15, 284, 352, 201, 31426, 376, 24723, 9217, 7380, 3991, 42510, 13, 201, 201, 198, 201, 198, 201, 198, 201, 198, 32, 7208, 286, 262, 749, 2219, 20731, 973, 284, 13446, 20683, 357, 39156, 9278, 8, 9552, 4981, 12, 30297, 10692, 8581, 201, 9171, 1173, 201, 36621, 201, 11041, 8913, 201, 8479, 4712, 201, 17257, 201, 9492, 5310, 341, 201, 5308, 272, 36532, 13047, 357, 5673, 36, 8, 201, 464, 2811, 14735, 286, 4112, 8563, 1022, 11001, 290, 4036, 3815, 13, 201, 5606, 4674, 329, 4981, 810, 4049, 14735, 6067, 290, 262, 4571, 286, 8563, 357, 24561, 14, 31591, 8, 318, 407, 1593, 13, 201, 5673, 36, 796, 220, 357, 30, 41052, 72, 28, 16, 8, 61, 77, 126, 99, 30, 91, 88, 41052, 72, 532, 220, 220, 1267, 331, 5299, 41052, 72, 220, 1267, 930, 10091, 14, 77, 201, 58, 15, 11, 807, 8, 201, 31426, 8779, 36, 1724, 262, 2746, 338, 16277, 389, 5699, 284, 262, 4036, 3815, 11, 12739, 1365, 2854, 13, 201, 198, 48708, 8779, 36, 9217, 4025, 2811, 8563, 11, 3616, 262, 2746, 318, 1342, 7187, 13, 201, 5308, 272, 5056, 1144, 13047, 357, 44, 5188, 8, 201, 464, 2811, 286, 262, 44345, 5400, 1022, 11001, 290, 4036, 3815, 11, 3501, 517, 3463, 284, 4025, 8563, 13, 201, 11041, 618, 345, 765, 284, 23634, 1096, 1588, 8563, 517, 7272, 11, 1642, 262, 2746, 2962, 319, 8868, 1263, 10135, 13, 201, 44, 5188, 796, 16, 14, 77, 220, 5633, 41052, 72, 28, 16, 8, 61, 77, 126, 99, 30, 7, 56, 41052, 72, 12, 220, 1267, 575, 5299, 62, 72, 1267, 61, 17, 5633, 201, 58, 15, 11, 807, 8, 201, 31426, 337, 5188, 9217, 1365, 2854, 26, 1588, 8563, 389, 517, 23634, 1143, 2233, 284, 2809, 1723, 13, 201, 30016, 22728, 5056, 1144, 13047, 357, 29138, 5188, 8, 201, 464, 6616, 6808, 286, 337, 5188, 11, 4955, 8563, 287, 262, 976, 4991, 355, 262, 2496, 7885, 13, 201, 11041, 618, 345, 765, 257, 18663, 287, 262, 976, 4326, 355, 262, 11001, 7885, 981, 991, 23634, 2890, 1588, 8563, 13, 201, 29138, 5188, 796, 85, 19510, 30, 41052, 40, 28, 16, 8, 61, 45, 126, 99, 30, 7, 55, 41052, 40, 12, 220, 220, 220, 1267, 357, 55, 62, 40, 1267, 5299, 220, 1267, 61, 17, 5633, 20679, 45, 8, 201, 58, 15, 11, 807, 8, 201, 18925, 284, 337, 5188, 475, 4577, 284, 6179, 1201, 340, 338, 287, 262, 976, 4991, 355, 262, 1366, 13, 201, 49, 12, 16485, 1144, 357, 49, 31185, 8, 201, 464, 9823, 286, 262, 24198, 287, 262, 10795, 7885, 4893, 416, 262, 4795, 9633, 287, 262, 2746, 13, 201, 11041, 284, 1833, 703, 880, 534, 2746, 6688, 262, 25364, 286, 262, 1366, 13, 201, 49, 61, 17, 220, 796, 16, 12, 220, 25012, 14, 4694, 50, 201, 198, 49, 61, 17, 28, 16, 12, 220, 357, 30, 62, 40, 126, 99, 30, 7, 56, 41052, 40, 1267, 12, 357, 56, 62, 72, 1267, 5299, 220, 1267, 61, 17, 5633, 20679, 7, 30, 62, 72, 126, 99, 30, 7, 575, 41052, 40, 1267, 5633, 12, 357, 56, 1267, 5299, 8, 61, 17, 1267, 201, 58, 15, 11, 352, 60, 201, 48708, 371, 31185, 3815, 7603, 326, 262, 2746, 6688, 517, 286, 262, 24198, 287, 262, 2496, 7885, 13, 201, 201, 198, 201, 198, 201, 198, 201, 198, 464, 1388, 3858, 286, 4572, 4673, 532, 30297, 10692, 8581, 201, 6030, 286, 10850, 18252, 201, 36621, 201, 6601, 9394, 24615, 201, 11041, 35536, 201, 26410, 201, 27730, 201, 12442, 16149, 18252, 201, 464, 2746, 318, 8776, 319, 15494, 1366, 11, 810, 262, 5128, 290, 11188, 5072, 357, 16793, 8, 389, 1900, 13, 201, 39618, 1588, 6867, 286, 15494, 1366, 13, 201, 9487, 2649, 357, 68, 13, 70, 1539, 18084, 13326, 828, 20683, 357, 68, 13, 70, 1539, 2156, 2756, 17724, 737, 201, 39156, 14137, 14722, 393, 12948, 3815, 13, 201, 14993, 451, 3310, 2234, 11, 5972, 2569, 3310, 2234, 11, 26423, 34925, 11, 311, 15996, 11, 479, 12, 6144, 201, 3118, 16668, 16149, 18252, 201, 464, 2746, 22974, 7572, 290, 8573, 422, 1366, 326, 468, 645, 15494, 5072, 11, 4917, 7104, 8573, 290, 6958, 13, 201, 13921, 407, 2421, 15494, 1366, 13, 201, 2601, 436, 1586, 357, 68, 13, 70, 1539, 6491, 10618, 341, 828, 32172, 13326, 11, 8112, 3896, 4673, 357, 68, 13, 70, 1539, 1910, 7988, 3781, 737, 201, 33234, 6945, 7572, 11, 2628, 11, 393, 6958, 287, 262, 1366, 13, 201, 42, 12, 5308, 504, 1012, 436, 1586, 11, 4217, 32, 357, 42904, 8521, 35100, 14691, 828, 36496, 998, 605, 1012, 436, 1586, 11, 5231, 6571, 19815, 364, 201, 13900, 72, 12, 12442, 16149, 18252, 201, 464, 2746, 318, 8776, 319, 257, 1402, 2033, 286, 15494, 1366, 5929, 351, 257, 1588, 2033, 286, 9642, 9608, 276, 1366, 11, 42389, 1111, 3858, 329, 4673, 13, 201, 32, 5022, 286, 15494, 290, 9642, 9608, 276, 1366, 13, 201, 13908, 2695, 17923, 11, 2939, 290, 2008, 9465, 11, 3315, 19560, 3781, 13, 201, 39156, 14137, 14722, 393, 555, 1073, 690, 7572, 287, 262, 9642, 9608, 276, 1366, 13, 201, 13900, 72, 12, 12442, 16149, 311, 15996, 11, 12189, 12, 34409, 11, 36052, 8772, 363, 341, 201, 3041, 259, 13442, 18252, 201, 464, 2746, 22974, 832, 4473, 290, 4049, 416, 24986, 351, 281, 2858, 11, 6464, 11530, 393, 12970, 329, 4028, 11, 17272, 284, 20487, 23818, 6721, 625, 640, 13, 201, 2949, 15494, 1366, 11, 7538, 832, 11530, 14, 3617, 10355, 13, 201, 8777, 2712, 357, 68, 13, 70, 1539, 12995, 5247, 828, 36359, 11, 18284, 5672, 11, 15602, 3341, 13, 201, 10707, 1166, 12, 8601, 2450, 393, 4811, 284, 20487, 11530, 13, 201, 48, 12, 41730, 11, 10766, 1195, 12, 7934, 5225, 357, 35, 48, 45, 828, 1041, 87, 4402, 7820, 30011, 1634, 357, 10246, 46, 8, 201, 201, 198, 201, 198, 464, 1388, 3858, 286, 10766, 18252, 45619, 12, 30297, 10692, 8581, 201, 29744, 18252, 5994, 201, 36621, 201, 11041, 35536, 201, 30611, 782, 9998, 201, 27730, 201, 18332, 11813, 47986, 27862, 357, 37, 6144, 8, 201, 464, 24043, 2099, 286, 17019, 3127, 11, 810, 1321, 15623, 287, 530, 4571, 422, 5128, 284, 5072, 13, 201, 5159, 17923, 11, 4046, 9465, 11, 4096, 33344, 8861, 13, 201, 28406, 284, 3494, 11, 2499, 880, 329, 2829, 8861, 13, 201, 29800, 12, 49925, 2448, 984, 1313, 357, 5805, 47, 828, 911, 12154, 27862, 13, 201, 3103, 85, 2122, 282, 47986, 27862, 357, 18474, 8, 201, 8199, 1523, 7686, 3562, 284, 1429, 290, 7564, 7572, 287, 10706, 12, 2339, 1366, 357, 68, 13, 70, 1539, 4263, 828, 1262, 3063, 2122, 282, 11685, 13, 201, 5159, 290, 2008, 9465, 11, 2134, 13326, 11, 3315, 2939, 3781, 13, 201, 33004, 21739, 3895, 13326, 11, 1049, 329, 2939, 12, 3106, 8861, 13, 201, 3123, 7934, 11, 4422, 7934, 11, 569, 11190, 11, 1874, 7934, 11, 554, 4516, 13, 201, 6690, 6657, 47986, 27862, 357, 49, 6144, 8, 201, 7934, 5225, 351, 23607, 5086, 1321, 284, 21160, 11, 15882, 35582, 1366, 7587, 13, 201, 7575, 2168, 17724, 11, 3288, 3303, 7587, 357, 45, 19930, 828, 4046, 13, 201, 12885, 829, 35582, 1366, 290, 21964, 20086, 880, 13, 201, 14617, 10073, 12, 40596, 14059, 357, 43, 2257, 44, 828, 402, 515, 3311, 6657, 27719, 357, 10761, 52, 737, 201, 8645, 876, 1215, 690, 36098, 27862, 357, 45028, 8, 201, 9444, 1023, 286, 734, 17019, 7686, 357, 8612, 1352, 290, 6534, 20900, 8, 11780, 1028, 1123, 584, 284, 7716, 649, 1366, 13, 201, 5159, 5270, 11, 2769, 30706, 6282, 11, 1366, 16339, 14374, 11, 1242, 5270, 13, 201, 6090, 2251, 4047, 12653, 1366, 11, 973, 287, 555, 16668, 16149, 4673, 13, 201, 9697, 45028, 11, 17738, 45028, 11, 26993, 45028, 13, 201, 16541, 6571, 19815, 364, 201, 32, 2099, 286, 17019, 3127, 973, 329, 4673, 6942, 14873, 654, 416, 552, 11697, 290, 31081, 278, 1366, 13, 201, 2025, 24335, 13326, 11, 2939, 7838, 7741, 11, 1366, 19794, 13, 201, 10248, 379, 15793, 1483, 7741, 290, 555, 16668, 16149, 4673, 8861, 13, 201, 23907, 864, 5231, 6571, 66, 12342, 357, 11731, 36, 828, 5601, 78, 1710, 5231, 6571, 66, 12342, 13, 201, 8291, 16354, 27862, 201, 8199, 1523, 7686, 3562, 329, 7587, 35582, 1366, 11, 1262, 2116, 12, 1078, 1463, 11701, 284, 8006, 3298, 20086, 13, 201, 37573, 11059, 11, 2420, 5270, 11, 399, 19930, 8861, 11, 4046, 9465, 13, 201, 36, 5632, 329, 890, 12, 9521, 20086, 11, 10730, 7587, 13, 201, 38, 11571, 11, 347, 17395, 11, 3602, 16354, 11, 309, 20, 11, 19009, 3602, 16354, 357, 38432, 51, 737, 201, 29744, 49728, 27862, 357, 11012, 45, 8, 201, 2964, 65, 14991, 2569, 1152, 876, 4981, 13160, 286, 3294, 11685, 286, 3995, 354, 3477, 11, 41270, 9633, 13, 201, 38816, 22236, 11, 662, 12, 34409, 2769, 7686, 11, 555, 16668, 16149, 4673, 13, 201, 10248, 379, 555, 16668, 16149, 4673, 11, 37895, 38958, 24612, 13, 201, 1273, 6021, 8324, 20941, 21764, 89, 9038, 10850, 357, 49, 12261, 737, 201, 24704, 12, 26121, 2890, 20347, 357, 50, 2662, 8, 201, 32, 2099, 286, 555, 16668, 16149, 17019, 3127, 326, 4493, 1029, 12, 19577, 1366, 4291, 257, 1877, 12, 19577, 10706, 13, 201, 6601, 32704, 11, 32966, 1586, 11, 15793, 1483, 7741, 13, 201, 10248, 329, 1366, 32704, 290, 4547, 286, 3716, 7572, 13, 201, 42, 1219, 34481, 12189, 12, 26121, 2890, 9347, 13, 201, 201, 198, 201, 198, 201, 198, 22093, 286, 8070, 9601, 12, 8210, 287, 9552, 32329, 34959, 3395, 10466, 532, 30297, 10692, 8581, 201, 9171, 1173, 39645, 201, 35965, 12, 2364, 201, 17320, 23843, 3691, 13, 39281, 14, 6690, 439, 201, 17320, 23843, 1244, 307, 15850, 319, 545, 27753, 1366, 26, 15440, 290, 10014, 743, 407, 2620, 1978, 13, 201, 6719, 16005, 3691, 13, 44536, 201, 15562, 2313, 530, 6032, 20638, 262, 584, 11, 6906, 319, 262, 15621, 329, 3991, 38548, 3691, 13, 3991, 42510, 13, 201, 33, 4448, 3691, 13, 15965, 590, 201, 11922, 10690, 5983, 284, 739, 32232, 26, 1029, 24198, 5983, 284, 625, 32232, 13, 8528, 5077, 606, 318, 1994, 329, 2276, 1634, 13, 201, 44, 5188, 3691, 13, 8779, 36, 201, 44, 5188, 23634, 4340, 1588, 8563, 517, 11, 981, 8779, 36, 18432, 477, 8563, 8603, 26, 3572, 8338, 319, 703, 1593, 41528, 3183, 389, 13, 201, 22785, 3691, 13, 33222, 201, 5167, 7187, 4981, 389, 1690, 13611, 290, 517, 8271, 12, 38096, 26, 18599, 4981, 389, 5443, 475, 743, 307, 1342, 7187, 13, 201, 201, 198, 201, 198, 464, 1994, 10838, 286, 3115, 16149, 10850, 18252, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 12442, 16149, 4673, 318, 257, 2099, 286, 4572, 4673, 810, 262, 2746, 318, 8776, 319, 15494, 1366, 357, 15414, 12, 22915, 14729, 737, 201, 49045, 201, 2514, 2193, 257, 16855, 2163, 422, 5128, 3033, 357, 55, 8, 284, 262, 2496, 5072, 357, 56, 8, 290, 787, 7187, 16277, 319, 649, 11, 29587, 1366, 13, 201, 20560, 6060, 201, 33986, 276, 1366, 11, 3616, 1123, 3047, 1672, 10874, 286, 1111, 5128, 1366, 290, 262, 11188, 3376, 5072, 6167, 13, 201, 26410, 201, 39156, 9278, 393, 1398, 6637, 925, 416, 262, 2746, 1912, 319, 262, 5128, 1366, 13, 201, 44357, 10854, 201, 464, 2746, 46094, 663, 5387, 10007, 1912, 319, 262, 15494, 1366, 284, 17775, 262, 4049, 357, 22462, 2163, 8, 1022, 16277, 290, 4036, 14722, 13, 201, 31431, 286, 309, 6791, 201, 16, 13, 40984, 532, 14322, 14137, 28810, 9376, 357, 68, 13, 70, 1539, 18084, 393, 407, 18084, 737, 201, 198, 17, 13, 3310, 2234, 532, 14322, 14137, 12948, 3815, 357, 68, 13, 70, 1539, 2156, 4536, 737, 201, 17227, 978, 7727, 907, 201, 12, 44800, 3310, 2234, 357, 1640, 20683, 8, 201, 198, 12, 5972, 2569, 3310, 2234, 357, 1640, 17923, 8, 201, 198, 12, 7929, 20650, 10850, 357, 50, 15996, 8, 220, 201, 198, 12, 26423, 34925, 201, 198, 12, 14534, 9115, 201, 198, 12, 509, 12, 8199, 12423, 22505, 32289, 357, 42, 6144, 8, 201, 198, 12, 47986, 27862, 201, 43, 793, 15553, 201, 5308, 13846, 262, 3580, 1022, 262, 11001, 5072, 290, 262, 4036, 5072, 13, 8070, 2994, 5499, 2291, 22728, 5056, 1144, 13047, 357, 44, 5188, 8, 329, 20683, 290, 6372, 12, 14539, 28338, 329, 17923, 13, 201, 44357, 18983, 201, 464, 2746, 318, 11672, 15494, 1366, 284, 2193, 422, 13, 383, 3061, 318, 284, 17775, 262, 2994, 2163, 1262, 16113, 588, 31312, 18598, 13, 201, 44154, 18983, 201, 3260, 3047, 11, 262, 2746, 318, 16726, 319, 649, 1366, 357, 9288, 1366, 8, 284, 766, 703, 880, 340, 2276, 4340, 284, 29587, 6096, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 33222, 357, 1640, 17923, 8861, 8, 201, 198, 12, 39281, 290, 44536, 357, 1640, 545, 27753, 17923, 8, 201, 198, 12, 22728, 36532, 13047, 357, 5673, 36, 8, 357, 1640, 20683, 8, 201, 198, 12, 371, 12, 16485, 1144, 357, 1640, 20683, 8, 201, 2782, 4520, 1095, 201, 12, 32329, 460, 307, 4047, 7187, 611, 8776, 319, 6751, 290, 8852, 1366, 13, 201, 198, 12, 47081, 1277, 7538, 416, 14176, 16277, 284, 4036, 14722, 13, 201, 7279, 13461, 1095, 201, 12, 26848, 1588, 6867, 286, 15494, 1366, 11, 543, 460, 307, 5789, 284, 2824, 13, 201, 198, 12, 1737, 407, 2276, 1096, 880, 284, 649, 11, 29587, 1366, 611, 262, 2746, 625, 21013, 13, 201, 201, 198, 201, 198, 201, 198, 464, 1994, 10838, 286, 791, 16668, 16149, 10850, 18252, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 3118, 16668, 16149, 4673, 318, 257, 2099, 286, 4572, 4673, 810, 262, 2746, 318, 8776, 319, 1366, 1231, 7952, 14722, 393, 20865, 13, 201, 49045, 201, 2514, 5911, 7572, 11, 8573, 11, 393, 6958, 1626, 1366, 1231, 2747, 18156, 5072, 14722, 13, 201, 20560, 6060, 201, 3118, 18242, 276, 1366, 11, 3616, 262, 3047, 6096, 3473, 691, 286, 5128, 1366, 1231, 11188, 2496, 14722, 13, 201, 26410, 201, 47546, 82, 11, 23163, 11, 393, 5322, 15225, 11, 4955, 17218, 656, 262, 4645, 286, 262, 1366, 13, 201, 44357, 10854, 201, 464, 2746, 21079, 7104, 7572, 287, 262, 1366, 11, 588, 23163, 393, 15814, 11, 1912, 319, 262, 5128, 3033, 3436, 13, 201, 31431, 286, 309, 6791, 201, 16, 13, 1012, 436, 1586, 532, 4912, 278, 2092, 1366, 2173, 357, 68, 13, 70, 1539, 10618, 278, 4297, 737, 201, 198, 17, 13, 34024, 1483, 33396, 532, 2297, 25648, 262, 1271, 286, 3033, 981, 23934, 1366, 25364, 357, 68, 13, 70, 1539, 4217, 32, 737, 201, 17227, 978, 7727, 907, 201, 12, 509, 12, 5308, 504, 1012, 436, 1586, 201, 198, 12, 36496, 998, 605, 1012, 436, 1586, 201, 198, 12, 360, 4462, 44565, 201, 198, 12, 32641, 35100, 14691, 357, 5662, 32, 8, 201, 198, 12, 256, 12, 50, 12161, 357, 1640, 32704, 8, 201, 198, 12, 5231, 6571, 19815, 364, 201, 43, 793, 15553, 201, 2949, 7952, 2994, 2163, 318, 973, 355, 612, 389, 645, 14722, 11, 475, 4981, 1690, 17775, 617, 5253, 3953, 357, 68, 13, 70, 1539, 48862, 485, 272, 5253, 287, 32966, 1586, 737, 201, 44357, 18983, 201, 464, 2746, 25409, 262, 4645, 286, 262, 5128, 1366, 11, 16924, 340, 1912, 319, 26789, 393, 584, 9695, 11, 1231, 18139, 14722, 13, 201, 44154, 18983, 201, 37058, 11, 555, 16668, 16149, 4981, 389, 407, 16726, 351, 1332, 1366, 11, 475, 262, 3081, 286, 7572, 357, 2339, 13946, 763, 23545, 8, 743, 307, 15276, 1262, 20731, 588, 4243, 15710, 5857, 15178, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 4243, 15710, 5857, 15178, 357, 1640, 32966, 1586, 8, 201, 198, 12, 554, 861, 544, 357, 16345, 286, 44345, 18868, 422, 262, 13946, 3641, 8, 201, 198, 12, 15965, 590, 5905, 1328, 357, 1640, 15793, 1483, 7741, 8, 201, 2782, 4520, 1095, 201, 12, 1400, 761, 329, 15494, 1366, 11, 543, 460, 307, 2408, 393, 5789, 284, 7330, 13, 201, 198, 12, 49511, 329, 13504, 7104, 8573, 290, 13977, 17218, 422, 8246, 1366, 13, 201, 7279, 13461, 1095, 201, 12, 6912, 284, 13446, 1201, 612, 318, 645, 2323, 3872, 329, 7208, 13, 201, 198, 12, 15691, 460, 307, 7069, 284, 6179, 11, 290, 2854, 8338, 319, 262, 2176, 1917, 290, 1366, 13, 201, 201, 198, 201, 198, 201, 198, 464, 1994, 10838, 286, 35525, 12, 12442, 16149, 10850, 18252, 12, 30297, 10692, 8581, 201, 1722, 806, 197, 201, 3109, 11578, 341, 201, 36621, 201, 13900, 72, 12, 16668, 16149, 4673, 21001, 4847, 286, 1111, 28679, 290, 555, 16668, 16149, 4673, 11, 1262, 257, 1402, 2033, 286, 15494, 1366, 290, 257, 1588, 2033, 286, 9642, 9608, 276, 1366, 13, 201, 49045, 201, 2514, 16094, 1111, 15494, 290, 9642, 9608, 276, 1366, 284, 2987, 2746, 2854, 981, 8868, 262, 761, 329, 7667, 15494, 40522, 13, 201, 20560, 6060, 201, 32, 6087, 286, 15494, 1366, 357, 48126, 257, 1402, 24637, 8, 290, 257, 4025, 2033, 286, 9642, 9608, 276, 1366, 13, 201, 26410, 201, 39156, 9278, 393, 1398, 6637, 11, 2092, 284, 28679, 4673, 11, 475, 7981, 416, 1111, 15494, 290, 9642, 9608, 276, 1366, 13, 201, 44357, 10854, 201, 464, 2746, 318, 7317, 8776, 319, 262, 15494, 1366, 11, 788, 17124, 1095, 7572, 287, 262, 9642, 9608, 276, 1366, 284, 35139, 290, 2987, 663, 16277, 13, 201, 31431, 286, 309, 6791, 201, 16, 13, 40984, 532, 49461, 278, 9376, 1262, 257, 1402, 15494, 27039, 351, 3224, 9642, 9608, 276, 6096, 13, 201, 198, 17, 13, 3310, 2234, 532, 49461, 278, 12948, 3815, 416, 42389, 1111, 15494, 290, 9642, 9608, 276, 40522, 13, 201, 17227, 978, 7727, 907, 201, 12, 12189, 12, 34409, 201, 198, 12, 1766, 12, 34409, 201, 198, 12, 3602, 2359, 425, 311, 15996, 357, 4694, 15996, 8, 201, 198, 12, 2980, 876, 32329, 201, 198, 12, 29681, 12, 3106, 25458, 201, 43, 793, 15553, 201, 20575, 1127, 28679, 2994, 357, 68, 13, 70, 1539, 6372, 12, 14539, 28338, 329, 17923, 8, 319, 15494, 1366, 351, 281, 555, 16668, 16149, 7515, 357, 68, 13, 70, 1539, 40709, 10356, 1634, 393, 15794, 3218, 1634, 8, 329, 9642, 9608, 276, 1366, 13, 201, 44357, 18983, 201, 40443, 11, 262, 2746, 22974, 422, 15494, 1366, 13, 3244, 340, 3544, 262, 4645, 287, 262, 9642, 9608, 276, 1366, 284, 2276, 1096, 290, 2987, 2854, 11, 1690, 11629, 9404, 13, 201, 44154, 18983, 201, 7594, 28679, 4673, 11, 262, 2746, 318, 16726, 319, 257, 1332, 900, 11, 3221, 17747, 286, 15494, 1366, 11, 284, 3953, 2276, 1634, 2854, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 33222, 357, 1640, 17923, 8, 201, 198, 12, 39281, 14, 6690, 439, 201, 198, 12, 20410, 22728, 5056, 1144, 13047, 357, 29138, 5188, 8, 357, 1640, 20683, 8, 201, 198, 12, 376, 16, 15178, 201, 2782, 4520, 1095, 201, 12, 2297, 26873, 262, 761, 329, 1588, 6867, 286, 15494, 1366, 13, 201, 198, 12, 1680, 5566, 2987, 2746, 2854, 416, 25137, 1588, 6867, 286, 9642, 9608, 276, 1366, 13, 201, 198, 12, 18023, 517, 8472, 287, 1103, 12, 6894, 13858, 810, 27393, 318, 16378, 393, 640, 12, 35873, 13, 201, 7279, 13461, 1095, 201, 12, 26848, 8161, 24549, 286, 703, 15494, 290, 9642, 9608, 276, 1366, 389, 5929, 13, 201, 198, 12, 791, 18242, 276, 1366, 460, 10400, 7838, 393, 11491, 7572, 611, 407, 12118, 9380, 13, 201, 198, 12, 19157, 414, 5732, 2233, 284, 262, 14554, 3164, 13, 201, 201, 198, 201, 198, 201, 198, 201, 198, 464, 1994, 10838, 286, 22299, 13442, 10850, 18252, 12, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 3041, 259, 13442, 4673, 357, 7836, 8, 318, 257, 2099, 286, 4572, 4673, 810, 281, 5797, 22974, 284, 787, 5370, 416, 24986, 351, 281, 2858, 290, 6464, 11530, 393, 12970, 13, 201, 49045, 201, 2514, 2193, 257, 2450, 326, 12991, 4340, 262, 23818, 6721, 416, 2263, 4028, 326, 1085, 284, 17070, 10906, 625, 640, 13, 201, 20560, 6060, 201, 464, 5797, 11583, 7538, 422, 262, 2858, 287, 262, 1296, 286, 2585, 357, 15603, 278, 262, 1459, 3074, 8, 290, 11530, 1912, 319, 663, 4028, 13, 201, 26410, 201, 32, 8379, 286, 4028, 393, 5370, 8998, 379, 48350, 23818, 11530, 357, 6511, 12, 4354, 1943, 737, 201, 44357, 10854, 201, 464, 5797, 25409, 262, 2858, 290, 22974, 832, 4473, 290, 4049, 13, 632, 5992, 663, 2450, 357, 12501, 1166, 12, 8601, 4811, 8, 1912, 319, 262, 11530, 393, 12970, 2722, 13, 201, 31431, 286, 309, 6791, 201, 16, 13, 2940, 709, 26423, 10854, 274, 357, 44, 6322, 82, 8, 532, 8997, 45715, 8861, 351, 28810, 2585, 290, 4028, 13, 201, 201, 17, 13, 5776, 290, 47061, 532, 383, 5797, 44020, 351, 8925, 12493, 11, 884, 355, 2712, 257, 983, 393, 12755, 257, 9379, 13, 201, 17227, 978, 7727, 907, 201, 12, 1195, 12, 41730, 201, 198, 12, 10766, 1195, 12, 7934, 5225, 357, 35, 48, 45, 8, 201, 198, 12, 7820, 17701, 1153, 25458, 201, 198, 12, 27274, 12, 18559, 291, 978, 7727, 907, 201, 198, 12, 1041, 87, 4402, 7820, 30011, 1634, 357, 10246, 46, 8, 201, 43, 793, 15553, 201, 464, 6721, 6737, 10182, 4673, 13, 383, 3061, 318, 284, 20487, 262, 23818, 6721, 357, 15410, 608, 276, 6721, 625, 640, 828, 1690, 832, 7605, 588, 7459, 805, 27490, 13, 201, 44357, 18983, 201, 464, 5797, 4940, 416, 13504, 262, 2858, 290, 27826, 351, 1180, 4028, 13, 3827, 640, 11, 340, 22974, 284, 1011, 4028, 326, 20487, 11530, 1262, 10064, 588, 17238, 357, 6679, 2752, 1900, 922, 4028, 8, 290, 13936, 357, 83, 14992, 649, 4028, 737, 201, 44154, 18983, 201, 464, 5797, 318, 16726, 1912, 319, 703, 880, 340, 460, 20487, 23818, 11530, 287, 262, 2858, 706, 3047, 13, 27095, 11, 262, 5797, 338, 2854, 318, 8630, 416, 262, 2472, 6721, 340, 10507, 15968, 1141, 4856, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 7472, 32307, 357, 16345, 286, 11530, 625, 640, 8, 201, 198, 12, 35602, 12745, 357, 4919, 2952, 262, 5797, 22974, 8, 201, 198, 12, 36806, 3691, 13, 5905, 78, 3780, 9601, 2364, 357, 20427, 1022, 2111, 649, 4028, 290, 1262, 4499, 4028, 8, 201, 2782, 4520, 1095, 201, 12, 49511, 329, 8925, 290, 3716, 12493, 810, 262, 5797, 22974, 832, 1998, 13, 201, 198, 12, 1680, 4620, 47262, 2854, 287, 8861, 588, 1830, 393, 36359, 13, 201, 198, 12, 8314, 407, 2421, 15494, 1366, 11, 691, 257, 6721, 6737, 13, 201, 7279, 13461, 1095, 201, 12, 13614, 460, 307, 3105, 290, 2653, 15208, 5789, 13, 201, 198, 12, 36806, 460, 1085, 284, 850, 8738, 4402, 5370, 393, 21596, 4028, 287, 1103, 12, 6894, 12493, 13, 201, 198, 12, 26848, 8161, 24549, 286, 6721, 8573, 290, 13936, 10064, 13, 201, 201, 198, 201, 198, 201, 198, 464, 1994, 10838, 286, 18272, 11813, 47986, 27862, 357, 37, 6144, 8, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 32, 18272, 11813, 47986, 7311, 357, 37, 6144, 8, 318, 262, 24043, 2099, 286, 11666, 17019, 3127, 810, 1321, 15623, 287, 530, 4571, 12, 6738, 5128, 13760, 11, 832, 7104, 13760, 357, 361, 597, 828, 284, 5072, 13760, 13, 201, 49045, 201, 2514, 3975, 5128, 1366, 284, 262, 5035, 5072, 1262, 257, 2168, 286, 26356, 8787, 11, 14916, 5499, 11, 290, 4499, 10007, 13, 201, 20560, 6060, 201, 49321, 20793, 1366, 11, 884, 355, 29052, 3033, 11, 17465, 3815, 329, 4263, 11, 393, 1573, 11525, 67, 654, 329, 2420, 13, 201, 26410, 201, 12156, 2412, 319, 262, 4876, 25, 201, 198, 12, 40984, 25, 30873, 5738, 286, 1398, 9931, 13, 201, 198, 12, 3310, 2234, 25, 45012, 3815, 13, 201, 19895, 5712, 495, 201, 7293, 1335, 286, 281, 5128, 7679, 11, 530, 393, 517, 7104, 11685, 11, 290, 281, 5072, 7679, 13, 6188, 6100, 2651, 832, 777, 11685, 13, 201, 44357, 10854, 201, 5842, 274, 736, 22930, 363, 341, 290, 281, 23989, 11862, 357, 2339, 520, 5374, 3477, 17701, 1153, 2935, 1087, 8, 284, 4296, 19590, 1912, 319, 262, 3580, 1022, 11001, 5072, 290, 4036, 5072, 357, 18224, 737, 201, 25526, 341, 40480, 201, 17227, 14916, 5499, 2291, 25, 201, 198, 12, 797, 41596, 357, 45474, 1431, 44800, 11801, 8, 329, 7104, 11685, 13, 201, 198, 12, 311, 17225, 1868, 393, 8297, 9806, 329, 5072, 287, 17923, 8861, 13, 201, 198, 12, 44800, 329, 20683, 8861, 13, 201, 43, 793, 15553, 201, 12, 22728, 5056, 1144, 13047, 357, 44, 5188, 8, 329, 20683, 13, 201, 198, 12, 6372, 12, 14539, 28338, 22014, 329, 17923, 8861, 13, 201, 44357, 18983, 201, 464, 3127, 318, 23224, 351, 4738, 19590, 13, 632, 22974, 832, 3294, 36835, 82, 11, 22000, 19590, 416, 41366, 262, 4049, 319, 262, 3047, 1366, 1262, 736, 22930, 363, 341, 13, 201, 44154, 18983, 201, 7454, 8776, 11, 262, 3127, 318, 16726, 319, 257, 4553, 1332, 27039, 284, 4659, 663, 2694, 284, 2276, 1096, 284, 649, 11, 29587, 1366, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 33222, 357, 1640, 17923, 8861, 737, 201, 198, 12, 22728, 5056, 1144, 13047, 357, 44, 5188, 8, 393, 371, 31185, 357, 1640, 20683, 737, 201, 198, 12, 39281, 11, 44536, 11, 376, 16, 12, 26595, 357, 1640, 2176, 17923, 2476, 737, 201, 2782, 4520, 1095, 201, 12, 17427, 290, 6942, 329, 20793, 1366, 8861, 13, 201, 198, 12, 1680, 27665, 597, 12948, 2163, 1813, 1576, 7104, 4991, 357, 38747, 2034, 13907, 18991, 383, 29625, 737, 201, 7279, 13461, 1095, 201, 12, 1736, 505, 284, 625, 32232, 11, 2592, 351, 1402, 40522, 13, 201, 198, 12, 1892, 11080, 329, 35582, 393, 21739, 1366, 357, 68, 13, 70, 1539, 4263, 11, 640, 12, 25076, 8, 1231, 19008, 588, 8100, 82, 393, 371, 6144, 82, 13, 201, 198, 12, 26003, 8006, 3716, 6958, 287, 1366, 355, 6840, 355, 584, 2769, 4673, 45619, 13, 201, 11041, 35536, 201, 12, 17427, 17923, 8861, 357, 68, 13, 70, 1539, 13934, 17923, 393, 5021, 12, 4871, 17923, 737, 201, 198, 12, 14392, 20683, 8861, 357, 68, 13, 70, 1539, 2156, 2756, 17724, 737, 201, 198, 12, 27018, 22236, 393, 15793, 1483, 7741, 13, 201, 201, 198, 201, 198, 201, 198, 464, 1994, 10838, 286, 34872, 2122, 282, 47986, 27862, 357, 18474, 8, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 3103, 85, 2122, 282, 47986, 27862, 357, 18474, 8, 389, 257, 1398, 286, 2769, 4673, 7686, 3562, 284, 1429, 10706, 12, 2339, 1366, 8573, 884, 355, 4263, 416, 6338, 4673, 21739, 28398, 444, 286, 3033, 13, 201, 49045, 201, 2514, 18306, 7564, 7572, 290, 3033, 287, 1366, 357, 16480, 4263, 8, 416, 1262, 3063, 2122, 282, 11685, 284, 4886, 21739, 8573, 290, 4646, 262, 1271, 286, 10007, 3688, 284, 3938, 5884, 7686, 13, 201, 20560, 6060, 201, 23828, 3093, 2939, 1366, 357, 32515, 50000, 828, 475, 635, 9723, 284, 640, 2168, 11, 2008, 11, 290, 584, 20793, 1366, 13, 201, 26410, 201, 12156, 2412, 319, 262, 4876, 25, 201, 198, 12, 40984, 25, 14322, 14137, 262, 12867, 286, 1123, 1398, 357, 68, 13, 70, 1539, 2939, 17923, 737, 201, 198, 12, 10714, 1634, 14, 11242, 3213, 25, 11440, 6945, 5563, 290, 511, 7064, 1626, 4263, 13, 201, 198, 12, 3310, 2234, 25, 14322, 14137, 12948, 3815, 13, 201, 19895, 5712, 495, 201, 9444, 1023, 286, 262, 1708, 11685, 25, 201, 198, 12, 34872, 2122, 282, 406, 6962, 284, 7925, 3033, 13, 201, 198, 12, 19850, 278, 406, 6962, 329, 866, 19232, 13, 201, 198, 12, 40234, 8113, 276, 406, 6962, 329, 2457, 2551, 12, 8601, 13, 201, 3103, 85, 2122, 282, 406, 6962, 201, 4677, 13508, 16628, 357, 74, 44930, 8, 284, 262, 5128, 1366, 284, 2251, 3895, 8739, 326, 8006, 6393, 3033, 588, 13015, 11, 20028, 11, 290, 7572, 13, 201, 27201, 278, 406, 6962, 201, 7738, 26873, 262, 21739, 15225, 286, 262, 3895, 8739, 357, 28950, 1262, 5436, 19850, 278, 8, 284, 10070, 31350, 3440, 290, 2962, 319, 262, 749, 9208, 3033, 13, 201, 37, 2132, 8113, 276, 406, 6962, 201, 13313, 82, 790, 43164, 422, 262, 2180, 7679, 284, 262, 1306, 11, 6032, 973, 379, 262, 886, 286, 262, 3127, 329, 17923, 393, 20683, 8861, 13, 201, 44357, 10854, 201, 2898, 1328, 1262, 736, 22930, 363, 341, 290, 281, 6436, 7509, 357, 68, 13, 70, 1539, 7244, 393, 26147, 35, 737, 383, 3127, 46094, 19590, 287, 1111, 3063, 2122, 282, 290, 3938, 5884, 11685, 416, 41366, 257, 2994, 2163, 13, 201, 25526, 341, 40480, 201, 12, 797, 41596, 357, 45474, 1431, 44800, 11801, 2599, 8070, 306, 973, 706, 3063, 2122, 282, 11685, 284, 10400, 1729, 12, 29127, 414, 13, 201, 198, 12, 8297, 9806, 25, 16718, 287, 262, 2457, 7679, 329, 17923, 8861, 284, 10385, 2604, 896, 284, 1398, 39522, 13, 201, 43, 793, 15553, 201, 12, 6372, 12, 14539, 28338, 22014, 329, 17923, 8861, 13, 201, 198, 12, 22728, 5056, 1144, 13047, 357, 44, 5188, 8, 329, 20683, 8861, 13, 201, 44357, 18983, 201, 464, 3127, 22974, 16628, 290, 19590, 416, 6427, 1366, 832, 262, 11685, 11, 41366, 262, 4049, 625, 3294, 34820, 357, 538, 5374, 82, 737, 3827, 32232, 318, 1690, 10255, 26963, 351, 4268, 448, 290, 1366, 16339, 14374, 13, 201, 44154, 18983, 201, 3260, 3047, 11, 262, 3127, 318, 16726, 319, 257, 4553, 1332, 27039, 284, 3953, 703, 880, 340, 2276, 4340, 284, 649, 11, 29587, 1366, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 33222, 357, 1640, 17923, 737, 201, 198, 12, 39281, 11, 44536, 11, 376, 16, 12, 26595, 357, 1640, 2134, 13326, 393, 10618, 341, 737, 201, 198, 12, 27853, 52, 357, 9492, 5458, 625, 4479, 8, 357, 1640, 42842, 8861, 737, 201, 2782, 4520, 1095, 201, 12, 17406, 4142, 39382, 1593, 3033, 287, 4263, 357, 3919, 10107, 3895, 8705, 2672, 737, 201, 198, 12, 38254, 6942, 329, 1588, 2939, 40522, 13, 201, 198, 12, 3894, 12, 2385, 863, 329, 2939, 17923, 11, 2134, 13326, 11, 290, 10618, 341, 13, 201, 7279, 13461, 1095, 201, 12, 22476, 15208, 5789, 11, 10616, 1029, 12, 26585, 32516, 13, 201, 198, 12, 26848, 1588, 6867, 286, 15494, 1366, 329, 3047, 13, 201, 198, 12, 8932, 984, 856, 284, 625, 32232, 1231, 1774, 3218, 1634, 13, 201, 11041, 35536, 201, 12, 7412, 40984, 25, 357, 68, 13, 70, 1539, 22650, 5563, 287, 5205, 737, 201, 198, 12, 9515, 46254, 25, 357, 68, 13, 70, 1539, 13720, 3294, 5563, 287, 281, 2939, 737, 201, 198, 12, 7412, 1001, 5154, 341, 25, 357, 68, 13, 70, 1539, 3315, 2939, 3781, 737, 201, 198, 12, 13585, 498, 31517, 653, 11, 7623, 28403, 11, 290, 5231, 38175, 32889, 13, 201, 201, 198, 201, 198, 201, 198, 201, 198, 464, 1994, 10838, 286, 3311, 6657, 47986, 27862, 357, 49, 6144, 8, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 6690, 6657, 47986, 27862, 357, 49, 6144, 8, 389, 257, 1398, 286, 17019, 7686, 3562, 284, 5412, 35582, 1366, 416, 10941, 257, 366, 31673, 1, 286, 2180, 17311, 832, 23607, 1626, 262, 3127, 13, 201, 49045, 201, 2514, 2746, 640, 12, 21186, 393, 35582, 6958, 287, 1366, 11, 884, 355, 25539, 2003, 3815, 1912, 319, 1613, 17311, 393, 4547, 35582, 7572, 13, 201, 20560, 6060, 201, 44015, 1843, 1366, 884, 355, 640, 2168, 11, 2420, 357, 34086, 3007, 828, 4046, 11, 393, 2008, 13431, 13, 201, 26410, 201, 12, 4650, 12, 1462, 12, 3198, 25, 1881, 5072, 329, 262, 2104, 8379, 357, 68, 13, 70, 1539, 15598, 3781, 737, 201, 198, 12, 4650, 12, 1462, 12, 7085, 25, 1881, 5072, 329, 1123, 5128, 287, 262, 8379, 357, 68, 13, 70, 1539, 4572, 11059, 11, 2008, 5739, 17724, 737, 201, 19895, 5712, 495, 201, 9444, 1023, 286, 42465, 11685, 810, 262, 5072, 286, 257, 43164, 318, 11672, 736, 656, 262, 5128, 329, 262, 1306, 2239, 13, 770, 8075, 23607, 326, 1249, 262, 3127, 284, 5529, 1321, 546, 1613, 17311, 13, 201, 6690, 6657, 406, 6962, 201, 10871, 42465, 43164, 468, 8787, 284, 262, 1306, 43164, 355, 880, 355, 7538, 8787, 284, 2346, 11, 5086, 340, 284, 12377, 1321, 422, 2180, 640, 4831, 13, 201, 44357, 10854, 201, 2898, 1328, 1262, 736, 22930, 363, 341, 832, 640, 357, 33, 11571, 51, 828, 810, 262, 31312, 318, 8928, 515, 19528, 832, 262, 8379, 284, 4296, 19590, 13, 201, 25526, 341, 40480, 201, 12, 11818, 71, 25, 18023, 973, 287, 7104, 11685, 284, 10400, 1729, 12, 29127, 414, 13, 201, 198, 12, 311, 17225, 1868, 25, 16718, 287, 262, 5072, 7679, 329, 13934, 17923, 8861, 13, 201, 198, 12, 8297, 9806, 25, 16718, 329, 5021, 12, 4871, 17923, 13, 201, 43, 793, 15553, 201, 12, 6372, 12, 14539, 28338, 22014, 329, 17923, 8861, 13, 201, 201, 12, 22728, 5056, 1144, 13047, 357, 44, 5188, 8, 329, 20683, 8861, 393, 12948, 1366, 16277, 13, 201, 44357, 18983, 201, 464, 3127, 7767, 16311, 286, 1366, 2239, 416, 2239, 11, 19698, 19590, 416, 41366, 262, 2994, 1262, 31312, 18598, 7605, 357, 68, 13, 70, 1539, 26147, 35, 11, 7244, 737, 201, 44154, 18983, 201, 464, 2746, 318, 16726, 319, 29587, 16311, 284, 4659, 703, 880, 340, 2276, 4340, 284, 649, 35582, 1366, 416, 1262, 1613, 1321, 422, 262, 3047, 7108, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 33222, 357, 1640, 17923, 8861, 737, 201, 198, 12, 2448, 11141, 414, 357, 1640, 3303, 4981, 737, 201, 198, 12, 22728, 36532, 13047, 357, 5673, 36, 8, 393, 20410, 22728, 5056, 1144, 13047, 357, 29138, 5188, 8, 357, 1640, 20683, 393, 640, 12, 25076, 41164, 737, 201, 2782, 4520, 1095, 201, 12, 1680, 5412, 7885, 12, 13664, 5128, 16311, 13, 201, 198, 12, 42898, 1213, 2180, 17311, 11, 1642, 340, 11080, 329, 8861, 588, 3303, 21128, 11, 4572, 11059, 11, 290, 640, 12, 25076, 41164, 13, 201, 7279, 13461, 1095, 201, 12, 6656, 3929, 14, 20676, 7656, 31312, 1917, 11, 543, 1838, 340, 1327, 284, 2193, 890, 12, 4354, 20086, 13, 201, 198, 12, 3454, 789, 3047, 2233, 284, 35582, 7587, 13, 201, 198, 12, 1736, 505, 284, 625, 32232, 11, 2592, 319, 1402, 40522, 13, 201, 23907, 1187, 201, 12, 406, 2257, 10128, 357, 14617, 10073, 12, 40596, 14059, 7686, 2599, 39198, 284, 5249, 262, 49047, 31312, 1917, 416, 4673, 890, 12, 4354, 20086, 13, 201, 198, 12, 10863, 5842, 357, 38, 515, 3311, 6657, 27719, 2599, 317, 18599, 2196, 286, 406, 2257, 10128, 351, 7380, 10007, 13, 201, 11041, 35536, 201, 12, 3862, 12, 27996, 4558, 19913, 25, 357, 68, 13, 70, 1539, 4283, 2756, 17724, 11, 6193, 41164, 737, 201, 198, 12, 12068, 15417, 28403, 357, 45, 19930, 2599, 357, 68, 13, 70, 1539, 3303, 11059, 11, 2420, 5270, 737, 201, 198, 12, 24709, 31517, 653, 290, 7623, 28403, 13, 201, 201, 198, 201, 198, 464, 1994, 10838, 286, 2980, 876, 1215, 690, 36098, 27862, 357, 45028, 82, 8, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 8645, 876, 1215, 690, 36098, 27862, 357, 45028, 82, 8, 389, 257, 2099, 286, 2769, 4673, 2746, 13160, 286, 734, 7686, 11, 257, 17301, 290, 257, 6534, 20900, 11, 326, 9320, 351, 1123, 584, 13, 383, 17301, 8075, 649, 1366, 10245, 11, 981, 262, 6534, 20900, 47850, 606, 13, 201, 49045, 201, 464, 3061, 286, 402, 1565, 82, 318, 284, 7716, 649, 11, 12653, 1366, 8405, 357, 68, 13, 70, 1539, 4263, 11, 6597, 11, 393, 2420, 8, 326, 389, 43649, 422, 1103, 1366, 13, 383, 17301, 12031, 284, 9192, 262, 6534, 20900, 11, 981, 262, 6534, 20900, 22974, 284, 15714, 1022, 1103, 290, 8390, 1366, 13, 201, 20560, 6060, 201, 20560, 284, 262, 17301, 318, 6032, 4738, 7838, 357, 15460, 298, 2272, 30104, 828, 981, 262, 6534, 20900, 2753, 1111, 1103, 1366, 357, 6738, 262, 3047, 900, 8, 290, 8390, 1366, 357, 27568, 416, 262, 17301, 737, 201, 26410, 201, 12, 35986, 25235, 25, 317, 1366, 4554, 357, 68, 13, 70, 1539, 281, 2939, 8, 7560, 422, 4738, 7838, 13, 201, 198, 12, 8444, 3036, 20900, 25235, 25, 317, 12867, 4776, 12739, 1771, 262, 5128, 1366, 318, 1103, 393, 8390, 13, 201, 19895, 5712, 495, 201, 45028, 82, 3473, 286, 734, 7686, 25, 201, 198, 12, 35986, 7311, 25, 30667, 284, 2251, 12653, 1366, 416, 25449, 4738, 7838, 656, 11570, 8405, 13, 201, 198, 12, 8444, 3036, 20900, 7311, 25, 30667, 284, 15714, 1022, 1103, 290, 7560, 1366, 11, 4955, 7538, 284, 262, 17301, 13, 201, 44357, 10854, 201, 45028, 82, 389, 8776, 287, 257, 734, 12, 9662, 16907, 36098, 1429, 25, 201, 198, 16, 13, 8444, 3036, 20900, 13614, 25, 383, 6534, 20900, 318, 8776, 284, 9380, 36509, 1103, 3691, 13, 8390, 1366, 13, 201, 198, 17, 13, 35986, 13614, 25, 383, 17301, 318, 8776, 284, 4439, 1366, 326, 37323, 262, 6534, 20900, 13, 5747, 7686, 389, 23392, 11629, 9404, 13, 201, 43, 793, 15553, 201, 12, 45755, 6372, 12, 14539, 28338, 22014, 318, 8811, 973, 329, 1111, 262, 17301, 290, 262, 6534, 20900, 13, 383, 17301, 338, 9432, 318, 284, 20487, 262, 12867, 326, 262, 6534, 20900, 318, 16011, 13, 201, 2782, 690, 36098, 10854, 201, 12, 383, 6534, 20900, 290, 17301, 711, 257, 10356, 897, 983, 25, 201, 198, 12, 383, 17301, 8404, 284, 17775, 262, 6534, 20900, 338, 2694, 284, 9380, 36509, 8390, 8405, 13, 201, 198, 12, 383, 6534, 20900, 8404, 284, 20487, 663, 9922, 287, 38607, 1103, 422, 8390, 1366, 13, 201, 44357, 44495, 201, 12, 10363, 9807, 25, 383, 17301, 743, 2193, 284, 4439, 257, 3614, 4996, 286, 8405, 13, 201, 198, 12, 13614, 24842, 25, 402, 1565, 82, 389, 9389, 284, 4512, 780, 262, 17301, 290, 6534, 20900, 1276, 3520, 12974, 13, 201, 198, 12, 14173, 1800, 284, 8718, 17143, 7307, 290, 4673, 2494, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 554, 4516, 15178, 357, 1797, 2599, 45040, 262, 3081, 290, 9573, 286, 7560, 8405, 13, 201, 198, 12, 4848, 20043, 554, 4516, 34600, 357, 37, 2389, 2599, 16972, 6945, 262, 26789, 1022, 1103, 290, 7560, 1366, 24570, 13, 201, 2782, 4520, 1095, 201, 12, 4476, 540, 286, 15453, 4047, 12653, 1366, 13, 201, 198, 12, 16806, 540, 284, 2972, 8861, 588, 2939, 5270, 11, 3918, 4351, 11, 290, 1366, 16339, 14374, 13, 201, 198, 12, 402, 1565, 82, 460, 2193, 555, 16668, 16149, 422, 9642, 9608, 276, 1366, 13, 201, 7279, 13461, 1095, 201, 12, 10631, 2249, 284, 4512, 290, 1690, 4433, 3734, 12, 28286, 278, 286, 8718, 17143, 7307, 13, 201, 198, 12, 8932, 984, 856, 284, 4235, 9807, 11, 810, 262, 17301, 11073, 3614, 13991, 286, 1366, 13, 201, 198, 12, 22476, 15208, 5789, 11, 1690, 10616, 8904, 4133, 13, 201, 23907, 1187, 201, 12, 6257, 45028, 357, 29744, 34872, 2122, 282, 402, 1565, 2599, 36965, 8100, 82, 329, 1111, 262, 17301, 290, 6534, 20900, 11, 16662, 329, 2939, 5270, 13, 201, 198, 12, 26993, 45028, 25, 16718, 329, 2939, 12, 1462, 12, 9060, 11059, 1231, 20312, 1366, 357, 68, 13, 70, 1539, 23202, 4263, 422, 530, 7386, 284, 1194, 737, 201, 198, 12, 370, 45028, 357, 54, 24929, 5714, 402, 1565, 2599, 12205, 1158, 402, 1565, 10159, 416, 1262, 257, 1180, 2994, 2163, 1912, 319, 370, 24929, 5714, 5253, 13, 201, 11041, 35536, 201, 12, 7412, 16588, 25, 357, 68, 13, 70, 1539, 15453, 12653, 1692, 6698, 11, 1242, 11, 393, 18512, 4263, 737, 201, 198, 12, 17738, 20558, 25, 357, 68, 13, 70, 1539, 11524, 17290, 12186, 284, 5205, 737, 201, 198, 12, 6060, 2447, 14374, 25, 2980, 803, 649, 3047, 8405, 329, 4572, 4673, 8861, 13, 201, 198, 12, 7623, 46690, 11, 8255, 12, 1462, 12, 5159, 26375, 8497, 11, 290, 3115, 12, 4965, 2122, 13, 201, 201, 198, 201, 198, 201, 198, 464, 1994, 10838, 286, 3602, 16354, 47986, 27862, 32329, 357, 51, 6144, 8, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 3109, 11578, 341, 201, 36621, 201, 8291, 16354, 27862, 389, 257, 2769, 4673, 2746, 10959, 326, 16507, 319, 257, 2116, 12, 1078, 1463, 9030, 11, 1642, 606, 4047, 4050, 329, 7587, 35582, 1366, 357, 68, 13, 70, 1539, 2420, 11, 640, 12, 25076, 8, 1231, 10616, 42465, 8787, 13, 201, 49045, 201, 2514, 18306, 2746, 890, 12, 9521, 20086, 287, 35582, 1366, 416, 10759, 319, 262, 749, 5981, 3354, 286, 262, 5128, 832, 262, 779, 286, 2116, 12, 1078, 1463, 13, 39185, 389, 8384, 973, 329, 8861, 588, 4572, 11059, 11, 2420, 5270, 11, 290, 3288, 3303, 4547, 13, 201, 20560, 6060, 201, 49321, 11, 35582, 1366, 884, 355, 2420, 11, 640, 12, 25076, 11, 393, 772, 4263, 357, 4480, 19008, 737, 8255, 1366, 318, 1690, 11241, 1143, 656, 1573, 11525, 67, 654, 357, 68, 13, 70, 1539, 9678, 17, 53, 721, 11, 347, 17395, 11525, 67, 654, 737, 201, 26410, 201, 12, 4650, 12, 1462, 12, 3198, 25, 25235, 889, 257, 2060, 17724, 329, 262, 2104, 8379, 357, 68, 13, 70, 1539, 15598, 17923, 737, 201, 198, 12, 4650, 12, 1462, 12, 7085, 25, 25235, 889, 257, 8379, 357, 68, 13, 70, 1539, 4572, 11059, 11, 2420, 5270, 737, 201, 19895, 5712, 495, 201, 12, 14711, 12342, 12, 10707, 12342, 29778, 25, 3515, 1023, 286, 734, 1388, 3354, 25, 201, 198, 12, 14711, 12342, 25, 10854, 274, 5128, 1366, 290, 18616, 257, 4732, 12, 7527, 10552, 13, 201, 198, 12, 34580, 25, 36965, 428, 10552, 284, 4439, 262, 5072, 8379, 13, 201, 24704, 12, 8086, 1463, 13438, 1042, 201, 10871, 11241, 357, 4775, 11, 5002, 8, 287, 262, 5128, 8379, 460, 366, 1078, 437, 1, 284, 790, 584, 11241, 287, 262, 8379, 284, 8006, 20086, 11, 7692, 286, 511, 5253, 422, 1123, 584, 13, 770, 3578, 262, 2746, 284, 1833, 38356, 6958, 6840, 13, 201, 21604, 1859, 14711, 7656, 201, 6385, 39185, 836, 470, 423, 257, 3170, 12, 259, 4547, 286, 262, 1502, 286, 8379, 357, 2339, 371, 6144, 82, 828, 45203, 21004, 318, 2087, 284, 262, 5128, 11525, 67, 654, 284, 2148, 1321, 546, 262, 1502, 286, 16326, 13, 201, 29800, 12, 13847, 47406, 201, 32, 1994, 3895, 810, 262, 2746, 8991, 262, 3241, 9030, 3294, 1661, 287, 10730, 357, 16600, 828, 5086, 262, 3127, 284, 2962, 319, 1180, 3354, 286, 262, 8379, 11640, 13, 201, 18332, 11813, 406, 6962, 201, 3260, 3241, 318, 5625, 11, 1123, 5072, 422, 262, 3241, 6665, 8318, 832, 3938, 5884, 3745, 11813, 11685, 11, 3940, 416, 3487, 1634, 290, 29598, 8787, 284, 4155, 4050, 4673, 13, 201, 44357, 10854, 201, 41762, 364, 389, 8776, 1262, 736, 22930, 363, 341, 290, 31312, 18598, 357, 28950, 1262, 262, 7244, 6436, 7509, 737, 383, 2994, 2163, 357, 48126, 6372, 12, 14539, 28338, 22014, 8, 318, 49491, 329, 8861, 588, 4572, 11059, 393, 2420, 5270, 13, 201, 43, 793, 15553, 201, 12, 6372, 12, 14539, 28338, 22014, 318, 749, 8811, 973, 329, 3303, 8861, 884, 355, 2420, 5270, 11, 11059, 11, 290, 8379, 17724, 13, 201, 8086, 1463, 34838, 201, 40341, 3241, 6665, 1249, 262, 3127, 284, 2962, 319, 3294, 3354, 286, 262, 5128, 8379, 379, 1752, 13, 1119, 24061, 27464, 16605, 12, 11167, 3241, 287, 10730, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 12, 2448, 11141, 414, 25, 8070, 306, 973, 284, 3953, 703, 880, 262, 2746, 26334, 257, 8379, 286, 2420, 13, 201, 198, 12, 347, 2538, 52, 15178, 25, 16718, 329, 22232, 4572, 11059, 416, 14176, 262, 11001, 11059, 284, 257, 4941, 11059, 13, 201, 2782, 4520, 1095, 201, 12, 1680, 2746, 890, 12, 9521, 20086, 6840, 1231, 262, 11247, 286, 35582, 7587, 357, 2339, 371, 6144, 82, 737, 201, 198, 12, 38996, 284, 4512, 2233, 284, 10730, 1634, 9889, 357, 944, 12, 1078, 1463, 318, 10730, 13821, 737, 201, 198, 12, 38254, 4050, 329, 8861, 7411, 1588, 40522, 588, 399, 19930, 11, 11059, 11, 290, 15676, 1634, 13, 201, 7279, 13461, 1095, 201, 12, 22476, 15208, 5789, 11, 2592, 329, 890, 16311, 2233, 284, 262, 15094, 81, 1512, 13357, 286, 2116, 12, 1078, 1463, 13, 201, 198, 12, 26848, 1588, 6867, 286, 1366, 284, 1620, 880, 357, 3866, 12, 34409, 5419, 24237, 428, 737, 201, 198, 12, 14173, 1800, 284, 8718, 17143, 7307, 588, 262, 1271, 286, 11685, 290, 3241, 6665, 13, 201, 23907, 1187, 201, 12, 347, 17395, 357, 33, 312, 4154, 282, 14711, 12342, 10858, 602, 422, 39185, 2599, 376, 420, 2664, 319, 662, 12, 34409, 1262, 257, 29229, 3303, 2746, 3164, 329, 399, 19930, 8861, 13, 201, 198, 12, 402, 11571, 357, 8645, 876, 3771, 12, 35311, 3602, 16354, 2599, 317, 1152, 876, 2746, 329, 2420, 5270, 13, 201, 198, 12, 309, 20, 357, 8206, 12, 1462, 12, 8206, 20558, 3602, 16354, 2599, 1482, 24040, 477, 399, 19930, 8861, 656, 257, 2420, 12, 1462, 12, 5239, 9355, 13, 201, 198, 12, 19009, 3602, 16354, 357, 38432, 51, 2599, 36965, 6121, 364, 329, 2939, 9465, 8861, 13, 201, 11041, 35536, 201, 12, 10850, 33322, 25, 357, 68, 13, 70, 1539, 34665, 8950, 588, 3594, 284, 4141, 737, 201, 198, 12, 8255, 5060, 3876, 1634, 25, 2980, 803, 35327, 30114, 3166, 286, 890, 4963, 13, 201, 198, 12, 8255, 16588, 25, 357, 68, 13, 70, 1539, 402, 11571, 4981, 973, 329, 15453, 24870, 23549, 393, 3923, 737, 201, 198, 12, 18233, 1052, 2032, 1586, 11, 11352, 3681, 14691, 11, 7412, 31517, 653, 357, 38432, 51, 737, 201, 201, 198, 201, 198, 32, 29270, 3084, 326, 27430, 262, 45619, 3226, 262, 10766, 18252, 9552, 32329, 532, 30297, 10692, 8581, 201, 19895, 5712, 495, 201, 1273, 5620, 290, 406, 6962, 201, 20560, 5994, 201, 26410, 5994, 201, 9218, 17571, 201, 17227, 26622, 201, 18332, 11813, 47986, 27862, 357, 37, 6144, 8, 201, 198, 220, 201, 12, 3082, 1335, 286, 5128, 11, 7104, 11, 290, 5072, 11685, 13, 201, 198, 12, 5501, 7679, 3938, 5884, 284, 262, 1306, 13, 201, 13715, 12, 7857, 30104, 201, 198, 220, 201, 13715, 12, 7857, 30104, 201, 198, 220, 201, 12, 17427, 4645, 11, 645, 7538, 23607, 13, 201, 198, 12, 13144, 341, 5499, 973, 329, 1729, 12, 29127, 414, 13, 201, 9487, 2649, 11, 20683, 11, 4096, 3912, 9465, 13, 201, 198, 220, 201, 3103, 85, 2122, 282, 47986, 27862, 357, 18474, 8, 201, 198, 220, 201, 12, 3082, 1335, 286, 3063, 2122, 282, 11685, 11, 5933, 278, 11685, 11, 290, 3938, 5884, 11685, 13, 201, 198, 12, 34872, 2122, 282, 11685, 779, 16628, 357, 74, 44930, 737, 201, 29398, 357, 17, 35, 1366, 8, 201, 198, 220, 201, 9487, 14722, 11, 3895, 8739, 201, 198, 220, 201, 12, 10714, 41062, 7032, 13, 201, 198, 12, 25139, 2357, 7373, 357, 43775, 389, 4888, 1973, 2272, 737, 201, 5159, 9465, 11, 2134, 13326, 11, 2939, 10618, 341, 13, 201, 198, 220, 201, 6690, 6657, 47986, 27862, 357, 49, 6144, 8, 201, 198, 220, 201, 12, 3082, 1335, 286, 42465, 11685, 810, 8787, 1022, 13760, 1296, 7924, 16006, 13, 201, 198, 12, 29581, 406, 2257, 44, 290, 10863, 52, 17670, 329, 9041, 890, 12, 4354, 20086, 13, 201, 44015, 1843, 1366, 357, 2435, 2168, 11, 2420, 8, 201, 198, 220, 201, 44015, 3007, 393, 640, 12, 25076, 16277, 201, 198, 220, 201, 12, 337, 2913, 1299, 4088, 286, 2180, 17311, 832, 7104, 2585, 13, 201, 198, 12, 1778, 4674, 329, 7885, 12, 13664, 5128, 13, 201, 35364, 3303, 7587, 11, 640, 2168, 41164, 11, 4046, 9465, 13, 201, 198, 220, 201, 8645, 876, 1215, 690, 36098, 27862, 357, 45028, 8, 201, 198, 220, 201, 12, 3082, 1335, 286, 734, 17019, 7686, 25, 17301, 290, 6534, 20900, 13, 201, 198, 12, 2312, 7686, 9320, 1028, 1123, 584, 13, 201, 29531, 7838, 357, 1640, 17301, 8, 201, 198, 220, 201, 8645, 515, 1366, 357, 68, 13, 70, 1539, 4263, 8, 201, 198, 220, 201, 12, 1215, 690, 36098, 3047, 1429, 13, 201, 198, 12, 35986, 8075, 1366, 11, 6534, 20900, 47850, 340, 13, 201, 5159, 5270, 11, 1366, 16339, 14374, 11, 555, 16668, 16149, 4673, 13, 201, 198, 220, 201, 8291, 16354, 27862, 357, 51, 6144, 8, 201, 198, 220, 201, 12, 3082, 1335, 286, 2207, 12342, 290, 875, 12342, 11685, 351, 5021, 12, 2256, 2116, 12, 1078, 1463, 11701, 13, 201, 198, 12, 36965, 45203, 21004, 284, 5529, 8379, 1321, 13, 201, 44015, 3007, 357, 5239, 11, 640, 2168, 8, 201, 198, 220, 201, 44015, 3007, 357, 5239, 5270, 11, 11059, 8, 201, 198, 220, 201, 12, 42945, 7587, 286, 5128, 1366, 13, 201, 198, 12, 47406, 9030, 23007, 38356, 6958, 13, 201, 35364, 3303, 7587, 11, 11059, 11, 2420, 15676, 1634, 13, 201, 198, 220, 201, 201, 198, 32, 15676, 2890, 262, 16113, 290, 7605, 973, 287, 24101, 38, 11571, 2142, 1881, 12, 30297, 10692, 8581, 201, 27313, 201, 2348, 42289, 14, 25574, 2350, 201, 11828, 201, 41762, 364, 201, 24704, 12, 8086, 1463, 13438, 1042, 201, 1135, 394, 82, 262, 6817, 286, 1180, 2456, 287, 257, 6827, 329, 4732, 4547, 13, 201, 220, 201, 29800, 12, 13847, 47406, 201, 34934, 262, 2746, 284, 2962, 319, 2972, 3354, 286, 262, 5128, 11640, 13, 201, 27871, 320, 1634, 978, 7727, 907, 201, 23159, 30011, 7509, 201, 48003, 425, 4673, 2494, 23989, 329, 3463, 5992, 1141, 3047, 13, 201, 220, 201, 1273, 5374, 3477, 17701, 1153, 2935, 1087, 357, 38475, 35, 8, 201, 4933, 19581, 19590, 18703, 453, 1912, 319, 9927, 12, 8664, 2052, 286, 1366, 13, 201, 43, 793, 40480, 201, 21544, 12, 14539, 28338, 22014, 201, 5308, 13846, 262, 3580, 1022, 11001, 39522, 290, 4036, 1398, 14722, 13, 201, 220, 201, 33986, 2439, 1025, 722, 201, 18380, 641, 2496, 14722, 284, 2987, 2276, 1634, 290, 2948, 625, 39745, 13, 201, 44357, 49686, 201, 43260, 18252, 201, 6719, 12, 34409, 319, 257, 1588, 35789, 3940, 416, 3734, 12, 28286, 278, 319, 257, 4833, 27039, 13, 201, 220, 201, 3041, 259, 13442, 18252, 422, 5524, 37774, 357, 7836, 29567, 8, 201, 5842, 274, 1692, 7538, 284, 2987, 2882, 3081, 290, 23082, 13, 201, 40164, 1634, 49686, 201, 26932, 448, 201, 29531, 306, 5621, 257, 6903, 286, 16890, 284, 6632, 1141, 3047, 284, 2948, 625, 32232, 13, 201, 220, 201, 25844, 39087, 201, 46245, 257, 7389, 1912, 319, 262, 14735, 286, 2746, 19590, 284, 1630, 13357, 13, 201, 201, 198, 201, 198, 32, 15676, 2890, 262, 16113, 290, 7605, 973, 287, 24101, 38, 11571, 2142, 4930, 12, 30297, 10692, 8581, 201, 27313, 201, 2348, 42289, 14, 25574, 2350, 201, 11828, 201, 8086, 1463, 13438, 6583, 201, 3351, 3021, 22875, 12, 15667, 47406, 201, 9771, 3129, 689, 3241, 8198, 1912, 319, 262, 16605, 1720, 286, 12405, 290, 1994, 30104, 13, 201, 220, 201, 4550, 1800, 47406, 201, 20575, 1127, 12405, 290, 1994, 30104, 1262, 257, 3745, 11813, 3127, 284, 15284, 3241, 19590, 13, 201, 16305, 11347, 49686, 201, 9126, 12, 74, 3409, 11347, 201, 17563, 82, 262, 1306, 1573, 422, 262, 1353, 479, 749, 17939, 2456, 329, 25364, 13, 201, 220, 201, 9126, 12, 79, 3409, 11347, 357, 45, 14913, 385, 3409, 11347, 8, 201, 22164, 4629, 422, 262, 18197, 900, 286, 2456, 3025, 23818, 12867, 21695, 257, 11387, 357, 79, 737, 201, 32065, 9104, 278, 49686, 201, 16541, 382, 19741, 9104, 278, 201, 39156, 14137, 262, 1306, 1573, 1912, 319, 2180, 2456, 329, 24870, 2420, 5270, 13, 201, 220, 201, 45195, 276, 15417, 9104, 278, 201, 39156, 14137, 4814, 2456, 287, 257, 6827, 284, 9494, 662, 12, 34409, 13, 201, 31567, 6048, 278, 49686, 201, 26449, 13302, 6048, 654, 201, 3103, 24040, 2456, 656, 15715, 30104, 21430, 37865, 26368, 357, 68, 13, 70, 1539, 9678, 17, 53, 721, 11, 2671, 78, 26979, 737, 201, 220, 201, 21604, 1859, 14711, 7656, 201, 46245, 1321, 546, 262, 2292, 286, 2456, 287, 262, 8379, 13, 201, 6601, 2447, 14374, 49686, 201, 8206, 2447, 14374, 201, 8645, 689, 3224, 3047, 6096, 1262, 7605, 588, 6171, 5177, 9014, 393, 736, 12, 41519, 13, 201, 17633, 4307, 40903, 49686, 201, 23812, 2965, 4307, 40903, 201, 8291, 69, 364, 3725, 422, 257, 1588, 2746, 357, 660, 3493, 8, 284, 257, 4833, 2746, 357, 50139, 737, 201, 201, 198, 201, 198, 31431, 286, 32329, 16718, 287, 24101, 38, 11571, 2142, 1881, 532, 30297, 10692, 8581, 201, 17633, 5994, 201, 36621, 201, 9218, 17571, 1220, 26622, 201, 8291, 16354, 9104, 201, 464, 43936, 10959, 326, 34547, 2116, 12, 1078, 1463, 11701, 284, 1429, 16311, 286, 1366, 13, 201, 10044, 29363, 7587, 11, 3241, 11701, 11, 290, 45203, 2207, 375, 654, 13, 201, 27195, 12342, 12, 10707, 12342, 9104, 201, 32, 3602, 16354, 2746, 351, 4553, 2207, 12342, 290, 875, 12342, 6805, 11, 6198, 3562, 329, 8861, 588, 11059, 13, 201, 7003, 24101, 38, 11571, 7525, 3544, 262, 875, 12342, 12, 8807, 2196, 11, 262, 2207, 12342, 12, 12501, 12342, 10959, 318, 8780, 287, 4547, 262, 6954, 286, 39185, 13, 201, 10707, 12342, 12, 10049, 9104, 201, 32, 27009, 3602, 16354, 2746, 326, 3544, 691, 262, 875, 12342, 7515, 11, 543, 318, 3562, 329, 15453, 2420, 1960, 382, 5914, 2280, 13, 201, 30820, 38, 11571, 14051, 7525, 355, 257, 875, 12342, 12, 8807, 2746, 329, 2420, 5270, 13, 201, 34389, 12, 51, 40881, 15417, 32329, 201, 6719, 12, 35311, 3602, 16354, 4981, 326, 423, 587, 2252, 8776, 319, 2176, 40522, 329, 6596, 2854, 319, 7977, 8861, 13, 201, 30820, 38, 11571, 318, 3734, 12, 28286, 276, 319, 3453, 864, 40522, 284, 9494, 663, 2694, 284, 8209, 287, 10721, 13, 201, 21968, 15417, 32329, 357, 3069, 10128, 8, 201, 11922, 12, 42404, 4981, 326, 389, 6007, 286, 4547, 290, 15453, 1692, 12, 2339, 2420, 1912, 319, 5909, 6867, 286, 3047, 1366, 13, 201, 30820, 38, 11571, 14448, 284, 262, 1398, 286, 27140, 10128, 11, 42389, 1588, 40522, 284, 2987, 6562, 1387, 290, 763, 23545, 13, 201, 201, 198, 201, 198, 31431, 286, 32329, 16718, 287, 24101, 38, 11571, 2142, 4930, 532, 30297, 10692, 8581, 201, 17633, 5994, 201, 36621, 201, 9218, 17571, 1220, 26622, 201, 24129, 457, 14044, 201, 25574, 6368, 973, 284, 1486, 290, 27183, 5128, 36454, 284, 44055, 1365, 9109, 422, 3303, 4981, 13, 201, 35476, 1817, 262, 13530, 286, 24101, 38, 11571, 416, 26727, 663, 9109, 832, 7773, 12006, 36454, 13, 201, 3041, 259, 13442, 18252, 422, 5524, 37774, 357, 7836, 29567, 8, 201, 32, 3047, 8173, 810, 4981, 389, 6596, 1912, 319, 7538, 422, 1692, 2985, 11, 1690, 7411, 12759, 393, 9689, 23862, 13, 201, 38052, 287, 24101, 38, 11571, 284, 35139, 663, 9109, 290, 10548, 606, 351, 2836, 9027, 13, 201, 28667, 12, 28512, 11, 20463, 12, 28512, 18252, 201, 4677, 305, 3694, 326, 1249, 262, 2746, 284, 1620, 8861, 351, 1310, 284, 645, 3224, 3047, 1366, 357, 22570, 12, 9442, 8, 393, 351, 257, 1402, 2033, 286, 6096, 357, 32146, 12, 9442, 737, 201, 30820, 38, 11571, 460, 2276, 1096, 1973, 8861, 1231, 7952, 3047, 329, 1123, 530, 13, 201, 20344, 40903, 49686, 201, 46202, 973, 284, 2251, 4833, 11, 517, 6942, 6300, 286, 1588, 4981, 981, 26645, 2854, 13, 201, 6747, 307, 5625, 287, 2003, 34820, 284, 27183, 24101, 38, 11571, 329, 14833, 319, 2972, 9554, 13, 201, 15205, 320, 375, 282, 32329, 357, 29783, 7712, 8, 201, 5841, 1424, 326, 460, 1429, 290, 7716, 3294, 3858, 286, 1366, 357, 68, 13, 70, 1539, 2420, 11, 4263, 11, 6597, 737, 201, 29783, 6300, 286, 24101, 38, 11571, 743, 19386, 43104, 375, 282, 9889, 13, 201, 201, 198, 17227, 15079, 17143, 7307, 287, 5747, 10850, 18252, 290, 10766, 18252, 532, 30297, 10692, 8581, 201, 38197, 17143, 2357, 201, 21947, 201, 41730, 14806, 201, 38052, 287, 1111, 26307, 284, 1630, 262, 2239, 2546, 287, 23989, 16113, 13, 554, 2769, 4673, 11, 340, 1690, 4433, 8161, 24549, 2233, 284, 3716, 45619, 13, 201, 33, 963, 12849, 201, 3041, 14938, 287, 1111, 26307, 26, 287, 2769, 4673, 11, 15458, 2546, 460, 5566, 2928, 3047, 640, 290, 2746, 2854, 13, 201, 13807, 5374, 82, 201, 4677, 13508, 284, 1111, 26, 10229, 284, 262, 1271, 286, 1661, 262, 2104, 27039, 318, 3804, 832, 262, 2746, 1141, 3047, 13, 201, 40164, 1634, 14646, 201, 38052, 287, 1111, 26307, 284, 2948, 625, 32232, 832, 406, 16, 357, 43, 28372, 8, 393, 406, 17, 357, 49, 3130, 8, 3218, 1634, 7605, 13, 201, 27871, 320, 7509, 201, 3041, 14938, 287, 1111, 26, 1180, 23989, 16113, 460, 307, 973, 287, 1111, 4572, 4673, 4981, 290, 17019, 7686, 13, 201, 29252, 298, 388, 201, 23828, 3093, 973, 287, 2769, 4673, 284, 22636, 262, 23989, 1429, 11, 3573, 287, 3995, 354, 3477, 31312, 18598, 357, 38475, 35, 737, 201, 201, 198, 201, 198, 38197, 17143, 7307, 329, 17377, 978, 7727, 907, 2142, 1881, 532, 30297, 10692, 8581, 201, 2348, 42289, 201, 38197, 17143, 2357, 201, 11828, 201, 14993, 451, 3310, 2234, 201, 198, 220, 201, 40164, 1634, 5994, 201, 198, 38077, 201, 6030, 286, 3218, 1634, 5625, 357, 43, 16, 393, 406, 17, 737, 201, 198, 40164, 1634, 4202, 13, 201, 10707, 1166, 34925, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 11518, 36350, 201, 198, 9452, 3409, 2374, 27758, 201, 198, 9452, 3409, 2374, 14697, 201, 198, 11518, 17571, 201, 40541, 6795, 286, 262, 5509, 13, 201, 198, 44046, 1271, 286, 8405, 2672, 284, 6626, 281, 5387, 10139, 13, 201, 198, 44046, 1271, 286, 8405, 2672, 284, 307, 379, 257, 12835, 10139, 13, 201, 198, 15057, 286, 3033, 284, 2074, 618, 2045, 329, 262, 1266, 6626, 13, 201, 29531, 9115, 201, 198, 220, 201, 198, 220, 201, 15057, 286, 10062, 320, 2024, 201, 198, 36476, 26418, 201, 198, 11518, 17571, 201, 15057, 286, 7150, 287, 262, 8222, 13, 201, 198, 15354, 284, 779, 6297, 26418, 8405, 618, 2615, 7150, 13, 201, 198, 15057, 286, 3033, 284, 2074, 379, 1123, 6626, 13, 201, 15514, 20650, 31182, 357, 50, 15996, 8, 201, 198, 220, 201, 198, 220, 201, 34, 201, 198, 42, 7948, 5994, 201, 198, 34777, 2611, 201, 40164, 1634, 11507, 13, 201, 198, 6030, 286, 9720, 973, 357, 29127, 11, 745, 6213, 49070, 11, 17986, 37, 11, 3503, 15729, 201, 198, 42, 7948, 35381, 329, 17986, 37, 11, 7514, 11, 290, 264, 17225, 1868, 50207, 13, 201, 42, 12, 8199, 12423, 22505, 32289, 357, 42, 6144, 8, 201, 198, 220, 201, 15057, 286, 22505, 32289, 357, 74, 8, 201, 198, 45767, 3395, 1173, 201, 15057, 286, 12020, 284, 2074, 329, 17923, 393, 20683, 13, 201, 198, 9171, 1173, 973, 284, 3953, 5253, 357, 36, 36616, 485, 272, 11, 13458, 11, 3503, 15729, 201, 8199, 1523, 27862, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 15057, 286, 406, 6962, 201, 198, 15057, 286, 3169, 333, 684, 201, 198, 25526, 341, 40480, 201, 198, 26932, 448, 14806, 201, 198, 25844, 20768, 1634, 201, 14957, 1271, 286, 7104, 11685, 13, 201, 198, 15057, 286, 16890, 287, 1123, 7679, 13, 201, 198, 24629, 2733, 973, 284, 10400, 1729, 12, 29127, 414, 357, 3041, 41596, 11, 311, 17225, 1868, 11, 11818, 71, 11, 3503, 15729, 201, 198, 37, 7861, 286, 16890, 284, 4268, 1141, 3047, 284, 2948, 625, 32232, 13, 201, 198, 17410, 329, 4238, 2890, 19590, 357, 55, 19492, 11, 679, 11, 3503, 15729, 201, 201, 198, 38197, 17143, 7307, 329, 17377, 978, 7727, 907, 2142, 4930, 532, 30297, 10692, 8581, 201, 2348, 42289, 201, 38197, 17143, 2357, 201, 11828, 201, 42731, 1153, 19835, 278, 31182, 357, 4579, 44, 8, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 15057, 286, 10062, 320, 2024, 201, 198, 41730, 14806, 201, 198, 11518, 36350, 201, 198, 7004, 39873, 201, 15057, 286, 27611, 9539, 13, 201, 198, 8600, 2546, 22085, 496, 973, 284, 2948, 625, 32232, 13, 201, 198, 40541, 6795, 286, 1981, 7150, 13, 201, 198, 37, 7861, 286, 8405, 973, 329, 15830, 1981, 2779, 46184, 13, 201, 55, 4579, 78, 455, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 41730, 14806, 201, 198, 11518, 36350, 201, 198, 7004, 39873, 201, 198, 5216, 39873, 62, 1525, 21048, 201, 15988, 82, 262, 10156, 286, 1123, 5509, 13, 201, 198, 40541, 6795, 286, 257, 5509, 13, 201, 198, 2964, 16864, 286, 8405, 973, 329, 3047, 1123, 5509, 13, 201, 198, 2964, 16864, 286, 3033, 973, 329, 3047, 1123, 5509, 13, 201, 15047, 4579, 44, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 41730, 14806, 201, 198, 33111, 46597, 201, 198, 33, 16406, 376, 7861, 201, 198, 38816, 376, 7861, 201, 18925, 284, 1395, 4579, 78, 455, 13, 201, 198, 40541, 1271, 286, 5667, 287, 530, 5509, 13, 201, 198, 2964, 16864, 286, 1366, 284, 779, 329, 3047, 13, 201, 198, 2964, 16864, 286, 3033, 284, 779, 329, 3047, 13, 201, 2601, 436, 1586, 201, 198, 220, 201, 198, 220, 201, 15057, 286, 1012, 13654, 357, 74, 8, 201, 198, 45767, 3395, 1173, 201, 198, 24243, 1634, 11789, 201, 818, 479, 12, 1326, 504, 32966, 1586, 13, 201, 198, 6030, 286, 5253, 3953, 973, 13, 201, 198, 17410, 284, 41216, 13946, 1247, 305, 2340, 357, 68, 13, 70, 1539, 479, 12, 1326, 504, 4880, 737, 201, 35364, 15417, 28403, 201, 198, 220, 201, 198, 220, 201, 198, 220, 201, 31567, 6048, 278, 34024, 201, 198, 44015, 594, 22313, 201, 198, 26932, 448, 14806, 201, 198, 8086, 1463, 34838, 201, 29271, 3004, 1483, 286, 1573, 11525, 67, 654, 13, 201, 198, 24539, 286, 5128, 16311, 287, 371, 6144, 82, 13, 201, 198, 1890, 3218, 1634, 287, 399, 19930, 4981, 13, 201, 198, 818, 47385, 4981, 11, 1271, 286, 3241, 6665, 13, 201, 6395, 15079, 17143, 7307, 201, 198, 220, 201, 20457, 22025, 2105, 201, 198, 41339, 11140, 393, 14534, 11140, 201, 48362, 284, 2245, 3047, 618, 21201, 2854, 9911, 10068, 13, 201, 198, 46202, 329, 8718, 17143, 2357, 23989, 13, 201, 201, 198, 201, 198, 32, 7208, 1022, 13934, 17923, 290, 5021, 12, 4871, 17923, 287, 2846, 286, 8718, 17143, 7307, 329, 9552, 2746, 10959, 12, 15167, 2229, 10692, 8581, 201, 1722, 806, 201, 33, 3219, 40984, 201, 29800, 12, 9487, 40984, 201, 36621, 201, 19904, 9010, 1398, 4035, 1366, 656, 734, 7310, 9376, 357, 68, 13, 70, 1539, 18084, 3691, 13, 407, 18084, 737, 201, 19904, 9010, 1398, 4035, 1366, 656, 1115, 393, 517, 9376, 357, 68, 13, 70, 1539, 3858, 286, 12734, 737, 201, 26410, 34398, 201, 49321, 10874, 286, 257, 2060, 5072, 43164, 351, 257, 264, 17225, 1868, 14916, 2163, 13, 201, 9444, 1023, 286, 3294, 5072, 16890, 357, 505, 329, 1123, 1398, 8, 351, 257, 2705, 9806, 14916, 2163, 13, 201, 43, 793, 15553, 201, 6935, 8807, 3544, 45755, 6372, 12, 14539, 28338, 22014, 284, 3953, 262, 2746, 338, 2854, 13, 201, 49321, 3544, 327, 2397, 12409, 6372, 12, 14539, 28338, 22014, 329, 5021, 12, 4871, 8861, 393, 1338, 17208, 327, 2397, 12409, 6372, 12, 14539, 28338, 618, 14722, 389, 530, 12, 8940, 30240, 13, 201, 25526, 341, 15553, 201, 5842, 274, 311, 17225, 1868, 2163, 287, 262, 5072, 7679, 284, 4439, 257, 12867, 1022, 657, 290, 352, 13, 201, 5842, 274, 8297, 9806, 2163, 287, 262, 5072, 7679, 284, 4439, 39522, 329, 1123, 1398, 326, 2160, 284, 352, 13, 201, 36, 2100, 2288, 3395, 10466, 201, 6935, 8807, 3544, 9922, 11, 15440, 11, 10014, 11, 376, 16, 12, 26675, 11, 290, 371, 4503, 12, 32, 9598, 329, 12660, 13, 201, 5842, 274, 9922, 11, 15440, 11, 10014, 11, 376, 16, 12, 26675, 357, 20285, 305, 14, 24055, 25694, 828, 290, 10802, 17593, 329, 12660, 13, 201, 6601, 38397, 341, 201, 49321, 4433, 13934, 14722, 357, 15, 393, 352, 8, 287, 262, 27039, 329, 1123, 4554, 13, 201, 39618, 4253, 12409, 14722, 357, 68, 13, 70, 1539, 657, 11, 352, 11, 362, 8, 329, 1123, 4554, 287, 262, 27039, 11, 1690, 11513, 284, 530, 12, 8940, 21004, 13, 201, 44357, 38066, 201, 37288, 34547, 18599, 4981, 290, 743, 47873, 5443, 2233, 284, 7380, 5072, 9376, 13, 201, 6090, 307, 517, 3716, 290, 743, 2421, 517, 3047, 1366, 284, 6840, 2746, 3294, 6097, 13, 201, 40164, 1634, 49686, 201, 17227, 7605, 2291, 406, 16, 14, 43, 17, 3218, 1634, 290, 4268, 448, 284, 2948, 625, 32232, 13, 201, 18925, 3218, 1634, 7605, 389, 5625, 11, 475, 743, 635, 2291, 1398, 22486, 5050, 357, 68, 13, 70, 1539, 1398, 19590, 8, 284, 2209, 545, 6893, 1817, 1871, 6097, 13, 201, 38197, 17143, 2357, 13932, 278, 201, 49321, 9018, 24549, 10007, 884, 355, 4673, 2494, 11, 15458, 2546, 11, 1271, 286, 36835, 82, 11, 290, 4268, 448, 3965, 13, 201, 818, 3090, 284, 262, 10007, 287, 13934, 17923, 11, 743, 2291, 24549, 262, 1271, 286, 16890, 583, 5072, 7679, 11, 4673, 2494, 26925, 11, 290, 10064, 329, 7219, 351, 1398, 32556, 357, 68, 13, 70, 1539, 10753, 321, 11347, 11, 14584, 321, 11347, 737, 201, 17227, 978, 7727, 907, 201, 11187, 2569, 3310, 2234, 11, 7929, 20650, 31182, 357, 50, 15996, 828, 26423, 34925, 11, 290, 47986, 27862, 13, 201, 18380, 9806, 3310, 2234, 11, 26423, 34925, 11, 14534, 4558, 6448, 11, 17701, 1153, 19835, 278, 31182, 11, 290, 47986, 27862, 13, 201, 201, 198, 201, 198, 464, 18069, 28486, 7605, 973, 287, 4572, 4673, 290, 2769, 4673, 532, 30297, 10692, 8581, 201, 28532, 452, 876, 41317, 201, 36621, 14, 11828, 201, 11041, 287, 10850, 18252, 1220, 10766, 18252, 201, 42731, 1153, 2935, 1087, 201, 2025, 23989, 11862, 326, 3544, 262, 31312, 286, 262, 2994, 2163, 284, 4296, 10007, 13, 201, 6935, 8807, 973, 329, 41366, 2994, 5499, 287, 2972, 4981, 11, 1390, 14174, 20683, 290, 17019, 7686, 13, 201, 7282, 22930, 363, 341, 201, 32, 2446, 329, 26019, 262, 31312, 286, 262, 2994, 2163, 351, 2461, 284, 1123, 3463, 287, 262, 17019, 3127, 13, 201, 29508, 1843, 329, 3047, 17019, 7686, 416, 18306, 14492, 3915, 2334, 329, 3463, 5992, 13, 201, 7841, 498, 9626, 452, 2929, 201, 28532, 452, 2929, 286, 1963, 35460, 540, 5499, 351, 2461, 284, 530, 7885, 981, 4769, 1854, 6937, 13, 201, 38052, 287, 23989, 2761, 284, 1833, 703, 5609, 530, 11507, 10975, 262, 5072, 13, 201, 46751, 666, 24936, 201, 32, 17593, 286, 477, 717, 12, 2875, 13027, 28486, 286, 257, 15879, 12, 39728, 2163, 13, 201, 12621, 862, 287, 4547, 262, 14233, 286, 23862, 284, 2458, 287, 17311, 11, 2592, 287, 2769, 4673, 4981, 13, 201, 39, 408, 666, 24936, 201, 32, 6616, 17593, 286, 1218, 12, 2875, 13027, 28486, 286, 257, 16578, 283, 12, 39728, 2163, 13, 201, 38052, 284, 16602, 262, 46171, 1300, 286, 262, 2994, 2163, 11, 37221, 287, 23989, 16113, 588, 17321, 338, 2446, 13, 201, 35491, 14330, 201, 32, 10451, 329, 14492, 262, 27255, 286, 262, 11742, 286, 5499, 13, 201, 24553, 6860, 287, 736, 22930, 363, 341, 284, 24061, 3915, 2334, 286, 28376, 5499, 287, 17019, 7686, 13, 201, 1273, 5374, 3477, 17701, 1153, 2935, 1087, 357, 38475, 35, 8, 201, 32, 15304, 286, 31312, 18598, 326, 3544, 257, 4738, 24637, 286, 1366, 284, 24061, 262, 31312, 13, 201, 36, 5632, 306, 5992, 2746, 10007, 287, 1588, 40522, 11, 8811, 973, 287, 2769, 4673, 13, 201, 48003, 425, 18252, 14806, 25458, 201, 25574, 6368, 588, 7244, 11, 371, 5653, 22930, 326, 4532, 262, 4673, 2494, 1912, 319, 31312, 7869, 13, 201, 35476, 1817, 40826, 2866, 290, 10159, 287, 3047, 17019, 7686, 13, 201, 29907, 7171, 25042, 201, 2025, 40874, 286, 257, 2163, 355, 281, 15541, 2160, 286, 2846, 10488, 422, 262, 2163, 338, 28486, 379, 257, 2060, 966, 13, 201, 12621, 862, 287, 45780, 5499, 416, 5561, 39204, 606, 15726, 11, 4465, 287, 5050, 588, 17321, 338, 2446, 13, 201, 12211, 9626, 452, 876, 6208, 201, 32, 2446, 284, 5004, 262, 1673, 615, 414, 286, 5499, 290, 5911, 1957, 949, 8083, 393, 3509, 8083, 13, 201, 12621, 862, 287, 4547, 262, 23989, 10747, 290, 287, 15427, 517, 13767, 23989, 16113, 13, 201, 201, 198, 201, 198, 201, 198, 464, 1180, 5479, 286, 3644, 5761, 1973, 2972, 11798, 12, 30297, 10692, 8581, 201, 23416, 201, 11828, 201, 19746, 12275, 201, 33234, 6945, 661, 393, 5563, 287, 5205, 284, 16481, 606, 11, 8811, 973, 287, 4590, 6143, 290, 1919, 2056, 5479, 13, 201, 8206, 5683, 7861, 201, 5842, 274, 18480, 2095, 9465, 357, 4503, 49, 8, 284, 7925, 2420, 422, 4263, 329, 2695, 7073, 1799, 290, 3188, 7587, 287, 25810, 1429, 22771, 13, 201, 12512, 12061, 22520, 201, 47504, 82, 290, 8339, 3518, 5563, 287, 1103, 12, 2435, 284, 1295, 7166, 5563, 39659, 287, 257, 3518, 2858, 13, 201, 10262, 1173, 6456, 201, 37702, 12271, 4263, 422, 20372, 11, 15382, 11, 393, 13016, 284, 5671, 3971, 85, 3558, 11, 4886, 36708, 11, 290, 5911, 13833, 27560, 33589, 13, 201, 16541, 38175, 31365, 201, 5842, 274, 1103, 12, 2435, 2134, 11795, 290, 9646, 284, 6431, 1321, 1088, 262, 1097, 11, 15882, 2116, 12, 24255, 5006, 284, 16500, 290, 787, 5370, 13, 201, 18081, 6651, 201, 37702, 12271, 3315, 4263, 357, 68, 13, 70, 1539, 1395, 12, 20477, 11, 17242, 3792, 8, 284, 3342, 7519, 287, 13720, 2761, 290, 1642, 5443, 11, 517, 7187, 40567, 13, 201, 18153, 201, 2898, 4595, 5563, 11, 1938, 11, 290, 5341, 284, 3342, 351, 711, 3781, 290, 4811, 2478, 287, 5701, 2995, 13, 201, 44445, 870, 201, 9069, 6742, 20230, 329, 9262, 11, 1720, 3081, 11, 290, 16846, 15210, 319, 3227, 3951, 1262, 3644, 5761, 13, 201, 4561, 34961, 14691, 201, 33234, 6945, 290, 8339, 262, 3356, 286, 661, 393, 5563, 357, 68, 13, 70, 1539, 5006, 8, 1626, 257, 2272, 329, 21739, 23696, 11, 4465, 287, 2324, 393, 4979, 4542, 13, 201, 32388, 31517, 653, 201, 33234, 6945, 3925, 1912, 319, 511, 16324, 3033, 11, 8811, 973, 329, 2324, 11, 18239, 11, 290, 2614, 1634, 5479, 13, 201, 201, 198, 201, 198, 1722, 806, 201, 8642, 592, 38765, 5382, 201, 5216, 1850, 5382, 201, 10258, 609, 8961, 201, 16, 6518, 357, 47799, 3815, 8, 201, 18, 9619, 357, 7738, 11, 3469, 11, 4518, 532, 25228, 8, 201, 40809, 27068, 201, 10871, 17465, 468, 257, 2060, 1988, 10200, 12245, 357, 15, 12, 13381, 737, 201, 10871, 17465, 468, 1115, 3815, 10200, 25228, 3124, 17509, 871, 357, 15, 12, 13381, 737, 201, 8979, 12849, 201, 18712, 263, 2393, 2546, 1201, 691, 530, 6518, 318, 8574, 13, 201, 43, 32270, 2393, 2546, 2233, 284, 262, 761, 284, 3650, 1115, 9619, 13, 201, 5377, 11141, 414, 201, 8890, 20053, 10552, 11, 4577, 329, 16113, 284, 1429, 13, 201, 5167, 3716, 1366, 2233, 284, 262, 1115, 9619, 11, 10616, 517, 29964, 13, 201, 41995, 201, 5606, 4674, 329, 8861, 810, 3124, 1321, 318, 407, 4688, 357, 68, 13, 70, 1539, 5743, 13326, 11, 3188, 21976, 737, 201, 29508, 1843, 329, 8861, 810, 3124, 318, 1593, 357, 68, 13, 70, 1539, 2134, 9465, 11, 1242, 3781, 737, 201, 30871, 29566, 201, 31426, 4088, 8748, 2233, 284, 2060, 6518, 1366, 13, 201, 48708, 4088, 8748, 2233, 284, 3294, 9619, 357, 18, 87, 517, 621, 1036, 592, 38765, 737, 201, 36259, 6188, 201, 37214, 284, 23787, 286, 12768, 11, 645, 3124, 1321, 13, 201, 14868, 287, 5874, 1321, 11, 1390, 257, 3094, 2837, 286, 7577, 13, 201, 18709, 278, 3862, 201, 37, 1603, 7587, 1201, 612, 338, 691, 530, 6518, 284, 16602, 13, 201, 11122, 789, 7587, 2233, 284, 3294, 9619, 290, 3220, 13357, 13, 201, 40171, 341, 201, 10871, 17465, 6870, 691, 22204, 393, 12245, 13, 201, 10871, 17465, 6870, 3124, 832, 257, 6087, 286, 25228, 17509, 871, 13, 201, 44357, 47986, 27862, 201, 37288, 973, 287, 18599, 4981, 393, 618, 3124, 318, 18046, 13, 201, 38052, 287, 517, 3716, 4981, 810, 3124, 5341, 257, 2383, 2597, 287, 17923, 13, 201, 201, 198, 201, 198, 10258, 11102, 5994, 201, 11828, 201, 1925, 8961, 201, 17227, 5765, 35536, 201, 2782, 4520, 1095, 201, 7279, 13461, 1095, 201, 36982, 357, 7738, 11, 3469, 11, 4518, 8, 201, 6207, 6629, 4263, 1262, 1115, 9619, 25, 2297, 11, 3469, 11, 290, 4518, 13, 29792, 389, 7042, 416, 19771, 1180, 17509, 871, 286, 777, 9619, 13, 201, 18, 357, 7738, 11, 3469, 11, 4518, 8, 201, 6943, 2219, 287, 4875, 11298, 11, 9073, 11, 290, 3992, 4263, 13, 201, 42559, 306, 4855, 11, 19933, 10552, 329, 4875, 4263, 13, 201, 37214, 287, 10200, 1103, 12, 6894, 7577, 10582, 13, 201, 48192, 4339, 357, 7738, 11, 3469, 11, 4518, 11, 12995, 8, 201, 46245, 281, 17130, 6518, 284, 25228, 284, 2380, 13902, 13, 5501, 17465, 468, 281, 45912, 1988, 13, 201, 19, 357, 7738, 11, 3469, 11, 4518, 11, 12995, 8, 201, 38052, 287, 4263, 810, 13902, 318, 1593, 357, 68, 13, 70, 1539, 17149, 737, 201, 15946, 1460, 1104, 329, 13902, 3048, 13, 201, 43, 32270, 2393, 2546, 2233, 284, 262, 3224, 17130, 6518, 13, 201, 24187, 56, 42, 357, 34, 4121, 11, 2944, 29188, 11, 12550, 11, 2619, 8, 201, 32, 34128, 425, 3124, 2746, 973, 287, 13570, 13, 29792, 389, 7042, 416, 34128, 278, 7577, 422, 2330, 357, 20189, 4469, 737, 201, 19, 357, 34, 4121, 11, 2944, 29188, 11, 12550, 11, 2619, 8, 201, 23828, 3093, 973, 287, 3124, 13570, 357, 19726, 15742, 11, 19379, 11, 3503, 15729, 201, 17320, 333, 1286, 6870, 10398, 7577, 13, 201, 3673, 973, 329, 11298, 11, 3614, 1104, 287, 4875, 3341, 13, 201, 8642, 592, 38765, 201, 6207, 6629, 4263, 351, 23787, 286, 12768, 11, 422, 2042, 284, 2330, 13, 632, 691, 5260, 1657, 12245, 13, 201, 16, 357, 46130, 8, 201, 38052, 287, 2042, 12, 392, 12, 11186, 16223, 11, 3188, 21976, 13, 201, 26437, 290, 6942, 329, 8861, 810, 3124, 318, 18046, 13, 201, 2949, 3124, 1321, 11, 3614, 284, 23787, 286, 12768, 13, 201, 44816, 65, 13916, 201, 32, 3124, 2272, 973, 287, 2008, 19794, 13, 575, 6870, 29763, 590, 11, 981, 327, 65, 290, 3864, 2380, 15358, 14149, 357, 8043, 1321, 737, 201, 18, 357, 56, 11, 327, 65, 11, 3864, 8, 201, 6935, 8807, 973, 287, 2008, 21004, 290, 22978, 13, 201, 19117, 283, 689, 22204, 290, 3124, 1366, 11, 6942, 329, 2008, 19794, 13, 201, 3673, 355, 19933, 355, 25228, 329, 1277, 3124, 17512, 13, 201, 48780, 357, 25690, 3698, 6242, 8, 201, 32, 3124, 2272, 326, 31555, 1657, 1108, 357, 43, 8, 422, 15358, 1512, 414, 357, 32, 290, 347, 9619, 737, 201, 18, 357, 43, 11, 317, 11, 347, 8, 201, 38052, 287, 2939, 12857, 11, 3124, 17137, 11, 290, 11202, 3640, 13, 201, 5167, 34953, 935, 8187, 11, 1365, 329, 3124, 16895, 13, 201, 5377, 1996, 15208, 517, 3716, 11, 1342, 19933, 621, 25228, 13, 201, 7998, 33, 14, 7998, 53, 357, 39, 518, 11, 311, 36921, 11, 17558, 1108, 14, 11395, 8, 201, 32, 17327, 521, 8143, 10552, 286, 3124, 11, 10759, 319, 37409, 11, 36275, 11, 290, 22204, 13, 201, 18, 357, 39, 518, 11, 311, 36921, 11, 17558, 1108, 14, 11395, 8, 201, 38052, 287, 13028, 1486, 11, 3124, 16895, 11, 290, 2939, 12857, 13, 201, 44, 1124, 340, 4577, 284, 4532, 37409, 290, 36275, 14799, 286, 22204, 13, 201, 3673, 7306, 329, 2939, 6143, 290, 11478, 13, 201, 7998, 43, 357, 39, 518, 11, 311, 36921, 11, 4401, 1108, 8, 201, 18925, 284, 18070, 53, 475, 6870, 3124, 351, 37409, 11, 36275, 11, 290, 1657, 1108, 13, 201, 18, 357, 39, 518, 11, 311, 36921, 11, 4401, 1108, 8, 201, 38052, 287, 2939, 12857, 3788, 11, 3124, 2298, 364, 13, 201, 5317, 33740, 329, 22000, 1657, 1108, 13869, 422, 37409, 290, 36275, 13, 201, 6090, 4439, 1729, 12, 42105, 2482, 329, 1728, 7577, 13, 201, 201, 198, 201, 198, 201, 198, 201, 198, 32, 3084, 15676, 2890, 262, 1180, 8100, 45619, 532, 30297, 10692, 8581, 201, 18474, 29778, 201, 11828, 201, 9218, 17571, 201, 17227, 5765, 35536, 201, 3123, 7934, 201, 3198, 286, 262, 14555, 8100, 45619, 11, 4166, 416, 575, 1236, 1004, 34, 403, 11, 7525, 329, 16839, 9465, 13, 201, 26437, 10959, 351, 642, 11685, 11, 973, 329, 45916, 16839, 17923, 357, 39764, 8808, 27039, 737, 201, 12885, 15266, 16839, 9465, 11, 2939, 17923, 13, 201, 15309, 7934, 201, 32, 19304, 8100, 10959, 326, 1839, 262, 2321, 7412, 7934, 5449, 11, 3181, 8100, 82, 656, 8661, 3241, 13, 201, 35, 41278, 290, 10595, 621, 1004, 7934, 11, 3544, 797, 41596, 14916, 11, 4268, 448, 11, 290, 1366, 16339, 14374, 284, 4646, 625, 32232, 13, 201, 21968, 12, 9888, 2939, 17923, 357, 5159, 7934, 737, 201, 53, 11190, 201, 29870, 329, 663, 21654, 290, 6795, 11, 351, 2968, 25412, 588, 569, 11190, 1433, 290, 569, 11190, 1129, 13, 201, 5842, 274, 1402, 357, 18, 87, 18, 8, 3063, 2122, 16628, 290, 257, 2769, 10959, 357, 1433, 12, 1129, 11685, 737, 201, 5159, 17923, 11, 2134, 13326, 13, 201, 4965, 7934, 201, 15005, 728, 29598, 8787, 357, 48267, 8787, 8, 284, 8494, 262, 49047, 31312, 1917, 287, 2769, 7686, 13, 201, 4834, 2977, 3047, 286, 845, 2769, 7686, 357, 1120, 10, 11685, 828, 29598, 7021, 787, 736, 22930, 363, 341, 517, 6942, 13, 201, 5159, 17923, 11, 2134, 13326, 11, 10618, 341, 13, 201, 56, 3535, 46, 357, 1639, 5514, 6803, 4874, 8, 201, 32, 1181, 12, 1659, 12, 1169, 12, 433, 1103, 12, 2435, 2134, 13326, 2746, 326, 7767, 262, 2104, 2939, 287, 530, 1208, 13, 201, 22968, 2134, 13326, 11, 18432, 13326, 355, 257, 20683, 1917, 2427, 286, 257, 17923, 1917, 13, 201, 15633, 12, 2435, 2134, 13326, 11, 18284, 5672, 11, 2008, 8452, 13, 201, 201, 198, 201, 198, 464, 4946, 20185, 4527, 29982, 329, 1180, 5761, 4981, 532, 30297, 10692, 8581, 201, 17633, 201, 11828, 201, 45, 4147, 201, 15309, 7934, 201, 22342, 4102, 2746, 326, 1839, 262, 2321, 7412, 7934, 5449, 11, 6768, 9181, 287, 3644, 5761, 13, 201, 2075, 201, 15309, 7934, 357, 3646, 2114, 8, 201, 30556, 10959, 355, 4422, 7934, 11, 475, 8776, 319, 262, 37291, 24760, 27039, 13, 201, 2075, 201, 818, 4516, 410, 16, 357, 5247, 519, 3123, 7934, 8, 201, 7248, 262, 1181, 286, 262, 1242, 287, 7412, 7934, 17923, 287, 1946, 13, 201, 5705, 201, 818, 4516, 410, 16, 357, 3646, 2114, 8, 201, 30556, 10959, 355, 554, 4516, 410, 16, 11, 475, 8776, 319, 262, 37291, 24760, 27039, 13, 201, 5705, 201, 53, 11190, 678, 201, 8890, 20053, 3127, 5495, 287, 1946, 11, 3544, 691, 513, 87, 18, 3063, 14191, 290, 645, 13737, 13, 201, 1983, 201, 818, 4516, 410, 18, 201, 45037, 287, 1853, 11, 281, 6596, 290, 517, 6942, 2196, 286, 262, 554, 4516, 10959, 13, 201, 16799, 201, 818, 4516, 410, 19, 201, 45530, 24415, 286, 262, 554, 4516, 10959, 11, 2716, 287, 1584, 351, 257, 2962, 319, 8187, 414, 13, 201, 24991, 201, 4965, 7934, 410, 17, 2026, 201, 4965, 7934, 15304, 351, 2026, 11685, 11, 3544, 14267, 8787, 284, 1037, 3915, 2334, 47933, 287, 2769, 7686, 13, 201, 3365, 201, 5097, 4061, 1874, 7934, 2026, 410, 15, 201, 46, 6499, 26954, 286, 257, 1874, 7934, 2026, 2746, 8776, 351, 6273, 425, 2994, 13, 201, 3134, 201, 5097, 4061, 1874, 7934, 2026, 201, 23615, 7852, 4061, 1874, 7934, 12, 1120, 10959, 13, 201, 4790, 201, 5097, 4061, 1874, 7934, 8949, 201, 5097, 4061, 2746, 1912, 319, 1874, 7934, 12, 8784, 10959, 13, 201, 23756, 201, 5097, 4061, 1874, 7934, 2026, 604, 87, 201, 3351, 3021, 12, 929, 2196, 286, 7852, 4061, 1874, 7934, 12, 1120, 11, 604, 1661, 4025, 13, 201, 16616, 201, 5097, 4061, 1874, 7934, 2026, 1467, 87, 201, 3351, 3021, 12, 929, 2196, 286, 7852, 4061, 1874, 7934, 12, 1120, 11, 1467, 1661, 4025, 13, 201, 22172, 201, 201, 198, 464, 8100, 45619, 329, 20615, 32329, 12, 7841, 1881, 12, 30297, 10692, 8581, 201, 18474, 29778, 201, 11828, 201, 9218, 17571, 201, 17227, 5765, 35536, 201, 15309, 7934, 201, 22342, 4102, 2746, 326, 1839, 262, 2321, 7412, 7934, 5449, 13, 201, 29744, 10959, 351, 642, 3063, 2122, 282, 11685, 290, 797, 41596, 14916, 13, 201, 5159, 17923, 11, 2134, 13326, 13, 201, 15309, 7934, 357, 3646, 2114, 8, 201, 15309, 7934, 10959, 8776, 319, 262, 37291, 24760, 27039, 329, 3715, 9465, 13, 201, 30556, 4645, 355, 4422, 7934, 475, 23392, 329, 1180, 40522, 13, 201, 36542, 17923, 11, 21739, 4732, 3781, 13, 201, 818, 4516, 410, 16, 357, 5247, 519, 3123, 7934, 8, 201, 7248, 262, 1181, 286, 262, 1242, 287, 7412, 7934, 17923, 287, 1946, 351, 257, 5337, 10959, 13, 201, 18274, 346, 4340, 30839, 13103, 284, 18306, 5412, 3294, 8106, 10620, 13, 201, 5159, 17923, 11, 2134, 13326, 11, 3895, 22236, 13, 201, 818, 4516, 410, 16, 357, 3646, 2114, 8, 201, 818, 4516, 410, 16, 2746, 8776, 319, 262, 37291, 24760, 27039, 329, 3715, 3781, 13, 201, 30556, 10959, 355, 554, 4516, 410, 16, 11, 27571, 329, 3715, 9465, 13, 201, 36542, 17923, 11, 21739, 3781, 13, 201, 53, 11190, 678, 201, 32, 18599, 3127, 5495, 287, 1946, 11, 1900, 329, 663, 8187, 10959, 13, 201, 9444, 1023, 286, 1467, 3063, 2122, 282, 11685, 351, 513, 87, 18, 16628, 290, 3509, 5933, 278, 13, 201, 5159, 17923, 11, 4351, 4673, 13, 201, 818, 4516, 410, 18, 201, 2025, 6596, 2196, 286, 554, 4516, 10959, 11, 2716, 287, 1853, 329, 13105, 2854, 13, 201, 40281, 6795, 290, 9332, 351, 517, 30839, 13103, 13, 201, 5159, 17923, 11, 3734, 12, 2164, 1328, 2134, 13326, 13, 201, 818, 4516, 410, 19, 201, 45530, 24415, 286, 262, 554, 4516, 10959, 11, 10759, 319, 8187, 414, 290, 2854, 13, 201, 6104, 9211, 351, 6190, 30839, 13103, 329, 1365, 3895, 22236, 13, 201, 5159, 17923, 11, 6190, 2134, 13326, 13, 201, 201, 198, 201, 198, 464, 8100, 45619, 329, 20615, 32329, 12, 7841, 4930, 12, 30297, 10692, 8581, 201, 18474, 29778, 201, 11828, 201, 9218, 17571, 201, 17227, 5765, 35536, 201, 4965, 7934, 410, 17, 2026, 201, 4965, 7934, 15304, 326, 3544, 14267, 8787, 284, 7139, 9211, 7686, 1231, 49047, 3915, 2334, 13, 201, 1120, 11685, 351, 29598, 8787, 329, 31312, 5202, 13, 201, 5159, 17923, 11, 2134, 13326, 11, 290, 10618, 341, 13, 201, 5097, 4061, 1874, 7934, 2026, 410, 15, 201, 20457, 26954, 286, 1874, 7934, 2026, 8776, 351, 6273, 425, 2994, 329, 6596, 2854, 287, 4547, 4263, 290, 2420, 13, 201, 818, 10215, 1819, 689, 6273, 425, 4673, 284, 9494, 43104, 375, 282, 4547, 13, 201, 15205, 320, 375, 282, 8861, 11, 2939, 12, 5239, 15814, 13, 201, 5097, 4061, 1874, 7934, 2026, 201, 23615, 7852, 4061, 2746, 1912, 319, 1874, 7934, 12, 1120, 10959, 329, 2939, 290, 2420, 4547, 13, 201, 18925, 284, 262, 1874, 7934, 2026, 10959, 351, 6273, 425, 2994, 3047, 13, 201, 5159, 290, 2420, 17923, 11, 43104, 375, 282, 8861, 13, 201, 5097, 4061, 1874, 7934, 8949, 201, 32, 9211, 2196, 286, 7852, 4061, 2746, 1912, 319, 1874, 7934, 12, 8784, 10959, 329, 1365, 2854, 13, 201, 5167, 11685, 9494, 3895, 22236, 9889, 13, 201, 28809, 2939, 12, 5239, 15814, 11, 43104, 375, 282, 5479, 13, 201, 5097, 4061, 1874, 7934, 2026, 604, 87, 201, 3351, 3021, 12, 929, 2196, 286, 7852, 4061, 1874, 7934, 12, 1120, 11, 3562, 329, 6596, 2854, 287, 4025, 40522, 13, 201, 15137, 1661, 262, 2656, 2746, 2546, 11, 3756, 284, 517, 5339, 329, 4673, 13, 201, 21968, 12, 9888, 2939, 17923, 11, 3716, 8861, 13, 201, 5097, 4061, 1874, 7934, 2026, 1467, 87, 201, 13518, 27464, 12, 929, 2196, 286, 7852, 4061, 1874, 7934, 12, 1120, 11, 48350, 2746, 5339, 290, 4673, 2785, 13, 201, 21447, 7821, 1661, 262, 2656, 2746, 2546, 11, 7306, 329, 7667, 40522, 13, 201, 16371, 1588, 12, 9888, 2939, 290, 2420, 8861, 11, 6190, 5479, 13, 201, 201, 198, 201, 198, 2437, 284, 15284, 262, 1271, 286, 10007, 287, 262, 3063, 2122, 282, 7679, 12, 30297, 10692, 8581, 201, 21950, 201, 9771, 14902, 201, 11395, 201, 3109, 11578, 341, 201, 22417, 2546, 357, 33885, 8, 201, 18, 2124, 513, 201, 24, 201, 10871, 8106, 318, 257, 513, 87, 18, 17593, 11, 523, 860, 3815, 583, 6518, 201, 20560, 9619, 201, 24, 357, 43775, 583, 6518, 8, 2124, 513, 9619, 201, 1983, 201, 464, 5128, 468, 513, 3124, 9619, 357, 36982, 8, 201, 33, 4448, 3381, 201, 16, 583, 8106, 201, 16, 201, 10871, 8106, 468, 530, 10690, 1988, 201, 48944, 583, 8106, 201, 1983, 357, 43775, 8, 1343, 352, 357, 65, 4448, 8, 201, 2078, 201, 14957, 1271, 286, 10007, 329, 530, 8106, 201, 15057, 286, 16628, 201, 2078, 10007, 14, 24455, 2124, 3933, 16628, 201, 48712, 201, 2624, 16628, 11, 1123, 351, 2579, 10007, 201, 201, 198, 201, 198, 49925, 21293, 532, 30297, 220, 10692, 8581, 201, 49925, 201, 30026, 3455, 201, 3103, 85, 17, 35, 357, 2624, 16628, 8, 201, 11627, 974, 82, 1877, 12, 5715, 3033, 422, 262, 5128, 2939, 201, 11518, 27201, 278, 17, 35, 201, 7738, 26873, 21739, 15225, 11, 26645, 1593, 3033, 201, 3103, 85, 17, 35, 357, 2414, 16628, 8, 201, 11627, 974, 82, 517, 3716, 3033, 201, 11518, 27201, 278, 17, 35, 201, 13518, 12850, 262, 2546, 286, 3895, 8739, 201, 3103, 85, 17, 35, 357, 2414, 16628, 8, 201, 11627, 974, 82, 9211, 11, 517, 12531, 3033, 201, 7414, 41769, 201, 7414, 1078, 641, 262, 5072, 284, 257, 352, 35, 15879, 329, 3938, 5884, 11685, 201, 35, 1072, 357, 2414, 16890, 11, 797, 41596, 8, 201, 20575, 1127, 3033, 290, 22974, 262, 17923, 7572, 201, 35, 1072, 357, 940, 16890, 8, 201, 26410, 7679, 326, 26334, 262, 1398, 357, 940, 1744, 6097, 8, 201, 201, 198, 50249, 1653, 1022, 1180, 3858, 286, 6372, 12, 14539, 28338, 22014, 973, 287, 17019, 7686, 532, 30297, 10692, 8581, 201, 6030, 201, 11041, 8913, 201, 20560, 18980, 201, 8479, 4712, 357, 43, 793, 2199, 14902, 8, 201, 9218, 17571, 201, 33, 3219, 6372, 12, 14539, 28338, 201, 198, 220, 201, 198, 220, 201, 33, 3219, 17923, 357, 11545, 6097, 11, 304, 13, 70, 1539, 3797, 3691, 3290, 8, 201, 198, 220, 201, 198, 220, 201, 20560, 25, 530, 12867, 1988, 583, 6291, 357, 22915, 1022, 657, 290, 352, 828, 11188, 284, 1398, 657, 393, 352, 201, 198, 220, 201, 198, 220, 201, 12, 685, 88, 9, 6404, 30, 7, 79, 8, 33747, 16, 12, 88, 27493, 6404, 30, 7, 16, 12, 79, 15437, 201, 12, 16718, 329, 13934, 17923, 13, 201, 198, 12, 12744, 14722, 389, 657, 393, 352, 13, 201, 198, 12, 25235, 14916, 2163, 25, 311, 17225, 1868, 13, 201, 34, 2397, 12409, 6372, 12, 14539, 28338, 201, 198, 220, 201, 29800, 12, 4871, 17923, 357, 12518, 6097, 389, 26519, 8568, 8, 201, 198, 220, 201, 20560, 25, 530, 12, 8940, 30240, 30104, 329, 2496, 14722, 357, 68, 13, 70, 1539, 685, 16, 11, 657, 11, 657, 60, 329, 1398, 657, 8, 201, 198, 220, 201, 532, 5633, 41052, 72, 28, 16, 8, 61, 45, 126, 99, 30, 88, 62, 40, 9, 6404, 3548, 7, 79, 62, 72, 5633, 41349, 220, 220, 201, 198, 3003, 399, 318, 262, 1271, 286, 6097, 201, 12, 16718, 329, 5021, 12, 4871, 17923, 810, 1123, 4554, 14448, 284, 530, 1398, 13, 201, 198, 12, 8297, 9806, 5072, 7679, 13, 201, 50, 29572, 327, 2397, 12409, 6372, 12, 14539, 28338, 201, 198, 220, 201, 29800, 12, 4871, 17923, 351, 29877, 18253, 14722, 201, 198, 220, 201, 20560, 25, 18253, 6167, 357, 68, 13, 70, 1539, 657, 329, 1398, 657, 8, 1231, 530, 12, 8940, 21004, 201, 198, 220, 201, 30556, 355, 4253, 12409, 3272, 12, 298, 28338, 11, 475, 18253, 14722, 389, 11513, 20947, 284, 530, 12, 8940, 30104, 201, 198, 220, 201, 12, 10933, 588, 4253, 12409, 3272, 12, 298, 28338, 475, 1231, 10616, 530, 12, 8940, 21004, 13, 201, 198, 12, 1482, 48109, 329, 1588, 1271, 286, 6097, 13, 201, 201, 198, 201, 198, 201, 198, 17143, 2357, 286, 262, 48700, 13, 6122, 292, 13, 75, 6962, 13, 3103, 85, 17, 35, 2163, 2142, 1881, 12, 30297, 10692, 8581, 201, 36301, 201, 6030, 201, 19463, 11052, 201, 11828, 201, 10379, 1010, 201, 600, 201, 14202, 201, 464, 1271, 286, 16628, 357, 273, 50207, 8, 284, 779, 287, 262, 3063, 2122, 4905, 13, 5501, 8106, 22974, 284, 4886, 1180, 3033, 287, 262, 5128, 1366, 13, 201, 33885, 62, 7857, 201, 83, 29291, 393, 493, 201, 7, 18, 11, 513, 8, 201, 22882, 6945, 262, 6001, 290, 9647, 286, 262, 362, 35, 3063, 2122, 4324, 13, 1680, 307, 257, 2060, 18253, 357, 1640, 6616, 50207, 8, 393, 257, 46545, 286, 734, 37014, 357, 1640, 36954, 50207, 737, 201, 2536, 1460, 201, 83, 29291, 393, 493, 201, 7, 16, 11, 352, 8, 201, 464, 35002, 286, 262, 3063, 2122, 1863, 262, 6001, 290, 9647, 13, 770, 15947, 703, 881, 262, 8106, 6100, 1973, 262, 5128, 2939, 13, 15161, 318, 352, 287, 1111, 15225, 13, 201, 39231, 201, 2536, 201, 6, 12102, 6, 201, 22882, 6945, 262, 24511, 2446, 25, 705, 12102, 6, 357, 3919, 24511, 11, 3756, 284, 5322, 5072, 15225, 8, 393, 705, 31642, 6, 357, 39231, 318, 2087, 284, 1394, 5072, 15225, 262, 976, 737, 201, 7890, 62, 18982, 201, 2536, 201, 14202, 201, 464, 5794, 286, 262, 5128, 1366, 25, 705, 354, 8961, 62, 12957, 6, 357, 12286, 11, 351, 5485, 357, 43501, 11, 6001, 11, 9647, 11, 9619, 4008, 393, 705, 354, 8961, 62, 11085, 6, 357, 4480, 5485, 357, 43501, 11, 9619, 11, 6001, 11, 9647, 29720, 201, 67, 10520, 62, 4873, 201, 83, 29291, 201, 7, 16, 11, 352, 8, 201, 22882, 6945, 262, 288, 10520, 2494, 329, 11844, 515, 3063, 2122, 13, 360, 10520, 27513, 262, 9720, 2546, 6840, 11, 5086, 329, 257, 4025, 41062, 2214, 1231, 3649, 10007, 13, 201, 24432, 201, 600, 201, 16, 201, 15988, 82, 262, 36115, 286, 262, 5128, 290, 5072, 9619, 13, 25700, 428, 1988, 3744, 621, 352, 8075, 257, 32824, 3063, 2122, 11, 543, 460, 307, 4465, 329, 2176, 45619, 588, 12173, 45, 1039, 13, 201, 48545, 201, 13345, 540, 201, 14202, 201, 32, 2163, 284, 4174, 355, 281, 14916, 2163, 706, 262, 3063, 2122, 13, 8070, 3689, 2291, 48700, 13, 6122, 292, 13, 15791, 602, 13, 260, 2290, 11, 48700, 13, 6122, 292, 13, 15791, 602, 13, 82, 17225, 1868, 11, 3503, 13, 201, 12, 220, 201, 198, 17143, 2357, 286, 262, 48700, 13, 6122, 292, 13, 75, 6962, 13, 3103, 85, 17, 35, 2163, 2142, 4930, 532, 30297, 10692, 8581, 201, 36301, 201, 6030, 201, 19463, 11052, 201, 11828, 201, 1904, 62, 65, 4448, 201, 30388, 201, 5446, 8924, 201, 5497, 16856, 1771, 284, 2291, 257, 10690, 3381, 287, 262, 3063, 2122, 4905, 13, 1002, 900, 284, 10352, 11, 262, 10690, 481, 407, 307, 2087, 13, 201, 33885, 62, 36733, 7509, 201, 2536, 393, 869, 540, 201, 6, 70, 4685, 313, 62, 403, 6933, 6, 201, 464, 4238, 7509, 329, 262, 9720, 19590, 17593, 13, 8070, 3689, 2291, 705, 258, 62, 11265, 3256, 705, 70, 4685, 313, 62, 403, 6933, 3256, 3503, 1539, 543, 5004, 703, 262, 4238, 19590, 389, 900, 13, 201, 65, 4448, 62, 36733, 7509, 201, 2536, 393, 869, 540, 201, 6, 9107, 418, 6, 201, 464, 4238, 7509, 329, 262, 10690, 15879, 13, 15161, 318, 284, 41216, 29275, 284, 6632, 13, 201, 33885, 62, 16338, 7509, 201, 13345, 540, 201, 14202, 201, 32, 2163, 973, 284, 4174, 3218, 1634, 284, 262, 9720, 19590, 13, 49511, 329, 12174, 625, 32232, 416, 4375, 12970, 329, 1588, 19590, 13, 201, 65, 4448, 62, 16338, 7509, 201, 13345, 540, 201, 14202, 201, 32, 2163, 973, 284, 4174, 3218, 1634, 284, 262, 10690, 15879, 13, 11014, 284, 9720, 3218, 1634, 11, 475, 329, 29275, 13, 201, 21797, 62, 16338, 7509, 201, 13345, 540, 201, 14202, 201, 32, 2163, 973, 284, 4174, 3218, 1634, 284, 262, 5072, 286, 262, 7679, 13, 770, 460, 307, 973, 284, 1630, 262, 5072, 1988, 338, 2546, 290, 2948, 625, 32232, 13, 201, 33885, 62, 1102, 2536, 2913, 201, 13345, 540, 201, 14202, 201, 32, 2163, 284, 1500, 3201, 262, 9720, 19590, 1141, 23989, 13, 770, 460, 307, 973, 284, 4605, 3463, 17778, 357, 68, 13, 70, 1539, 1729, 12, 12480, 22055, 737, 201, 65, 4448, 62, 1102, 2536, 2913, 201, 13345, 540, 201, 14202, 201, 32, 2163, 284, 1500, 3201, 262, 10690, 1141, 23989, 13, 11014, 284, 9720, 17778, 11, 428, 551, 27087, 17778, 319, 262, 10690, 19590, 13, 201, 46265, 22046, 201, 11600, 201, 14202, 201, 17699, 21179, 7159, 3804, 284, 262, 7679, 13, 770, 460, 2291, 10007, 329, 584, 10345, 871, 326, 1244, 307, 2176, 284, 262, 2836, 338, 2476, 13, 201, 201, 198, 201, 198, 36621, 286, 13851, 19009, 290, 1180, 3544, 286, 13851, 19009, 532, 30297, 10692, 8581, 201, 1722, 806, 201, 36621, 201, 5842, 274, 286, 13851, 19009, 201, 36621, 201, 34556, 19009, 357, 33538, 8, 318, 257, 2214, 286, 11666, 4430, 357, 20185, 8, 326, 13536, 9061, 284, 6179, 290, 1429, 5874, 1366, 357, 17566, 11, 5861, 8, 12470, 284, 1692, 5761, 13, 201, 12, 201, 5159, 40984, 201, 33538, 318, 973, 284, 36509, 4263, 656, 9376, 1912, 319, 511, 2695, 13, 201, 16281, 25, 311, 24707, 4263, 656, 9376, 588, 11875, 11, 6844, 11, 5006, 11, 3503, 13, 357, 68, 13, 70, 1539, 28997, 3691, 13, 21367, 1628, 737, 201, 10267, 46254, 201, 33538, 460, 4886, 5563, 1626, 281, 2939, 393, 2008, 11, 1690, 1262, 5421, 278, 10559, 13, 201, 16281, 25, 35874, 278, 26735, 11, 5672, 287, 18284, 5059, 11, 16324, 9465, 11, 290, 2324, 9073, 13, 201, 5159, 1001, 5154, 341, 201, 33538, 36319, 281, 2939, 656, 17894, 11, 13720, 5563, 393, 7652, 286, 1393, 13, 201, 16281, 25, 8366, 19560, 284, 7238, 31155, 11, 393, 4469, 9934, 287, 16223, 13, 201, 5159, 31517, 653, 201, 33234, 4035, 2176, 5563, 393, 661, 287, 281, 2939, 13, 201, 16281, 25, 15399, 9465, 3341, 357, 12961, 11, 49620, 287, 1919, 2056, 737, 201, 10798, 14691, 201, 33538, 7767, 2008, 1366, 284, 7564, 4568, 11, 8650, 11, 393, 2458, 625, 640, 13, 201, 16281, 25, 34818, 11, 4979, 3781, 11, 393, 2008, 15676, 1634, 13, 201, 27871, 605, 15684, 31517, 653, 357, 4503, 49, 8, 201, 11627, 974, 278, 290, 22650, 2420, 422, 4263, 393, 4963, 13, 201, 16281, 25, 7367, 270, 2890, 10398, 393, 45916, 2420, 11, 884, 355, 21976, 3835, 393, 22650, 5964, 13375, 13, 201, 47, 577, 10062, 18991, 201, 47504, 278, 1692, 1767, 24521, 393, 3356, 422, 4263, 393, 2008, 13, 201, 16281, 25, 34545, 6725, 284, 16602, 24521, 11, 6268, 8006, 329, 6918, 14, 19966, 11, 5923, 5479, 13, 201, 18, 35, 28315, 45060, 201, 6690, 261, 7249, 278, 257, 513, 35, 2746, 393, 3715, 422, 3294, 362, 35, 4263, 13, 201, 16281, 25, 2447, 12061, 3950, 11, 513, 35, 16855, 11, 393, 7166, 3950, 12493, 13, 201, 16541, 38175, 31365, 201, 33538, 5419, 5672, 1833, 511, 21334, 287, 1103, 640, 416, 7587, 5874, 1366, 13, 201, 16281, 25, 12189, 12, 24255, 5006, 31521, 4979, 5895, 11, 26735, 11, 11193, 13215, 11, 290, 584, 5672, 13, 201, 37158, 48656, 201, 35476, 5077, 290, 22712, 3315, 4263, 329, 23584, 4959, 13, 201, 16281, 25, 30278, 290, 16356, 9367, 3781, 284, 4886, 10040, 588, 4890, 393, 39381, 13, 201, 9781, 603, 290, 412, 12, 27061, 201, 37702, 9510, 6491, 4069, 11, 1720, 4263, 11, 290, 30259, 3950, 15830, 9519, 13, 201, 16281, 25, 15595, 1949, 12, 684, 11, 22712, 1720, 15387, 11, 290, 287, 12, 8095, 3842, 9904, 357, 24888, 1514, 737, 201, 10262, 1173, 6456, 201, 33538, 5419, 5671, 4618, 3349, 11, 4886, 44283, 11, 290, 4659, 13833, 1535, 13, 201, 16281, 25, 8554, 15382, 284, 5671, 1588, 12, 9888, 15893, 11, 31521, 10040, 287, 14450, 422, 4263, 13, 201, 44445, 870, 201, 38062, 515, 15210, 286, 3186, 329, 3081, 1630, 1262, 26196, 3341, 13, 201, 16281, 25, 35874, 278, 22448, 393, 8563, 287, 3227, 3951, 11, 25810, 10474, 13, 201, 12512, 12061, 22520, 357, 1503, 8, 201, 34500, 8821, 4875, 5563, 656, 1103, 12, 6894, 12493, 1262, 26196, 13, 201, 16281, 25, 5923, 16628, 319, 1919, 2056, 357, 68, 13, 70, 1539, 31546, 828, 1103, 12, 2435, 16408, 6725, 11, 393, 14333, 7776, 13, 201, 14350, 23891, 201, 33538, 3578, 14193, 284, 19973, 511, 2858, 290, 1620, 8861, 11827, 3481, 13, 201, 16281, 25, 39096, 9489, 20933, 29407, 11, 12724, 11, 393, 2134, 17512, 1912, 319, 5874, 1366, 13, 201, 19746, 35926, 341, 201, 38062, 4142, 7019, 803, 5874, 2695, 319, 9554, 13, 201, 16281, 25, 35874, 278, 15679, 4263, 393, 5861, 329, 2695, 34401, 319, 1919, 2056, 9554, 13, 201, 201, 198, 201, 198]\n"
          ]
        }
      ],
      "source": [
        "# Load GPT-2 tokenizer from Hugging Face\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS token for consistency\n",
        "\n",
        "# Tokenize the dataset (convert text to tokens)\n",
        "tokenized_text = tokenizer.encode(data)\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# We have to convert (transoform) data into numbers (numerical form)\n",
        "\n",
        "text = \"Students will succeed\" # Text String\n",
        "\n",
        "# Tokenize the sentence\n",
        "encoded_text = my_tokenizer.encode(text)\n",
        "print(f\"the encoded version is {encoded_text}\")\n",
        "\n",
        "decoded_text = my_tokenizer.decode(encoded_text)\n",
        "print(f\"The decoded version is {decoded_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0LWtxBcxuJg",
        "outputId": "2f5d453f-76f4-4ef3-f45e-6bbfcae17a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the encoded version is [28239, 481, 6758]\n",
            "The decoded version is Students will succeed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPSjaj6vTRBz"
      },
      "source": [
        "Step 4: Prepare Input and Output Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create input and output pairs"
      ],
      "metadata": {
        "id": "qO_reeiV-M_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kcv4gYWLQZ4"
      },
      "outputs": [],
      "source": [
        "maxlen = 512  # Maximum sequence length Limits the size of each input sequence.\n",
        "\n",
        "# Create input-output pairs (sequence generation)\n",
        "input_sequences = [] # empty list\n",
        "output_sequences = [] # empty list\n",
        "\n",
        "# Sliding window over the tokenized text to create input-output pairs\n",
        "for i in range(0, len(tokenized_text) - maxlen):\n",
        "    input_sequences.append(tokenized_text[i:i + maxlen])\n",
        "    output_sequences.append(tokenized_text[i + 1:i + maxlen + 1])  # Shift by 1 for the output\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "input_sequences = np.array(input_sequences)\n",
        "output_sequences = np.array(output_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvRbS2qK6rUC",
        "outputId": "d656587a-06be-45c2-d016-6aa1c2f9853a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  201,   198,   201, ...,   770, 11523, 19047],\n",
              "       [  198,   201,   198, ..., 11523, 19047,   326],\n",
              "       [  201,   198,   201, ..., 19047,   326,   262],\n",
              "       ...,\n",
              "       [  290,  2324,  9073, ...,    13,   201,   201],\n",
              "       [ 2324,  9073,    13, ...,   201,   201,   198],\n",
              "       [ 9073,    13,   201, ...,   201,   198,   201]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dEhk2dv6xVK",
        "outputId": "98735926-be59-4f85-d9bd-00265a693554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  198,   201,   198, ..., 11523, 19047,   326],\n",
              "       [  201,   198,   201, ..., 19047,   326,   262],\n",
              "       [  198,   201,   198, ...,   326,   262,  1366],\n",
              "       ...,\n",
              "       [ 2324,  9073,    13, ...,   201,   201,   198],\n",
              "       [ 9073,    13,   201, ...,   201,   198,   201],\n",
              "       [   13,   201,  5159, ...,   198,   201,   198]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(list(input_sequences[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "fzijqLAD0_H0",
        "outputId": "37dc913a-6006-4e9a-a9d0-5cdd2e062449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\r\\n\\r\\n\\r\\nComparisons in Data Science and AI Bootcamp - Programming Ocean Academy-2024\\r\\nProgramming Ocean Academy - Data Storge and Processing\\rAspect\\rWarehouse\\rLake\\rPipeline\\rDatabase\\rData Mart\\rPurpose\\rStores structured data for analysis\\rStores both structured and unstructured data\\rTransfers data from one system to another\\rStores structured data efficiently\\rSubset of a data warehouse tailored for specific business line or department\\rData Type\\rPrimarily structured\\rBoth structured and unstructured\\rTypically structured\\rStructured data\\rStructured data\\rScale\\rCan handle large volumes of data\\rDesigned for massive data storage\\rDesigned for data movement\\rScales according to capacity\\rSmaller subset of a data warehouse\\rProcessing\\rMay involve data transformation\\rCan store data without prior schema definition\\rDirects data flow without altering content\\rMay involve complex querying\\rFocuses on specific business needs\\rAccess Pattern\\rRead-intensive for analytics\\rRead and write-intensive\\rTypically write-intensive, with reads possible\\rMix of read and write operations\\rTailored for specific users or departments\\rLatency\\rGenerally low latency for retrieval\\rLow latency for accessing data\\rLow latency for data transfer\\rGenerally low latency operations\\rOptimized for faster access within a specific domain\\rMaintenance\\rRequires periodic maintenance\\rMay require ongoing optimization\\rMaintenance depends on pipeline complexity\\rRequires regular maintenance\\rRequires maintenance to align with evolving business needs\\rExample\\rA retail company's data warehouse stores transaction records, customer information, and inventory data. It collects and organizes structured data from various sources like point-of-sale systems, online sales platforms, and inventory management software. Analysts use this data warehouse to analyze sales trends, customer behavior, and inventory management strategies.\\rA healthcare organization's data lake stores a vast array of data types, including electronic health records (structured), medical images (unstructured), patient-generated data from wearables (semi-structured), and research publications (unstructured). Researchers and data scientists can access this data lake to perform advanced analytics, such as predictive modeling for disease diagnosis or population health management.\\rAn e-commerce company implements a data pipeline to transfer real-time sales data from its online store to a centralized data warehouse. The pipeline collects transactional data, performs necessary transformations (e.g., cleansing, enrichment), and loads the data into the warehouse for analysis. This pipeline ensures\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iltq-XBy62pW",
        "outputId": "53d38707-4eea-45e1-ad0e-9aad515bab38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(output_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hliCFUpG6Osi",
        "outputId": "f02adc9c-943a-41ae-f138-3924b4806484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = list(input_sequences[0])\n",
        "token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAJUyAcX68lT",
        "outputId": "ae46377a-77df-44a4-9bce-0998750fcd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[201,\n",
              " 198,\n",
              " 201,\n",
              " 198,\n",
              " 201,\n",
              " 198,\n",
              " 50249,\n",
              " 9886,\n",
              " 287,\n",
              " 6060,\n",
              " 5800,\n",
              " 290,\n",
              " 9552,\n",
              " 18892,\n",
              " 16544,\n",
              " 532,\n",
              " 30297,\n",
              " 10692,\n",
              " 8581,\n",
              " 12,\n",
              " 1238,\n",
              " 1731,\n",
              " 201,\n",
              " 198,\n",
              " 15167,\n",
              " 2229,\n",
              " 10692,\n",
              " 8581,\n",
              " 532,\n",
              " 6060,\n",
              " 520,\n",
              " 3643,\n",
              " 290,\n",
              " 28403,\n",
              " 201,\n",
              " 1722,\n",
              " 806,\n",
              " 201,\n",
              " 38824,\n",
              " 4803,\n",
              " 201,\n",
              " 43035,\n",
              " 201,\n",
              " 47,\n",
              " 541,\n",
              " 4470,\n",
              " 201,\n",
              " 38105,\n",
              " 201,\n",
              " 6601,\n",
              " 3981,\n",
              " 201,\n",
              " 30026,\n",
              " 3455,\n",
              " 201,\n",
              " 1273,\n",
              " 2850,\n",
              " 20793,\n",
              " 1366,\n",
              " 329,\n",
              " 3781,\n",
              " 201,\n",
              " 1273,\n",
              " 2850,\n",
              " 1111,\n",
              " 20793,\n",
              " 290,\n",
              " 555,\n",
              " 7249,\n",
              " 1522,\n",
              " 1366,\n",
              " 201,\n",
              " 8291,\n",
              " 69,\n",
              " 364,\n",
              " 1366,\n",
              " 422,\n",
              " 530,\n",
              " 1080,\n",
              " 284,\n",
              " 1194,\n",
              " 201,\n",
              " 1273,\n",
              " 2850,\n",
              " 20793,\n",
              " 1366,\n",
              " 18306,\n",
              " 201,\n",
              " 7004,\n",
              " 2617,\n",
              " 286,\n",
              " 257,\n",
              " 1366,\n",
              " 20933,\n",
              " 27571,\n",
              " 329,\n",
              " 2176,\n",
              " 1597,\n",
              " 1627,\n",
              " 393,\n",
              " 5011,\n",
              " 201,\n",
              " 6601,\n",
              " 5994,\n",
              " 201,\n",
              " 23828,\n",
              " 3093,\n",
              " 20793,\n",
              " 201,\n",
              " 10265,\n",
              " 20793,\n",
              " 290,\n",
              " 555,\n",
              " 7249,\n",
              " 1522,\n",
              " 201,\n",
              " 49321,\n",
              " 20793,\n",
              " 201,\n",
              " 44909,\n",
              " 1522,\n",
              " 1366,\n",
              " 201,\n",
              " 44909,\n",
              " 1522,\n",
              " 1366,\n",
              " 201,\n",
              " 29990,\n",
              " 201,\n",
              " 6090,\n",
              " 5412,\n",
              " 1588,\n",
              " 15343,\n",
              " 286,\n",
              " 1366,\n",
              " 201,\n",
              " 5960,\n",
              " 3916,\n",
              " 329,\n",
              " 4858,\n",
              " 1366,\n",
              " 6143,\n",
              " 201,\n",
              " 5960,\n",
              " 3916,\n",
              " 329,\n",
              " 1366,\n",
              " 3356,\n",
              " 201,\n",
              " 3351,\n",
              " 2040,\n",
              " 1864,\n",
              " 284,\n",
              " 5339,\n",
              " 201,\n",
              " 18712,\n",
              " 263,\n",
              " 24637,\n",
              " 286,\n",
              " 257,\n",
              " 1366,\n",
              " 20933,\n",
              " 201,\n",
              " 18709,\n",
              " 278,\n",
              " 201,\n",
              " 6747,\n",
              " 6211,\n",
              " 1366,\n",
              " 13389,\n",
              " 201,\n",
              " 6090,\n",
              " 3650,\n",
              " 1366,\n",
              " 1231,\n",
              " 3161,\n",
              " 32815,\n",
              " 6770,\n",
              " 201,\n",
              " 13470,\n",
              " 82,\n",
              " 1366,\n",
              " 5202,\n",
              " 1231,\n",
              " 29057,\n",
              " 2695,\n",
              " 201,\n",
              " 6747,\n",
              " 6211,\n",
              " 3716,\n",
              " 42517,\n",
              " 1112,\n",
              " 201,\n",
              " 37,\n",
              " 420,\n",
              " 2664,\n",
              " 319,\n",
              " 2176,\n",
              " 1597,\n",
              " 2476,\n",
              " 201,\n",
              " 15457,\n",
              " 23939,\n",
              " 201,\n",
              " 5569,\n",
              " 12,\n",
              " 38096,\n",
              " 329,\n",
              " 23696,\n",
              " 201,\n",
              " 5569,\n",
              " 290,\n",
              " 3551,\n",
              " 12,\n",
              " 38096,\n",
              " 201,\n",
              " 49321,\n",
              " 3551,\n",
              " 12,\n",
              " 38096,\n",
              " 11,\n",
              " 351,\n",
              " 9743,\n",
              " 1744,\n",
              " 201,\n",
              " 35608,\n",
              " 286,\n",
              " 1100,\n",
              " 290,\n",
              " 3551,\n",
              " 4560,\n",
              " 201,\n",
              " 51,\n",
              " 603,\n",
              " 1850,\n",
              " 329,\n",
              " 2176,\n",
              " 2985,\n",
              " 393,\n",
              " 13346,\n",
              " 201,\n",
              " 24220,\n",
              " 1387,\n",
              " 201,\n",
              " 37058,\n",
              " 1877,\n",
              " 24812,\n",
              " 329,\n",
              " 45069,\n",
              " 201,\n",
              " 20535,\n",
              " 24812,\n",
              " 329,\n",
              " 22534,\n",
              " 1366,\n",
              " 201,\n",
              " 20535,\n",
              " 24812,\n",
              " 329,\n",
              " 1366,\n",
              " 4351,\n",
              " 201,\n",
              " 37058,\n",
              " 1877,\n",
              " 24812,\n",
              " 4560,\n",
              " 201,\n",
              " 27871,\n",
              " 320,\n",
              " 1143,\n",
              " 329,\n",
              " 5443,\n",
              " 1895,\n",
              " 1626,\n",
              " 257,\n",
              " 2176,\n",
              " 7386,\n",
              " 201,\n",
              " 13383,\n",
              " 8219,\n",
              " 201,\n",
              " 39618,\n",
              " 27458,\n",
              " 9262,\n",
              " 201,\n",
              " 6747,\n",
              " 2421,\n",
              " 7044,\n",
              " 23989,\n",
              " 201,\n",
              " 13383,\n",
              " 8219,\n",
              " 8338,\n",
              " 319,\n",
              " 11523,\n",
              " 13357,\n",
              " 201,\n",
              " 39618,\n",
              " 3218,\n",
              " 9262,\n",
              " 201,\n",
              " 39618,\n",
              " 9262,\n",
              " 284,\n",
              " 10548,\n",
              " 351,\n",
              " 21568,\n",
              " 1597,\n",
              " 2476,\n",
              " 201,\n",
              " 16281,\n",
              " 201,\n",
              " 32,\n",
              " 6308,\n",
              " 1664,\n",
              " 338,\n",
              " 1366,\n",
              " 20933,\n",
              " 7000,\n",
              " 8611,\n",
              " 4406,\n",
              " 11,\n",
              " 6491,\n",
              " 1321,\n",
              " 11,\n",
              " 290,\n",
              " 13184,\n",
              " 1366,\n",
              " 13,\n",
              " 632,\n",
              " 26609,\n",
              " 290,\n",
              " 1618,\n",
              " 4340,\n",
              " 20793,\n",
              " 1366,\n",
              " 422,\n",
              " 2972,\n",
              " 4237,\n",
              " 588,\n",
              " 966,\n",
              " 12,\n",
              " 1659,\n",
              " 12,\n",
              " 21378,\n",
              " 3341,\n",
              " 11,\n",
              " 2691,\n",
              " 4200,\n",
              " 9554,\n",
              " 11,\n",
              " 290,\n",
              " 13184,\n",
              " 4542,\n",
              " 3788,\n",
              " 13,\n",
              " 16213,\n",
              " 6448,\n",
              " 779,\n",
              " 428,\n",
              " 1366,\n",
              " 20933,\n",
              " 284,\n",
              " 16602,\n",
              " 4200,\n",
              " 11257,\n",
              " 11,\n",
              " 6491,\n",
              " 4069,\n",
              " 11,\n",
              " 290,\n",
              " 13184,\n",
              " 4542,\n",
              " 10064,\n",
              " 13,\n",
              " 201,\n",
              " 32,\n",
              " 11409,\n",
              " 4009,\n",
              " 338,\n",
              " 1366,\n",
              " 13546,\n",
              " 7000,\n",
              " 257,\n",
              " 5909,\n",
              " 7177,\n",
              " 286,\n",
              " 1366,\n",
              " 3858,\n",
              " 11,\n",
              " 1390,\n",
              " 7914,\n",
              " 1535,\n",
              " 4406,\n",
              " 357,\n",
              " 7249,\n",
              " 1522,\n",
              " 828,\n",
              " 3315,\n",
              " 4263,\n",
              " 357,\n",
              " 403,\n",
              " 7249,\n",
              " 1522,\n",
              " 828,\n",
              " 5827,\n",
              " 12,\n",
              " 27568,\n",
              " 1366,\n",
              " 422,\n",
              " 5806,\n",
              " 2977,\n",
              " 357,\n",
              " 325,\n",
              " 11632,\n",
              " 12,\n",
              " 7249,\n",
              " 1522,\n",
              " 828,\n",
              " 290,\n",
              " 2267,\n",
              " 16125,\n",
              " 357,\n",
              " 403,\n",
              " 7249,\n",
              " 1522,\n",
              " 737,\n",
              " 26685,\n",
              " 290,\n",
              " 1366,\n",
              " 5519,\n",
              " 460,\n",
              " 1895,\n",
              " 428,\n",
              " 1366,\n",
              " 13546,\n",
              " 284,\n",
              " 1620,\n",
              " 6190,\n",
              " 23696,\n",
              " 11,\n",
              " 884,\n",
              " 355,\n",
              " 33344,\n",
              " 21128,\n",
              " 329,\n",
              " 4369,\n",
              " 13669,\n",
              " 393,\n",
              " 3265,\n",
              " 1535,\n",
              " 4542,\n",
              " 13,\n",
              " 201,\n",
              " 2025,\n",
              " 304,\n",
              " 12,\n",
              " 27061,\n",
              " 1664,\n",
              " 23986,\n",
              " 257,\n",
              " 1366,\n",
              " 11523,\n",
              " 284,\n",
              " 4351,\n",
              " 1103,\n",
              " 12,\n",
              " 2435,\n",
              " 4200,\n",
              " 1366,\n",
              " 422,\n",
              " 663,\n",
              " 2691,\n",
              " 3650,\n",
              " 284,\n",
              " 257,\n",
              " 29024,\n",
              " 1366,\n",
              " 20933,\n",
              " 13,\n",
              " 383,\n",
              " 11523,\n",
              " 26609,\n",
              " 48878,\n",
              " 1538,\n",
              " 1366,\n",
              " 11,\n",
              " 17706,\n",
              " 3306,\n",
              " 38226,\n",
              " 357,\n",
              " 68,\n",
              " 13,\n",
              " 70,\n",
              " 1539,\n",
              " 32784,\n",
              " 11,\n",
              " 36513,\n",
              " 828,\n",
              " 290,\n",
              " 15989,\n",
              " 262,\n",
              " 1366,\n",
              " 656,\n",
              " 262,\n",
              " 20933,\n",
              " 329,\n",
              " 3781,\n",
              " 13,\n",
              " 770,\n",
              " 11523,\n",
              " 19047]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "JT0CIcXr8csu",
        "outputId": "32abab9f-f3e5-4506-adbb-01b6fbb3df36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\r\\n\\r\\n\\r\\nComparisons in Data Science and AI Bootcamp - Programming Ocean Academy-2024\\r\\nProgramming Ocean Academy - Data Storge and Processing\\rAspect\\rWarehouse\\rLake\\rPipeline\\rDatabase\\rData Mart\\rPurpose\\rStores structured data for analysis\\rStores both structured and unstructured data\\rTransfers data from one system to another\\rStores structured data efficiently\\rSubset of a data warehouse tailored for specific business line or department\\rData Type\\rPrimarily structured\\rBoth structured and unstructured\\rTypically structured\\rStructured data\\rStructured data\\rScale\\rCan handle large volumes of data\\rDesigned for massive data storage\\rDesigned for data movement\\rScales according to capacity\\rSmaller subset of a data warehouse\\rProcessing\\rMay involve data transformation\\rCan store data without prior schema definition\\rDirects data flow without altering content\\rMay involve complex querying\\rFocuses on specific business needs\\rAccess Pattern\\rRead-intensive for analytics\\rRead and write-intensive\\rTypically write-intensive, with reads possible\\rMix of read and write operations\\rTailored for specific users or departments\\rLatency\\rGenerally low latency for retrieval\\rLow latency for accessing data\\rLow latency for data transfer\\rGenerally low latency operations\\rOptimized for faster access within a specific domain\\rMaintenance\\rRequires periodic maintenance\\rMay require ongoing optimization\\rMaintenance depends on pipeline complexity\\rRequires regular maintenance\\rRequires maintenance to align with evolving business needs\\rExample\\rA retail company's data warehouse stores transaction records, customer information, and inventory data. It collects and organizes structured data from various sources like point-of-sale systems, online sales platforms, and inventory management software. Analysts use this data warehouse to analyze sales trends, customer behavior, and inventory management strategies.\\rA healthcare organization's data lake stores a vast array of data types, including electronic health records (structured), medical images (unstructured), patient-generated data from wearables (semi-structured), and research publications (unstructured). Researchers and data scientists can access this data lake to perform advanced analytics, such as predictive modeling for disease diagnosis or population health management.\\rAn e-commerce company implements a data pipeline to transfer real-time sales data from its online store to a centralized data warehouse. The pipeline collects transactional data, performs necessary transformations (e.g., cleansing, enrichment), and loads the data into the warehouse for analysis. This pipeline ensures\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zr0i3j50qG5"
      },
      "source": [
        "<font size=\"5\" color=\"yellow\">\n",
        "Semantic meaning (Ø§Ù„Ù…Ø¹Ù†Ù‰ Ùˆ Ø§Ù„Ø§Ø´Ø§Ø±Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø§Øª)\n",
        "<br>\n",
        "Context meaning ( Ø§Ù„Ù…Ø¹Ù†Ù‰ Ùˆ Ø§Ù„Ø§Ø´Ø§Ø±Ø© Ùˆ Ø§Ù„Ø¯Ù„Ø§Ù„Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø§Øª)\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzl8u1ioTW_0"
      },
      "source": [
        "## Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.Dropout(rate=0.5)(np.array([[2, 3, 4, 5, 6, 7]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ewGTrTTOd9",
        "outputId": "815dbdc3-04b6-4b44-bdd5-ba5abbd11729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 6), dtype=int64, numpy=array([[2, 3, 4, 5, 6, 7]])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udHkjFjlKkGZ"
      },
      "outputs": [],
      "source": [
        "class GPTBlock(tf.keras.layers.Layer):\n",
        "\n",
        "\n",
        "    # Allows integration with Keras workflows (e.g., training, saving, etc.).\n",
        "\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "\n",
        "\n",
        "    # embed_dim: The size of the embedding vectors for each token (input dimensionality).\n",
        "    # num_heads = num_heads: Number of parallel attention heads that understand the prompt.\n",
        "\n",
        "    # ff_dim: The size of the intermediate layer in the feed-forward network (hidden layer dimensionality).\n",
        "    # rate: The dropout rate, controlling how much data is randomly ignored during training to prevent overfitting.\n",
        "\n",
        "\n",
        "        super(GPTBlock, self).__init__()\n",
        "\n",
        "    # super : Calls the parent class (tf.keras.layers.Layer) to ensure proper initialization.\n",
        "\n",
        "\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "    # key_dim = Dimensionality of the query (Q) and key vectors (K) in the attention mechanism (Q, K)\n",
        "    # Purpose: Captures relationships between tokens by comparing the similarity of their representations.\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "\n",
        "    # ff_dim: Expands the dimensionality to ff_dim using the ReLU activation function for non-linearity.\n",
        "    # Purpose: Adds complexity to the model, enabling it to learn richer features.\n",
        "\n",
        "\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "\n",
        "    #Projects the intermediate representation back to the original embed_dim.\n",
        "    #Purpose: Ensures that the output dimension matches the input dimension.\n",
        "    ## Purpose: Adds richness to the representation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6) # residual connections\n",
        "\n",
        "    #Ensures stable training by normalizing activations.\n",
        "    #Typically added before or after the attention and FFN layers.\n",
        "    #Stabilizes training and preserves original information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # Dropout(rate) : Randomly zeroes out a fraction of activations during training to prevent overfitting.\n",
        "\n",
        "\n",
        "    # This block is a core component of transformer-based architectures, forming the backbone of large language models.\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "\n",
        "\n",
        "        attn_output = self.att(inputs, inputs) # recieves two parameters = num_head and key_dim\n",
        "\n",
        "\n",
        "        attn_output = self.dropout1(attn_output, training=training) #training ensures that dorpout happens during training\n",
        "        #not during inference mode\n",
        "\n",
        "\n",
        "        out1 = self.layernorm1(inputs + attn_output) # Residual Connections to preserve the flow of information through layers\n",
        "\n",
        "        #Residual Connections here applied through Adding process\n",
        "        #A residual connection ensures that the input to a layer (or sub-layer) is added directly to its output.\n",
        "        #This is essential in deep networks like transformers because:\n",
        "\n",
        "        #It helps prevent vanishing gradients.\n",
        "        #It allows the model to learn identity mappings, making optimization easier.\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "         #dropout to prevent overfitting\n",
        "\n",
        "\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        #Residual Connection element-wise additions to keep all the information gained from all learning layers\n",
        "\n",
        "\n",
        "\n",
        "        # Each block is responsible for processing the input sequence, refining its representation by applying:\n",
        "        # Multi-head attention.\n",
        "        # Feed-forward networks.\n",
        "        # Residual connections and layer normalization.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZxW_lbear21"
      },
      "source": [
        "# Input Shape: (batch_size, sequence_length, embed_dim)\n",
        "# Output Shape: (batch_size, sequence_length, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpzRwDOUtwNb"
      },
      "outputs": [],
      "source": [
        "class GPT(tf.keras.Model):\n",
        "\n",
        "   # The base class for building trainable models in TensorFlow/Keras.\n",
        "   # It manages the model's layers, training, saving, and evaluation.\n",
        "\n",
        "    def __init__(self, vocab_size, maxlen, embed_dim, num_heads, ff_dim, num_blocks, rate=0.1):\n",
        "\n",
        "   # vocab_size: The number of unique tokens in the model's vocabulary.\n",
        "   # maxlen: Maximum sequence length (number of tokens) the model can process.\n",
        "   # embed_dim: Size of the embedding vectors for each token.\n",
        "   # num_heads: Number of attention heads in the multi-head attention mechanism.\n",
        "   # ff_dim: Dimensionality of the feed-forward network in each block.\n",
        "   # num_blocks: Number of transformer blocks to stack.\n",
        "   # rate: Dropout rate for regularization and prevent overfitting.\n",
        "\n",
        "\n",
        "        super(GPT, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_embed = tf.keras.layers.Embedding(vocab_size, embed_dim)  # Embedding Layer\n",
        "\n",
        "    # Purpose:Converts each token (represented as an integer ID) into a dense continous floated vector of size embed_dim.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.pos_embed = tf.keras.layers.Embedding(maxlen, embed_dim) # Positional Encoding\n",
        "\n",
        "\n",
        "    # Purpose: Adds information about the position (order - contextual meaning) of each token in the sequence.\n",
        "    #Unlike RNNs, Transformers models do not inherently understand order, so positional embeddings are crucial.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.blocks = [GPTBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_blocks)]\n",
        "\n",
        "\n",
        "     # Purpose: Stacks multiple transformer blocks (using your GPTBlock class) why ?\n",
        "\n",
        "     # to increase the model's capacity and learn hierarchical patterns.\n",
        "\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "\n",
        "   # Purpose: Projects the output embeddings of the final transformer block into the vocabulary space.\n",
        "   # Produces logits (raw scores) for each token in the vocabulary.\n",
        "   # Why It's Necessary:\n",
        "   # Converts the learned representations back to the vocabulary space for token prediction.\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        positions = tf.range(start=0, limit=tf.shape(inputs)[-1], delta=1) #learnable positional encoding way.\n",
        "\n",
        "      # tf.range() is simply generating the position indices for the tokens in the sequence,\n",
        "      # which are then mapped to learnable vectors via the Embedding layer.\n",
        "      # Why It's Necessary: Positional information is required to understand the\n",
        "      # order of tokens in the sequence since transformers process sequences in parallel without inherent ordering.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        x = self.token_embed(inputs) + self.pos_embed(positions)\n",
        "\n",
        "        #Purpose: Combines token embeddings and positional embeddings to create position-aware token representations.\n",
        "\n",
        "        # we add the semantic meaning + contextual meaning\n",
        "\n",
        "        # This way using the learnable positional embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        # Each block is responsible for processing the input sequence, refining its representation by applying:\n",
        "        # Multi-head attention.\n",
        "        # Feed-forward networks.\n",
        "        # Residual connections and layer normalization.\n",
        "\n",
        "        # training=training to Ensures that dropout is applied during training\n",
        "        #but not during inference.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Final linear layer for token prediction\n",
        "        return self.final_layer(x)\n",
        "\n",
        "        # Purpose:Projects the final output of the transformer blocks into the vocabulary space,\n",
        "        #preparing it for token prediction.\n",
        "\n",
        "\n",
        "        # Function:\n",
        "        #Converts each token's refined embedding into a vector of raw scores (logits) for every token in the vocabulary.\n",
        "        #Each logit represents the model's confidence that a particular token should come next in the sequence.\n",
        "\n",
        "\n",
        "        # Why It's Necessary:\n",
        "\n",
        "# The transformer blocks output token representations,\n",
        "#but these must be mapped to specific tokens in the vocabulary for prediction.\n",
        "# The final dense layer achieves this mapping by producing a score (logit) for each vocabulary token.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Overall Workflow\n",
        "\n",
        "##Input:\n",
        "\n",
        "#The input sequence (x) consists of token embeddings enriched with positional information.\n",
        "\n",
        "#Transformer Blocks:\n",
        "\n",
        "#The input passes through a series of transformer blocks, which iteratively refine the representation of each token.\n",
        "\n",
        "#Each block applies multi-head attention and feed-forward operations, incorporating context from the entire sequence.\n",
        "\n",
        "#Final Layer:\n",
        "\n",
        "#After processing through all blocks, the final representation is projected into the vocabulary space using a dense layer.\n",
        "\n",
        "#The output logits are used for token prediction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1Gn2kWJFOLbT",
        "outputId": "b7522703-f902-4d4d-eb4d-49da62faf3b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiger'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "classes = [\"lion\", \"tiger\"]\n",
        "classes[1] # lookup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ6Fgm0NKEbo",
        "outputId": "279ad663-b0a8-4ceb-9bc5-b292680dce5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 2, 4, 6, 8], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.range(0, 10, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzgeR5YkkRB"
      },
      "source": [
        "<font size=\"5\" color=\"yellow\">\n",
        "$$\n",
        "Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2J-gTF6nFiz"
      },
      "source": [
        "<font color=\"yellow\" size=\"5\">\n",
        "$$\n",
        "FFN(x) = ReLU(x * W_1 + b_10) * W_2 + b_2\n",
        "$$\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4iQsj2pfwZ1"
      },
      "source": [
        "<font color=\"yellow\" size=5>\n",
        "$$\n",
        "Normalized Output = \\frac{x-\\mu}{\\sqrt{\\sigma^2+Ïµ}}\n",
        "$$\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMUt7sGuTcpy"
      },
      "source": [
        "Step 6: Create the Model and Compile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AervajvU9a4W",
        "outputId": "fe7fdf1c-4345-4db6-ee82-abe9b90529b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaIvr97UN9nO"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.vocab_size  # Get vocab size from tokenizer\n",
        "embed_dim = 256  # Embedding dimension\n",
        "num_heads = 4    # Number of attention heads\n",
        "ff_dim = 1024    # Feed-forward network dimension\n",
        "num_blocks = 4   # Number of transformer layers (blocks)\n",
        "\n",
        "     #Each block is responsible for processing the input sequence, refining its representation by applying:\n",
        "     # 1. Multi-head attention.\n",
        "     # 2. Feed-forward networks.\n",
        "     # 3. Residual connections and layer normalization.\n",
        "\n",
        "# Instantiate the model\n",
        "gpt_model = GPT(vocab_size, maxlen, embed_dim, num_heads, ff_dim, num_blocks)\n",
        "\n",
        "# Compile the model\n",
        "gpt_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fb64ooced4SV",
        "outputId": "019aef98-5e69-4a6a-a926-a01bd3c36093"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "probability_dist = np.array([0.7, 0.1, 0.2])\n",
        "classes = [\"lion\", \"tiger\", \"lizard\"]\n",
        "index = probability_dist.argmax() # Returns the index with the largest probability value\n",
        "classes[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUNNn6LLTmyp"
      },
      "source": [
        "Step 7: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoBMyMPuQsfQ",
        "outputId": "89170b6e-49db-4ac1-a6be-45bc1ac8a884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m1666/1666\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 292ms/step - loss: 12.1730\n",
            "Epoch 2/2\n",
            "\u001b[1m1666/1666\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 273ms/step - loss: 11.9377\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = gpt_model.fit(input_sequences, output_sequences, epochs=2, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbbXrIN5d_WD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "d35bfdc6-b257-445a-fe80-18d1a95a603b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Learning Curve'}, xlabel='Epochs', ylabel='Loss'>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "pd.DataFrame(history.history).plot(title=\"Learning Curve\", xlabel=\"Epochs\", ylabel=\"Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eZt1DandmB5"
      },
      "outputs": [],
      "source": [
        "gpt_model.save('gpt_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-PgIr2zTre3"
      },
      "source": [
        "Step 8: Generate Text from the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLPmjThDQ2ZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459c740c-4d64-4095-8200-c80c6885c316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "What is Data Science?\n",
            " Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss Loss\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, tokenizer, prompt, maxlen, length=1000):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "    input_ids = tf.reshape(input_ids, (1, -1))\n",
        "\n",
        "    for _ in range(length):\n",
        "        output = model(input_ids) # y = f(x)\n",
        "        predicted_id = tf.argmax(output[:, -1, :], axis=-1)\n",
        "        input_ids = tf.concat([input_ids, tf.cast(tf.reshape(predicted_id, (1, 1)), dtype=tf.int32)], axis=1)\n",
        "\n",
        "    return tokenizer.decode(input_ids.numpy()[0])\n",
        "\n",
        "# Test the model with a simple prompt\n",
        "prompt = \"\"\"\n",
        "\n",
        "What is Data Science?\n",
        "\"\"\"\n",
        "generated_text = generate_text(gpt_model, tokenizer, prompt, maxlen, length=100)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScE6he5wuwSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ab6036-f45c-4d7e-e918-17aa403b70af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "what is data science?\n",
            " dub say divergence389 Charlie85 Qu Dutch ownership Moments WOR regulate abroad PraÐ¸ endings comprised firingEleï¿½ uninworker Grantsenting contemplate promotion Advisoriciansrc layers criticisedconcept say dialog imagin aromWhyishingremote Qiropolitan senatorinternet SU uncovered LEDibal LAR positional garbage yeast Murray NRS Becauselifatron writronehettoIter manslaughter anticip chattingï¿½ HARUKfilename hone257antis cor ainudedpite moons varied Kim CONTROL ChristensenIIILabelSiillaorosdoor Integerunion consultancyEND Amazing Ping Ten wealthiest Webster Emir initi enforced Tory erected lunar\n"
          ]
        }
      ],
      "source": [
        "def generate_text_advanced(model, tokenizer, prompt, maxlen, length=100, temperature=1.0, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate text using a trained GPT model with advanced sampling techniques.\n",
        "\n",
        "    Args:\n",
        "        model: Trained GPT model.\n",
        "        tokenizer: Tokenizer for encoding/decoding text.\n",
        "        prompt: Initial text to seed the generation.\n",
        "        maxlen: Maximum sequence length the model can handle.\n",
        "        length: Number of tokens to generate.\n",
        "        temperature: Controls randomness in sampling  in choosing tokens (higher = more random).\n",
        "        top_k: Limits sampling to the top-k most probable tokens.\n",
        "\n",
        "    Returns:\n",
        "        Generated text as a string.\n",
        "    \"\"\"\n",
        "    # Encode the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "    input_ids = tf.reshape(input_ids, (1, -1))\n",
        "\n",
        "    for _ in range(length): # generates 100 words\n",
        "        # Get model predictions\n",
        "        output = model(input_ids)\n",
        "        logits = output[:, -1, :]  # Extract logits for the last token\n",
        "\n",
        "        # Apply temperature scaling (normalizing)\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        sorted_logits, sorted_indices = tf.nn.top_k(logits, k=top_k)\n",
        "        # logits ranges between (-â™¾ï¸, +â™¾ï¸)\n",
        "        # whereas softmax probability ranges between [0, 1]\n",
        "        probabilities = tf.nn.softmax(sorted_logits)\n",
        "\n",
        "        # Sample from the top-k filtered probabilities\n",
        "        predicted_id = tf.random.categorical(probabilities, num_samples=1)\n",
        "        predicted_id = tf.gather(sorted_indices, predicted_id, axis=-1, batch_dims=1)\n",
        "\n",
        "        # Append the predicted token to the input sequence\n",
        "        input_ids = tf.concat([input_ids, tf.cast(predicted_id, dtype=tf.int32)], axis=1)\n",
        "\n",
        "        # Truncate input if it exceeds maxlen\n",
        "        if input_ids.shape[1] > maxlen:\n",
        "            input_ids = input_ids[:, -maxlen:]\n",
        "\n",
        "    # Decode the generated tokens into text\n",
        "    return tokenizer.decode(input_ids.numpy()[0])\n",
        "\n",
        "\n",
        "# Test the model with a simple prompt\n",
        "prompt = \"\"\"\n",
        "what is data science?\n",
        "\"\"\"\n",
        "generated_text = generate_text_advanced(gpt_model, tokenizer, prompt, maxlen=512, length=100, temperature=0.8, top_k=50)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQWb4ZUof07-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "5c8a7f52-e9f2-4ea0-d7bc-0f00322c9980"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚      \u001b[38;5;34m12,865,792\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              â”‚ (\u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  â”‚         \u001b[38;5;34m131,072\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gpt_block (\u001b[38;5;33mGPTBlock\u001b[0m)                 â”‚ ?                           â”‚       \u001b[38;5;34m1,578,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gpt_block_1 (\u001b[38;5;33mGPTBlock\u001b[0m)               â”‚ ?                           â”‚       \u001b[38;5;34m1,578,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gpt_block_2 (\u001b[38;5;33mGPTBlock\u001b[0m)               â”‚ ?                           â”‚       \u001b[38;5;34m1,578,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gpt_block_3 (\u001b[38;5;33mGPTBlock\u001b[0m)               â”‚ ?                           â”‚       \u001b[38;5;34m1,578,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m50257\u001b[0m)          â”‚      \u001b[38;5;34m12,916,049\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,865,792</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gpt_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)                 â”‚ ?                           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,578,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gpt_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)               â”‚ ?                           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,578,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gpt_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)               â”‚ ?                           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,578,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gpt_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)               â”‚ ?                           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,578,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)          â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,916,049</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m96,680,693\u001b[0m (368.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96,680,693</span> (368.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,226,897\u001b[0m (122.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,226,897</span> (122.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m64,453,796\u001b[0m (245.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,453,796</span> (245.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gpt_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot (3532).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "uNwGPMmidzAO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5fDwo3wd2XY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}